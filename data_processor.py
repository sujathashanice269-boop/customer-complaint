"""
æ•°æ®å¤„ç†å™¨ - å®Œæ•´æ”¹è¿›ç‰ˆ
âœ… æ”¹è¿›: å½»åº•åˆ é™¤æ‹¬å·ã€ç³»ç»Ÿå­—æ®µ,ä¿ç•™ä¸“ä¸šè¯(4G/5G/VoLTE)
âœ… é€‚ç”¨: é¢„è®­ç»ƒã€å¾®è°ƒã€å¤§æ•°æ®é›†ã€å°æ•°æ®é›†
âœ… ä¿®å¤: æ·»åŠ äº†load_data, prepare_datasets, ComplaintDataset, custom_collate_fn
"""

import pandas as pd
import numpy as np
import jieba
import re
import torch
import pickle
import os
from typing import List, Dict, Tuple
from collections import Counter
from sklearn.preprocessing import StandardScaler
from transformers import BertTokenizer
from tqdm import tqdm
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
import random

class ComplaintDataProcessor:
    """æŠ•è¯‰æ•°æ®å¤„ç†å™¨ - å¸¦æ™ºèƒ½æ¸…æ´—åŠŸèƒ½"""

    def __init__(self, config, user_dict_file='new_user_dict.txt', stopword_file='new_stopword_dict.txt'):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            config: é…ç½®å¯¹è±¡
            user_dict_file: ç”¨æˆ·è¯å…¸æ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºæ–‡æœ¬ï¼‰
            stopword_file: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºæ–‡æœ¬ï¼‰
        """
        self.config = config

        # ========== 1ï¸âƒ£ é¦–å…ˆåˆå§‹åŒ–tokenizerï¼ˆæœ€é‡è¦ï¼ï¼‰==========
        self.tokenizer = BertTokenizer.from_pretrained(config.model.bert_model_name)

        # ========== 2ï¸âƒ£ åŠ è½½ç”¨æˆ·è¯å…¸ï¼ˆä»…ç”¨äºæ–‡æœ¬ï¼‰==========
        self.user_dict_whitelist = set()

        if user_dict_file and os.path.exists(user_dict_file):
            # åŠ è½½åˆ°jieba
            jieba.load_userdict(user_dict_file)

            # åŒæ—¶ä¿å­˜åˆ°ç™½åå•
            with open(user_dict_file, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split()
                    if parts:
                        word = parts[0]
                        self.user_dict_whitelist.add(word)

            print(f"âœ… å·²åŠ è½½ç”¨æˆ·è¯å…¸: {user_dict_file} (ä»…ç”¨äºæ–‡æœ¬)")
            print(f"âœ… ç”¨æˆ·è¯å…¸ç™½åå•: {len(self.user_dict_whitelist)}ä¸ªè¯ (ç”¨äºæ™ºèƒ½æ¸…æ´—)")

        # ========== 3ï¸âƒ£ åŠ è½½åœç”¨è¯ï¼ˆä»…ç”¨äºæ–‡æœ¬ï¼‰==========
        self.stopwords = set()
        if stopword_file and os.path.exists(stopword_file):
            with open(stopword_file, 'r', encoding='utf-8') as f:
                self.stopwords = set(line.strip() for line in f if line.strip())
            print(f"âœ… å·²åŠ è½½åœç”¨è¯: {stopword_file} ({len(self.stopwords)}ä¸ª, ä»…ç”¨äºæ–‡æœ¬)")

        # ========== 4ï¸âƒ£ æ–‡æœ¬æ¸…æ´—ç¼“å­˜ ==========
        self.text_clean_cache = {}

        # ========== 5ï¸âƒ£ æ ‡ç­¾ç›¸å…³å±æ€§ ==========
        self.node_to_id = {}
        self.id_to_node = {}
        self.global_edges = []
        self.global_node_levels = {}

        # ========== 6ï¸âƒ£ ç»“æ„åŒ–ç‰¹å¾æ ‡å‡†åŒ–å™¨ ==========
        self.scaler = StandardScaler()

    def clean_text_smart(self, text: str) -> str:
        """
        æ™ºèƒ½æ¸…æ´—æ–‡æœ¬ - æ”¹è¿›ç‰ˆ

        æ”¹è¿›ç‚¹:
        1. â­ å…ˆåˆ é™¤ç³»ç»Ÿå™ªéŸ³(æ‹¬å·ã€ç³»ç»Ÿå­—æ®µç­‰)
        2. âœ… ä¿ç•™æ‰€æœ‰ä¸­æ–‡å­—ç¬¦
        3. âœ… ä¿ç•™ç”¨æˆ·è¯å…¸ä¸­çš„è¯(4Gã€5Gã€VoLTEç­‰)
        4. âœ… ä¿ç•™æ­£å¸¸æ ‡ç‚¹ç¬¦å·
        5. âŒ åˆ é™¤å…¶ä»–æ‰€æœ‰å†…å®¹

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        # âœ… æ–°å¢: æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤æ¸…æ´—
        if text in self.text_clean_cache:
            return self.text_clean_cache[text]

        if pd.isna(text) or not text:
            return ""

        text = str(text).strip()

        if not text:
            return ""

        # ========== ç¬¬ä¸€é˜¶æ®µ: åˆ é™¤ç³»ç»Ÿå™ªéŸ³ ==========

        # 1. åˆ é™¤#æ ‡è®°#
        text = re.sub(r'#[^#]*#', '', text)

        # 2. åˆ é™¤[æ•°å­—]æ ‡è®°å’Œ[begin][end]
        text = re.sub(r'\[\d+\]', '', text)
        text = re.sub(r'\[/\d+\]', '', text)
        text = re.sub(r'\[begin\]', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\[end\]', '', text, flags=re.IGNORECASE)

        # 3. â­ æ ¸å¿ƒæ”¹è¿›: åˆ é™¤æ‰€æœ‰å¸¦æ•°å­—çš„æ‹¬å·
        text = re.sub(r'\(\d+\)ã€[^ã€‘]*ã€‘', '', text)  # (1)ã€è‡ªåŠ¨ã€‘
        text = re.sub(r'ã€[^ã€‘]*ã€‘\(\d+\)', '', text)  # ã€äººå·¥ã€‘(1)
        text = re.sub(r'\(\d+\)', '', text)            # (0) (1) (123)
        text = re.sub(r'ã€\d+ã€‘', '', text)            # ã€1ã€‘ã€2ã€‘
        text = re.sub(r'ã€[^ã€‘]*ã€‘', '', text)          # ã€äººå·¥ã€‘ã€è‡ªåŠ¨ã€‘

        # 4. åˆ é™¤å‰©ä½™çš„ç©ºæ‹¬å·
        text = re.sub(r'\(\s*\)', '', text)
        text = re.sub(r'ã€\s*ã€‘', '', text)

        # 5. â­ åˆ é™¤ç³»ç»Ÿå­—æ®µå‰ç¼€
        system_fields = [
            'te_system_log:', 'con_status:', 'pending',
            'te_', 'con_', 'ho_', 'sys_',
            'æµç¨‹è½¨è¿¹', 'å¤„ç†å†…å®¹', 'æŠ•è¯‰å†…å®¹', 'æ´»åŠ¨ä¿¡æ¯',
            'å—ç†å·ç ', 'å–æ¶ˆåŸå› ', 'è”ç³»å·ç ', 'è¯¦æƒ…æè¿°',
            'è”ç³»è¦æ±‚', 'ä¸Šç½‘è´¦å·', 'è”ç³»ç”µè¯', 'ç”³å‘Šå·ç ',
            'å®½å¸¦åœ°å€', 'å®½å¸¦è´¦å·', 'å·¥å•å·', 'å®¢æˆ·è¦æ±‚'
        ]
        for field in system_fields:
            text = re.sub(f'{field}[:ï¼šï¼›]?', '', text, flags=re.IGNORECASE)

        # 6. åˆ é™¤éšç§è„±æ•ç¬¦å·
        text = re.sub(r'\*{3,}', '', text)  # ***
        text = re.sub(r'#{3,}', '', text)   # ###
        text = re.sub(r'WO\d+', '', text)   # å·¥å•å·

        # ========== ç¬¬äºŒé˜¶æ®µ: ä¿ç•™æœ‰ç”¨å†…å®¹ ==========

        # å®šä¹‰ä¿ç•™çš„æ ‡ç‚¹ç¬¦å·
        keep_punctuation = set('ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹')

        # ä½¿ç”¨jiebaåˆ†è¯(ä¼šè‡ªåŠ¨è¯†åˆ«ç”¨æˆ·è¯å…¸ä¸­çš„è¯)
        words = jieba.lcut(text)

        cleaned_words = []

        for word in words:
            # è§„åˆ™1: ç”¨æˆ·è¯å…¸ä¸­çš„è¯ â†’ ç›´æ¥ä¿ç•™(4Gã€5Gã€VoLTEç­‰)
            if word in self.user_dict_whitelist:
                cleaned_words.append(word)
                continue

            # è§„åˆ™2: çº¯ä¸­æ–‡è¯ â†’ ä¿ç•™
            if all('\u4e00' <= c <= '\u9fff' for c in word):
                cleaned_words.append(word)
                continue

            # è§„åˆ™3: çº¯æ ‡ç‚¹ç¬¦å· â†’ ä¿ç•™
            if all(c in keep_punctuation for c in word):
                cleaned_words.append(word)
                continue

            # è§„åˆ™4: æ··åˆè¯ â†’ æ‹†åˆ†åä¿ç•™ä¸­æ–‡å’Œæ ‡ç‚¹
            cleaned_chars = []
            for char in word:
                if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡
                    cleaned_chars.append(char)
                elif char in keep_punctuation:  # æ ‡ç‚¹
                    cleaned_chars.append(char)
                # å…¶ä»–å­—ç¬¦(è‹±æ–‡ã€æ•°å­—ã€ç‰¹æ®Šç¬¦å·) â†’ å¿½ç•¥

            if cleaned_chars:
                cleaned_words.append(''.join(cleaned_chars))

        # åˆå¹¶æ¸…æ´—åçš„è¯
        cleaned_text = ''.join(cleaned_words)

        # å»é™¤å¤šä½™çš„è¿ç»­æ ‡ç‚¹
        cleaned_text = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼š]+', 'ï¼Œ', cleaned_text)

        # å»é™¤å¼€å¤´å’Œç»“å°¾çš„æ ‡ç‚¹
        cleaned_text = re.sub(r'^[ï¼Œã€‚ï¼›ï¼š]+', '', cleaned_text)
        cleaned_text = re.sub(r'[ï¼Œã€‚ï¼›ï¼š]+$', '', cleaned_text)

        # å»é™¤æ‰€æœ‰ç©ºæ ¼
        cleaned_text = re.sub(r'\s+', '', cleaned_text)

        # âœ… æ–°å¢: ä¿å­˜åˆ°ç¼“å­˜
        cleaned_text = cleaned_text.strip()

        # â­ ä¿®æ”¹3-æ”¹è¿›ç‰ˆ: åªè¿‡æ»¤æç«¯é•¿åº¦,ä¸æ£€æµ‹é‡å¤åº¦
        # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬(< 5å­—ç¬¦,æ²¡æœ‰å®é™…å†…å®¹)
        if len(cleaned_text) < 5:
            self.text_clean_cache[text] = ""
            return ""

        # è¿‡æ»¤å¤ªé•¿çš„æ–‡æœ¬(> 500å­—ç¬¦,æˆªæ–­ä»¥é¿å…æç«¯æƒ…å†µ)
        if len(cleaned_text) > 500:
            cleaned_text = cleaned_text[:500]

        self.text_clean_cache[text] = cleaned_text
        return cleaned_text

    def process_text(self, texts: List[str], max_length: int = None) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†æ–‡æœ¬ï¼ˆå¸¦æ™ºèƒ½æ¸…æ´—å’Œåœç”¨è¯è¿‡æ»¤ï¼‰

        å¤„ç†æµç¨‹ï¼š
        1. æ™ºèƒ½æ¸…æ´—ï¼ˆå»é™¤å™ªéŸ³ï¼‰
        2. åœç”¨è¯è¿‡æ»¤
        3. BERT tokenization

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            åŒ…å«input_idså’Œattention_maskçš„å­—å…¸
        """
        if max_length is None:
            max_length = self.config.data.max_text_length

        cleaned_texts = []

        for text in texts:
            # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æ¸…æ´—
            cleaned = self.clean_text_smart(text)

            # ç¬¬äºŒæ­¥ï¼šåœç”¨è¯è¿‡æ»¤
            if self.stopwords and cleaned:
                words = jieba.lcut(cleaned)
                words = [w for w in words if w not in self.stopwords and len(w) > 0]
                cleaned = ''.join(words)

            # å¦‚æœæ¸…æ´—åä¸ºç©ºï¼Œä½¿ç”¨å ä½ç¬¦
            if not cleaned or len(cleaned) < 2:
                cleaned = "æ— æœ‰æ•ˆå†…å®¹"

            cleaned_texts.append(cleaned)

        # ç¬¬ä¸‰æ­¥ï¼šBERT tokenization
        encodings = self.tokenizer(
            cleaned_texts,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask']
        }

    # ==================== æ ‡ç­¾å¤„ç†ç›¸å…³æ–¹æ³• ====================

    def build_global_label_graph(self, label_paths: List[str]) -> Dict:
        """
        ä»æ ‡ç­¾è·¯å¾„æ„å»ºå…¨å±€æ ‡ç­¾å›¾
        """
        print("\nğŸ“Š æ„å»ºå…¨å±€æ ‡ç­¾å›¾...")

        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹
        all_nodes = set()
        all_edges = []
        node_levels = {}

        for path_str in tqdm(label_paths, desc="æ‰«ææ ‡ç­¾è·¯å¾„"):
            if pd.isna(path_str) or not path_str:
                continue

            # âœ… ç»Ÿä¸€å¤„ç†ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼ï¼Œç»Ÿä¸€ç®­å¤´ç¬¦å·
            path_str = str(path_str).strip()
            # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„ç®­å¤´ä¸ºæ ‡å‡†ç®­å¤´
            path_str = path_str.replace('-', 'â†’')  # è¿å­—ç¬¦
            path_str = path_str.replace('â€”', 'â†’')  # ç ´æŠ˜å·
            path_str = path_str.replace('â€“', 'â†’')  # en dash

            # åˆ†å‰²è·¯å¾„
            parts = path_str.split('â†’')
            # å»é™¤æ¯ä¸ªéƒ¨åˆ†çš„å‰åç©ºæ ¼
            parts = [p.strip() for p in parts if p.strip()]

            # æ„å»ºç´¯ç§¯è·¯å¾„
            for i in range(len(parts)):
                # ç´¯ç§¯è·¯å¾„ï¼šç§»åŠ¨ä¸šåŠ¡, ç§»åŠ¨ä¸šåŠ¡â†’ç½‘ç»œé—®é¢˜, ç§»åŠ¨ä¸šåŠ¡â†’ç½‘ç»œé—®é¢˜â†’ä¿¡å·è¦†ç›–
                cumulative_path = 'â†’'.join(parts[:i+1])
                all_nodes.add(cumulative_path)
                node_levels[cumulative_path] = i

                # æ·»åŠ çˆ¶å­è¾¹
                if i > 0:
                    parent_path = 'â†’'.join(parts[:i])
                    all_edges.append((parent_path, cumulative_path))
            # âœ… åœ¨è¿™é‡Œæ·»åŠ ä¸€è¡Œå»é‡!!!
            all_edges = list(set(all_edges))  # â† æ·»åŠ è¿™ä¸€è¡Œ!!!
        # æ„å»ºè¯æ±‡è¡¨
        self.node_to_id = {'[PAD]': 0, '[UNK]': 1}
        for node in sorted(all_nodes):
            if node not in self.node_to_id:
                self.node_to_id[node] = len(self.node_to_id)

        self.id_to_node = {v: k for k, v in self.node_to_id.items()}

        # è½¬æ¢è¾¹ä¸ºID
        self.global_edges = []
        for parent, child in all_edges:
            if parent in self.node_to_id and child in self.node_to_id:
                parent_id = self.node_to_id[parent]
                child_id = self.node_to_id[child]
                self.global_edges.append([parent_id, child_id])
                self.global_edges.append([child_id, parent_id])  # åŒå‘è¾¹

        # æ·»åŠ è‡ªç¯
        for node_id in range(len(self.node_to_id)):
            self.global_edges.append([node_id, node_id])

        # ä¿å­˜èŠ‚ç‚¹å±‚çº§
        self.global_node_levels = {}
        for node, level in node_levels.items():
            if node in self.node_to_id:
                self.global_node_levels[self.node_to_id[node]] = level

        print(f"âœ… å…¨å±€æ ‡ç­¾å›¾æ„å»ºå®Œæˆ:")
        print(f"   èŠ‚ç‚¹æ•°: {len(self.node_to_id)}")
        print(f"   è¾¹æ•°: {len(self.global_edges)}")

        # ç»Ÿè®¡å±‚çº§åˆ†å¸ƒ
        level_dist = Counter(node_levels.values())
        print(f"   å±‚çº§åˆ†å¸ƒ: {dict(sorted(level_dist.items()))}")
        # âœ… æ–°å¢: ä¿å­˜edge_indexå’Œnode_levelsä¸ºtensorå±æ€§
        # è¿™æ ·pretrain_label_global_graphå‡½æ•°å°±èƒ½è®¿é—®åˆ°äº†
        self.edge_index = torch.tensor(self.global_edges, dtype=torch.long).t().contiguous()
        self.node_levels = torch.zeros(len(self.node_to_id), dtype=torch.long)
        for node_id, level in self.global_node_levels.items():
            self.node_levels[node_id] = level

        return {
            'vocab_size': len(self.node_to_id),
            'num_edges': len(self.global_edges),
            'level_distribution': dict(level_dist)
        }

    def build_global_ontology_tree(self, label_paths: List[str]) -> Dict:
        """
        æ„å»ºå…¨å±€æœ¬ä½“æ ‘ï¼ˆåˆ«åæ–¹æ³•ï¼Œè°ƒç”¨build_global_label_graphï¼‰

        Args:
            label_paths: æ ‡ç­¾è·¯å¾„åˆ—è¡¨

        Returns:
            åŒ…å«èŠ‚ç‚¹è¯æ±‡è¡¨å’Œè¾¹ä¿¡æ¯çš„å­—å…¸
        """
        return self.build_global_label_graph(label_paths)

    def build_subgraph_labels(self, df: pd.DataFrame, repeat_column='Repeat complaint',
                              label_column='Complaint label', min_samples=5) -> Dict[str, int]:
        """
        ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰ç‡ï¼Œç”¨äºSubgraph Classificationé¢„è®­ç»ƒ

        Args:
            df: åŒ…å«æ ‡ç­¾å’Œé‡å¤æŠ•è¯‰æ ‡æ³¨çš„DataFrameï¼ˆ24ä¸‡æ•°æ®ï¼‰
            repeat_column: é‡å¤æŠ•è¯‰åˆ—å
            label_column: æ ‡ç­¾è·¯å¾„åˆ—å
            min_samples: æœ€å°æ ·æœ¬æ•°é˜ˆå€¼ï¼ˆè¿‡æ»¤æ ·æœ¬è¿‡å°‘çš„è·¯å¾„ï¼‰

        Returns:
            subgraph_labels: {æ ‡ç­¾è·¯å¾„: 0/1} çš„å­—å…¸
                - 0: ä½é£é™©ï¼ˆé‡å¤ç‡ â‰¤ 50%ï¼‰
                - 1: é«˜é£é™©ï¼ˆé‡å¤ç‡ > 50%ï¼‰
        """
        from collections import defaultdict

        print("\nğŸ“Š ç»Ÿè®¡æ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰ç‡...")

        # ç»Ÿè®¡æ¯ä¸ªè·¯å¾„çš„é‡å¤æƒ…å†µ
        path_stats = defaultdict(lambda: {'repeat': 0, 'total': 0})

        for idx, row in df.iterrows():
            if pd.isna(row[label_column]) or pd.isna(row[repeat_column]):
                continue

            # æ ‡å‡†åŒ–æ ‡ç­¾è·¯å¾„ï¼ˆä¸build_global_label_graphä¸€è‡´ï¼‰
            path_str = str(row[label_column]).strip()
            path_str = path_str.replace('-', 'â†’')
            path_str = path_str.replace('â€“', 'â†’')
            path_str = path_str.replace('â€”', 'â†’')

            is_repeat = int(row[repeat_column])

            path_stats[path_str]['total'] += 1
            if is_repeat == 1:
                path_stats[path_str]['repeat'] += 1

        # è®¡ç®—é‡å¤ç‡å¹¶äºŒå€¼åŒ–
        subgraph_labels = {}
        high_risk_count = 0
        low_risk_count = 0
        filtered_count = 0

        for path, stats in path_stats.items():
            # è¿‡æ»¤æ ·æœ¬è¿‡å°‘çš„è·¯å¾„
            if stats['total'] < min_samples:
                filtered_count += 1
                continue

            repeat_rate = stats['repeat'] / stats['total']

            # é‡å¤ç‡ > 50% è®¤ä¸ºæ˜¯é«˜é£é™©
            if repeat_rate > 0.5:
                subgraph_labels[path] = 1
                high_risk_count += 1
            else:
                subgraph_labels[path] = 0
                low_risk_count += 1

        print(f"âœ… å­å›¾æ ‡ç­¾ç»Ÿè®¡å®Œæˆ:")
        print(f"   æ€»è·¯å¾„æ•°: {len(path_stats)}")
        print(f"   è¿‡æ»¤è·¯å¾„æ•°: {filtered_count} (æ ·æœ¬ < {min_samples})")
        print(f"   æœ‰æ•ˆè·¯å¾„æ•°: {len(subgraph_labels)}")
        print(f"   é«˜é£é™©è·¯å¾„: {high_risk_count} ({high_risk_count / len(subgraph_labels) * 100:.1f}%)")
        print(f"   ä½é£é™©è·¯å¾„: {low_risk_count} ({low_risk_count / len(subgraph_labels) * 100:.1f}%)")

        # ä¿å­˜ä¸ºå®ä¾‹å±æ€§
        self.subgraph_labels = subgraph_labels

        return subgraph_labels

    def compute_path_risk_scores(self, df, min_samples=10):
        """
        ç»Ÿè®¡æ¯æ¡æ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰é£é™©åˆ†æ•°

        ç”¨é€”ï¼šLabelé¢„è®­ç»ƒ - è·¯å¾„é£é™©å›å½’ä»»åŠ¡

        ä»å¤§è§„æ¨¡æ•°æ®ï¼ˆ24ä¸‡ï¼‰ä¸­è®¡ç®—æ¯æ¡è·¯å¾„çš„å†å²é‡å¤ç‡ï¼š
        - è·¯å¾„Aå‡ºç°100æ¬¡ï¼Œå…¶ä¸­35æ¬¡é‡å¤ â†’ é£é™©åˆ†æ•°0.35
        - è·¯å¾„Bå‡ºç°50æ¬¡ï¼Œå…¶ä¸­6æ¬¡é‡å¤ â†’ é£é™©åˆ†æ•°0.12

        è¿™äº›é£é™©åˆ†æ•°å°†ä½œä¸ºå›å½’ç›®æ ‡ï¼Œè®­ç»ƒGATå­¦ä¼šè¯†åˆ«é«˜é£é™©è·¯å¾„æ¨¡å¼

        Args:
            df: DataFrameï¼ŒåŒ…å«'Complaint label'å’Œ'Repeat complaint'åˆ—
            min_samples: æœ€å°æ ·æœ¬æ•°é˜ˆå€¼ï¼Œæ ·æœ¬æ•°å°‘äºæ­¤å€¼çš„è·¯å¾„ä¸ç»Ÿè®¡

        Returns:
            dict: {è·¯å¾„å­—ç¬¦ä¸²: é£é™©åˆ†æ•°(0-1)}
        """
        from collections import defaultdict
        import numpy as np

        print("\n" + "=" * 70)
        print("ğŸ“Š ç»Ÿè®¡æ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰é£é™©åˆ†æ•°")
        print("=" * 70)

        path_stats = defaultdict(lambda: {'repeat': 0, 'total': 0})

        # éå†æ¯æ¡æ•°æ®ï¼Œç»Ÿè®¡æ¯æ¡è·¯å¾„çš„é‡å¤æƒ…å†µ
        for idx, row in df.iterrows():
            if pd.isna(row['Complaint label']):
                continue

            path = str(row['Complaint label']).strip()

            # æ£€æŸ¥æ˜¯å¦é‡å¤æŠ•è¯‰
            try:
                is_repeat = int(row['Repeat complaint'])
            except (ValueError, KeyError):
                continue

            path_stats[path]['total'] += 1
            if is_repeat == 1:
                path_stats[path]['repeat'] += 1

        # è®¡ç®—é£é™©åˆ†æ•°ï¼ˆåªä¿ç•™æ ·æœ¬æ•°å……è¶³çš„è·¯å¾„ï¼‰
        path_risk_scores = {}
        filtered_count = 0

        for path, stats in path_stats.items():
            if stats['total'] >= min_samples:
                # è®¡ç®—è¯¥è·¯å¾„çš„å†å²é‡å¤ç‡
                risk_score = stats['repeat'] / stats['total']
                path_risk_scores[path] = risk_score
            else:
                filtered_count += 1

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ“ ç»Ÿè®¡å®Œæˆ:")
        print(f"  - æ€»è·¯å¾„æ•°: {len(path_stats)}")
        print(f"  - æœ‰æ•ˆè·¯å¾„æ•° (æ ·æœ¬â‰¥{min_samples}): {len(path_risk_scores)}")
        print(f"  - è¿‡æ»¤è·¯å¾„æ•° (æ ·æœ¬<{min_samples}): {filtered_count}")

        if path_risk_scores:
            scores = list(path_risk_scores.values())
            print(f"\nğŸ“ˆ é£é™©åˆ†æ•°åˆ†å¸ƒ:")
            print(f"  - æœ€å°å€¼: {min(scores):.4f}")
            print(f"  - æœ€å¤§å€¼: {max(scores):.4f}")
            print(f"  - å¹³å‡å€¼: {np.mean(scores):.4f}")
            print(f"  - ä¸­ä½æ•°: {np.median(scores):.4f}")

            # ç»Ÿè®¡é£é™©ç­‰çº§åˆ†å¸ƒ
            low_risk = sum(1 for s in scores if s < 0.1)
            mid_risk = sum(1 for s in scores if 0.1 <= s < 0.3)
            high_risk = sum(1 for s in scores if s >= 0.3)

            print(f"\nğŸ¯ é£é™©ç­‰çº§åˆ†å¸ƒ:")
            print(f"  - ä½é£é™© (<10%):  {low_risk} æ¡è·¯å¾„ ({low_risk / len(scores) * 100:.1f}%)")
            print(f"  - ä¸­é£é™© (10-30%): {mid_risk} æ¡è·¯å¾„ ({mid_risk / len(scores) * 100:.1f}%)")
            print(f"  - é«˜é£é™© (â‰¥30%):  {high_risk} æ¡è·¯å¾„ ({high_risk / len(scores) * 100:.1f}%)")

        print("=" * 70 + "\n")

        return path_risk_scores
    def encode_label_path_as_graph(self, label: str) -> Tuple[List[int], List[List[int]], List[int]]:
        """
        å°†æ ‡ç­¾è·¯å¾„ç¼–ç ä¸ºå­å›¾ - ä¿®å¤ç‰ˆ

        Args:
            label: æ ‡ç­¾è·¯å¾„å­—ç¬¦ä¸²

        Returns:
            (èŠ‚ç‚¹IDåˆ—è¡¨, è¾¹åˆ—è¡¨, èŠ‚ç‚¹å±‚çº§åˆ—è¡¨)
        """
        # ========== æºå¤´é¢„é˜²: æ•°æ®éªŒè¯å’Œé»˜è®¤å€¼ ==========
        if pd.isna(label) or not self.node_to_id:
            # è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„æœ€å°å›¾ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰
            return [0], [], [0]

        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
        label_str = str(label).strip()
        if label_str == '' or label_str.lower() in ['nan', 'none', 'æœªçŸ¥', 'æ— ']:
            return [0], [], [0]

        # âœ… ç»Ÿä¸€å¤„ç†ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼ï¼Œç»Ÿä¸€ç®­å¤´ç¬¦å·
        label = str(label).strip()
        # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„ç®­å¤´ä¸ºæ ‡å‡†ç®­å¤´
        label = label.replace('-', 'â†’')  # è¿å­—ç¬¦
        label = label.replace('â€”', 'â†’')  # ç ´æŠ˜å·
        label = label.replace('â€“', 'â†’')  # en dash

        # åˆ†å‰²è·¯å¾„
        parts = label.split('â†’')
        # å»é™¤æ¯ä¸ªéƒ¨åˆ†çš„å‰åç©ºæ ¼
        parts = [p.strip() for p in parts if p.strip()]

        node_ids = []
        node_levels = []

        # æ„å»ºè·¯å¾„èŠ‚ç‚¹
        for i in range(len(parts)):
            cumulative_path = 'â†’'.join(parts[:i + 1])
            if cumulative_path in self.node_to_id:
                node_ids.append(self.node_to_id[cumulative_path])
                node_levels.append(i)
            else:
                # âœ… æ‰¾ä¸åˆ°èŠ‚ç‚¹æ—¶çš„å¤„ç†
                # ä½¿ç”¨ [UNK] ä»£æ›¿ç¼ºå¤±çš„èŠ‚ç‚¹
                node_ids.append(1)  # [UNK]çš„IDæ˜¯1
                node_levels.append(i)
                # å¯é€‰ï¼šæ‰“å°è­¦å‘Šï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡é‡åˆ°æ—¶æ‰“å°ï¼‰
                if not hasattr(self, '_warned_missing_nodes'):
                    self._warned_missing_nodes = set()
                if cumulative_path not in self._warned_missing_nodes:
                    print(f"âš ï¸  èŠ‚ç‚¹ä¸åœ¨è¯æ±‡è¡¨ä¸­: {cumulative_path}")
                    self._warned_missing_nodes.add(cumulative_path)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½æ˜¯[UNK]ï¼‰
        if not node_ids or all(nid == 1 for nid in node_ids):
            return [1], [], [0]  # è¿”å›å•ä¸ª[UNK]

        # æ„å»ºè·¯å¾„è¾¹ï¼ˆçˆ¶â†’å­ï¼Œå­â†’çˆ¶ï¼‰
        edges = []
        for i in range(len(node_ids) - 1):
            edges.append([i, i + 1])
            edges.append([i + 1, i])

        # æ·»åŠ è‡ªç¯
        for i in range(len(node_ids)):
            edges.append([i, i])

        return node_ids, edges, node_levels

    def save_vocabulary(self, save_path: str):
        """
        ä¿å­˜è¯æ±‡è¡¨åˆ°æ–‡ä»¶

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        vocab_data = {
            'node_to_id': self.node_to_id,
            'id_to_node': self.id_to_node,
            'global_edges': self.global_edges,
            'global_node_levels': self.global_node_levels
        }

        with open(save_path, 'wb') as f:
            pickle.dump(vocab_data, f)

        print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜: {save_path}")
        print(f"   èŠ‚ç‚¹æ•°: {len(self.node_to_id)}")

    def save_global_vocab(self, save_path: str):
        """
        ä¿å­˜å…¨å±€è¯æ±‡è¡¨ï¼ˆåˆ«åæ–¹æ³•ï¼‰

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        self.save_vocabulary(save_path)

    def load_vocabulary(self, load_path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½è¯æ±‡è¡¨

        Args:
            load_path: åŠ è½½è·¯å¾„

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not os.path.exists(load_path):
            print(f"âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False

        try:
            with open(load_path, 'rb') as f:
                vocab_data = pickle.load(f)

            self.node_to_id = vocab_data['node_to_id']
            self.id_to_node = vocab_data['id_to_node']
            self.global_edges = vocab_data['global_edges']
            self.global_node_levels = vocab_data['global_node_levels']

            print(f"âœ“ ä»processor.pklåŠ è½½è¯æ±‡è¡¨: {load_path}")
            print(f"   èŠ‚ç‚¹æ•°: {len(self.node_to_id)}")

            # âœ… æ–°å¢: åŒæ—¶æ„å»ºedge_indexå’Œnode_levels tensor
            if self.global_edges:
                self.edge_index = torch.tensor(self.global_edges, dtype=torch.long).t().contiguous()
            else:
                self.edge_index = torch.empty((2, 0), dtype=torch.long)

            if self.global_node_levels:
                self.node_levels = torch.zeros(len(self.node_to_id), dtype=torch.long)
                for node_id, level in self.global_node_levels.items():
                    self.node_levels[node_id] = level
            else:
                self.node_levels = torch.zeros(len(self.node_to_id), dtype=torch.long)

            return True

        except Exception as e:
            print(f"âŒ åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {str(e)}")
            return False

    def load_global_vocab(self, load_path: str) -> bool:
        """
        åŠ è½½å…¨å±€è¯æ±‡è¡¨ï¼ˆåˆ«åæ–¹æ³•ï¼‰

        Args:
            load_path: åŠ è½½è·¯å¾„

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        return self.load_vocabulary(load_path)

    # ==================== ç»“æ„åŒ–ç‰¹å¾å¤„ç† ====================

    def process_structured_features(self, df: pd.DataFrame) -> torch.Tensor:
        """
        å¤„ç†ç»“æ„åŒ–ç‰¹å¾ï¼ˆ53ä¸ªç‰¹å¾ï¼‰- ä¿®å¤ç‰ˆæœ¬

        æ•°æ®æ ¼å¼:
        - Aåˆ—(ç´¢å¼•0): new_code - IDåˆ—ï¼Œã€æ’é™¤ã€‘
        - Båˆ—(ç´¢å¼•1): biz_cntt - æŠ•è¯‰æ–‡æœ¬
        - Cåˆ—(ç´¢å¼•2): Complaint label - æŠ•è¯‰æ ‡ç­¾
        - Dåˆ—(ç´¢å¼•3)~BDåˆ—(ç´¢å¼•55): ç»“æ„åŒ–ç‰¹å¾ (53åˆ—) ã€ä½¿ç”¨è¿™äº›ã€‘
        - BEåˆ—(ç´¢å¼•56): Repeat complaint - ç›®æ ‡å˜é‡
        """
        print("\nğŸ“Š å¤„ç†ç»“æ„åŒ–ç‰¹å¾...")

        col_names = df.columns.tolist()

        # ã€æ ¸å¿ƒä¿®å¤ã€‘æ˜¾å¼å®šä¹‰è¦æ’é™¤çš„åˆ—
        exclude_cols = {'new_code', 'biz_cntt', 'Complaint label', 'Repeat complaint'}

        # æ–¹æ³•1: é€šè¿‡å…³é”®åˆ—åå®šä½
        if 'Complaint label' in col_names and 'Repeat complaint' in col_names:
            label_idx = col_names.index('Complaint label')
            target_idx = col_names.index('Repeat complaint')

            start_idx = label_idx + 1
            end_idx = target_idx

            feature_cols = col_names[start_idx:end_idx]

            print(f"âœ“ æ–¹æ³•1: ä» '{col_names[start_idx]}' åˆ° '{col_names[end_idx - 1]}'")
            print(f"  èµ·å§‹ç´¢å¼•: {start_idx}, ç»“æŸç´¢å¼•: {end_idx}")

        # æ–¹æ³•2: ä½¿ç”¨å›ºå®šåˆ—ç´¢å¼•
        elif len(col_names) >= 57:
            start_idx = 3
            end_idx = 56
            feature_cols = col_names[start_idx:end_idx]
            print(f"âœ“ æ–¹æ³•2: ä½¿ç”¨å›ºå®šç´¢å¼• [{start_idx}:{end_idx}]")
        else:
            raise ValueError(f"æ•°æ®é›†åˆ—æ•°ä¸è¶³: {len(col_names)} < 57")

        # äºŒæ¬¡è¿‡æ»¤
        feature_cols = [col for col in feature_cols if col not in exclude_cols]

        print(f"  å®é™…ç‰¹å¾åˆ—æ•°: {len(feature_cols)}")

        if len(feature_cols) != 53:
            print(f"âš ï¸ è­¦å‘Š: é¢„æœŸ53åˆ—ï¼Œå®é™…{len(feature_cols)}åˆ—")
            if len(feature_cols) > 53:
                feature_cols = feature_cols[:53]
                print(f"  å·²æˆªæ–­ä¸º53åˆ—")
            elif len(feature_cols) < 53:
                raise ValueError(f"ç»“æ„åŒ–ç‰¹å¾åˆ—æ•°ä¸è¶³: {len(feature_cols)} < 53")

        print(f"  å‰5åˆ—: {feature_cols[:5]}")
        print(f"  å5åˆ—: {feature_cols[-5:]}")

        if 'new_code' in feature_cols:
            print("âŒ é”™è¯¯: new_code åˆ—è¢«é”™è¯¯åŒ…å«ï¼Œæ­£åœ¨ç§»é™¤...")
            feature_cols = [col for col in feature_cols if col != 'new_code']

        features = df[feature_cols].values
        print(f"âœ“ æå–ç‰¹å¾çŸ©é˜µ: {features.shape}")

        features = np.nan_to_num(features, nan=0.0)

        if not hasattr(self.scaler, 'mean_') or self.scaler.mean_ is None:
            features = self.scaler.fit_transform(features)
            print("âœ“ é¦–æ¬¡æ ‡å‡†åŒ–ï¼ˆfit_transformï¼‰")
        else:
            features = self.scaler.transform(features)
            print("âœ“ æ ‡å‡†åŒ–ï¼ˆtransformï¼‰")

        print(f"âœ… ç»“æ„åŒ–ç‰¹å¾å¤„ç†å®Œæˆ: {features.shape}")

        return torch.FloatTensor(features)

    # ==================== å®Œæ•´æ•°æ®åŠ è½½ ====================

    def load_data(self, data_file: str, for_pretrain: bool = False):
        """
        åŠ è½½æ•°æ®æ–‡ä»¶(è¿”å›DataFrame)
        â­ main.pyéœ€è¦çš„æ–¹æ³•

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            for_pretrain: æ˜¯å¦ç”¨äºé¢„è®­ç»ƒ

        Returns:
            pd.DataFrame: æ•°æ®æ¡†
        """
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")

        # è¯»å–æ•°æ®
        if data_file.endswith('.xlsx'):
            df = pd.read_excel(data_file)
        elif data_file.endswith('.csv'):
            df = pd.read_csv(data_file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file}")

        print(f"âœ“ æ•°æ®é›†å¤§å°: {len(df)}")

        if 'Repeat complaint' in df.columns:
            print(f"âœ“ é‡å¤æŠ•è¯‰æ¯”ä¾‹: {df['Repeat complaint'].mean()*100:.2f}%")

        return df

    def prepare_datasets(self, train_file: str = None, pretrain_file: str = None,
                        for_pretrain: bool = False):
        """
        å‡†å¤‡è®­ç»ƒ/é¢„è®­ç»ƒæ•°æ®é›†
        â­ main.pyéœ€è¦çš„æ–¹æ³•

        Args:
            train_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            pretrain_file: é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            for_pretrain: æ˜¯å¦ç”¨äºé¢„è®­ç»ƒ

        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        # ç¡®å®šä½¿ç”¨å“ªä¸ªæ–‡ä»¶
        data_file = pretrain_file if for_pretrain else train_file

        if data_file is None:
            raise ValueError("å¿…é¡»æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")

        print(f"\n{'='*60}")
        print(f"ğŸ“Š å‡†å¤‡{'é¢„è®­ç»ƒ' if for_pretrain else 'è®­ç»ƒ'}æ•°æ®é›†")
        print(f"{'='*60}")
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")

        # åŠ è½½æ•°æ®
        df = self.load_data(data_file, for_pretrain)

        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = df['biz_cntt'].fillna('').astype(str).tolist()
        labels = df['Complaint label'].fillna('').astype(str).tolist()

        print(f"\nå¤„ç†æ–‡æœ¬å’Œæ ‡ç­¾...")
        print(f"  æ–‡æœ¬æ•°: {len(texts)}")
        print(f"  æ ‡ç­¾æ•°: {len(labels)}")

        # å¦‚æœæ²¡æœ‰è¯æ±‡è¡¨,æ„å»ºå…¨å±€æ ‡ç­¾å›¾
        if len(self.node_to_id) == 0:
            print("\nâš ï¸ è¯æ±‡è¡¨ä¸ºç©º,æ„å»ºæ–°çš„å…¨å±€æ ‡ç­¾å›¾...")
            self.build_global_label_graph(labels)
        else:
            print(f"\nâœ“ ä½¿ç”¨å·²åŠ è½½çš„è¯æ±‡è¡¨: {len(self.node_to_id)}ä¸ªèŠ‚ç‚¹")

        # ç¼–ç æ–‡æœ¬ - ä½¿ç”¨æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
        print("\nç¼–ç æ–‡æœ¬(æ‰¹é‡å¤„ç†)...")

        # æ­¥éª¤1: æ‰¹é‡æ¸…æ´—æ–‡æœ¬
        cleaned_texts = []
        for text in tqdm(texts, desc="æ¸…æ´—æ–‡æœ¬"):
            cleaned_texts.append(self.clean_text_smart(text))

        # æ­¥éª¤2: åˆ†æ‰¹ç¼–ç ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        batch_size = 1000
        all_input_ids = []
        all_attention_masks = []

        for i in tqdm(range(0, len(cleaned_texts), batch_size), desc="æ‰¹é‡ç¼–ç "):
            batch_texts = cleaned_texts[i:i + batch_size]

            # æ‰¹é‡BERTç¼–ç 
            batch_encoding = self.tokenizer(
                batch_texts,
                max_length=self.config.model.bert_max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            all_input_ids.append(batch_encoding['input_ids'])
            all_attention_masks.append(batch_encoding['attention_mask'])

        # æ­¥éª¤3: åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        text_data = {
            'input_ids': torch.cat(all_input_ids, dim=0),
            'attention_mask': torch.cat(all_attention_masks, dim=0)
        }

        print(f"âœ“ æ–‡æœ¬ç¼–ç å®Œæˆ: {text_data['input_ids'].shape}")

        # ç¼–ç æ ‡ç­¾ä¸ºå›¾
        print("\nç¼–ç æ ‡ç­¾ä¸ºå›¾ç»“æ„...")
        node_ids_list = []
        edges_list = []
        node_levels_list = []

        for label in tqdm(labels, desc="ç¼–ç æ ‡ç­¾å›¾"):
            node_ids, edges, levels = self.encode_label_path_as_graph(label)
            node_ids_list.append(node_ids)
            edges_list.append(edges)
            node_levels_list.append(levels)

        # å¦‚æœä¸æ˜¯é¢„è®­ç»ƒ,å¤„ç†ç»“æ„åŒ–ç‰¹å¾å’Œç›®æ ‡å˜é‡
        if not for_pretrain:
            print("\nå¤„ç†ç»“æ„åŒ–ç‰¹å¾å’Œç›®æ ‡å˜é‡...")
            struct_features = self.process_structured_features(df)

            if 'Repeat complaint' in df.columns:
                targets = torch.LongTensor(df['Repeat complaint'].values)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡å˜é‡åˆ—'Repeat complaint',ä½¿ç”¨å…¨0")
                targets = torch.zeros(len(df), dtype=torch.long)
        else:
            # é¢„è®­ç»ƒæ—¶åˆ›å»ºå ä½ç¬¦
            struct_features = torch.zeros((len(df), 53), dtype=torch.float32)
            targets = torch.zeros(len(df), dtype=torch.long)

        print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   æ–‡æœ¬æ•°æ®: {text_data['input_ids'].shape}")
        print(f"   æ ‡ç­¾æ•°æ®: {len(node_ids_list)}ä¸ªå›¾")
        print(f"   ç»“æ„åŒ–ç‰¹å¾: {struct_features.shape}")
        print(f"   ç›®æ ‡å˜é‡: {targets.shape}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.node_to_id)}")

        return {
            'text_data': text_data,
            'node_ids_list': node_ids_list,
            'edges_list': edges_list,
            'node_levels_list': node_levels_list,
            'struct_features': struct_features,
            'targets': targets,
            'vocab_size': len(self.node_to_id)
        }

    def load_and_process_data(self, data_file: str, text_col='biz_cntt',
                              label_col='Complaint label', target_col='Repeat complaint'):
        """
        åŠ è½½å¹¶å¤„ç†å®Œæ•´æ•°æ®é›†ï¼ˆæ–‡æœ¬ä¼šè¢«æ™ºèƒ½æ¸…æ´—ï¼‰

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            text_col: æ–‡æœ¬åˆ—å
            label_col: æ ‡ç­¾åˆ—å
            target_col: ç›®æ ‡å˜é‡åˆ—å

        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {data_file}")

        # è¯»å–æ•°æ®
        if data_file.endswith('.xlsx'):
            df = pd.read_excel(data_file)
        elif data_file.endswith('.csv'):
            df = pd.read_csv(data_file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file}")

        print(f"æ•°æ®é›†å¤§å°: {len(df)}")
        print(f"é‡å¤æŠ•è¯‰æ¯”ä¾‹: {df[target_col].mean()*100:.2f}%")

        # 1. å¤„ç†æ–‡æœ¬ï¼ˆä¼šè‡ªåŠ¨æ™ºèƒ½æ¸…æ´—ï¼‰
        print("\nå¤„ç†æ–‡æœ¬å’Œæ ‡ç­¾...")
        texts = df[text_col].fillna('').astype(str).tolist()

        # 2. å¤„ç†æ ‡ç­¾
        labels = df[label_col].fillna('').astype(str).tolist()

        # å¦‚æœæ²¡æœ‰è¯æ±‡è¡¨ï¼Œæ„å»ºå…¨å±€æ ‡ç­¾å›¾
        if len(self.node_to_id) == 0:
            print("\næ„å»ºæ–°çš„å…¨å±€æ ‡ç­¾å›¾...")
            self.build_global_label_graph(labels)
        else:
            print(f"\nä½¿ç”¨å·²åŠ è½½çš„è¯æ±‡è¡¨: {len(self.node_to_id)}ä¸ªèŠ‚ç‚¹")

        # 3. å¤„ç†ç»“æ„åŒ–ç‰¹å¾
        struct_features = self.process_structured_features(df)

        # 4. ç¼–ç æ ‡ç­¾ä¸ºå›¾
        print("\nç¼–ç æ ‡ç­¾ä¸ºå›¾ç»“æ„...")
        encoded_labels = []
        for label in tqdm(labels, desc="ç¼–ç æ ‡ç­¾å›¾"):
            node_ids, edges, levels = self.encode_label_path_as_graph(label)
            encoded_labels.append({
                'node_ids': node_ids,
                'edges': edges,
                'node_levels': levels
            })

        # 5. å¤„ç†ç›®æ ‡å˜é‡
        targets = df[target_col].values

        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆ:")
        print(f"   æ–‡æœ¬æ•°: {len(texts)}")
        print(f"   æ ‡ç­¾æ•°: {len(labels)}")
        print(f"   ç»“æ„åŒ–ç‰¹å¾: {struct_features.shape}")
        print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ: {Counter(targets)}")

        return {
            'texts': texts,
            'labels': labels,
            'encoded_labels': encoded_labels,
            'struct_features': struct_features,
            'targets': targets,
            'dataframe': df
        }

    def get_vocab_size(self) -> int:
        """è¿”å›æ ‡ç­¾è¯æ±‡è¡¨å¤§å°"""
        return len(self.node_to_id) if self.node_to_id else 0

    def save(self, save_path: str):
        """
        ä¿å­˜å¤„ç†å™¨çŠ¶æ€

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        state = {
            'node_to_id': self.node_to_id,
            'id_to_node': self.id_to_node,
            'global_edges': self.global_edges,
            'global_node_levels': self.global_node_levels,
            'scaler_mean': self.scaler.mean_ if hasattr(self.scaler, 'mean_') else None,
            'scaler_scale': self.scaler.scale_ if hasattr(self.scaler, 'scale_') else None,
        }

        with open(save_path, 'wb') as f:
            pickle.dump(state, f)

        print(f"âœ… å¤„ç†å™¨çŠ¶æ€å·²ä¿å­˜: {save_path}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.node_to_id)}")

    def load(self, load_path: str) -> bool:
        """
        åŠ è½½å¤„ç†å™¨çŠ¶æ€

        Args:
            load_path: åŠ è½½è·¯å¾„

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not os.path.exists(load_path):
            print(f"âš ï¸ å¤„ç†å™¨çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False

        try:
            with open(load_path, 'rb') as f:
                state = pickle.load(f)

            self.node_to_id = state['node_to_id']
            self.id_to_node = state['id_to_node']
            self.global_edges = state['global_edges']
            self.global_node_levels = state['global_node_levels']

            if state['scaler_mean'] is not None:
                self.scaler.mean_ = state['scaler_mean']
                self.scaler.scale_ = state['scaler_scale']

            print(f"âœ… å¤„ç†å™¨çŠ¶æ€å·²åŠ è½½: {load_path}")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.node_to_id)}")

            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å¤„ç†å™¨çŠ¶æ€å¤±è´¥: {str(e)}")
            return False


# ==================== ComplaintDatasetç±» ====================
# â­ å¿…é¡»å­˜åœ¨! main.py, train.py, ablation_study.pyéƒ½éœ€è¦å¯¼å…¥è¿™ä¸ªç±»

class ComplaintDataset(Dataset):
    """
    æŠ•è¯‰æ•°æ®é›†ç±»

    è¿™ä¸ªç±»å°è£…æ•°æ®ä¾›DataLoaderä½¿ç”¨,æ˜¯PyTorchæ•°æ®åŠ è½½çš„æ ‡å‡†æ–¹å¼
    """

    def __init__(self, text_data, node_ids_list, edges_list, node_levels_list,
                 struct_features, targets=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            text_data: æ–‡æœ¬æ•°æ®å­—å…¸,åŒ…å«input_idså’Œattention_mask
            node_ids_list: èŠ‚ç‚¹IDåˆ—è¡¨(æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªåˆ—è¡¨)
            edges_list: è¾¹åˆ—è¡¨(æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªåˆ—è¡¨)
            node_levels_list: èŠ‚ç‚¹å±‚çº§åˆ—è¡¨(æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªåˆ—è¡¨)
            struct_features: ç»“æ„åŒ–ç‰¹å¾å¼ é‡ [N, 53]
            targets: ç›®æ ‡æ ‡ç­¾(å¯é€‰)
        """
        self.input_ids = text_data['input_ids']
        self.attention_mask = text_data['attention_mask']
        self.node_ids_list = node_ids_list
        self.edges_list = edges_list
        self.node_levels_list = node_levels_list
        self.struct_features = struct_features
        self.targets = targets

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.input_ids)

    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬

        Args:
            idx: æ ·æœ¬ç´¢å¼•

        Returns:
            åŒ…å«æ‰€æœ‰æ¨¡æ€æ•°æ®çš„å­—å…¸
        """
        item = {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'node_ids': self.node_ids_list[idx],
            'edges': self.edges_list[idx],
            'node_levels': self.node_levels_list[idx],
            'struct_features': self.struct_features[idx]
        }

        if self.targets is not None:
            item['target'] = torch.tensor(self.targets[idx], dtype=torch.long)

        return item


# ==================== custom_collate_fnå‡½æ•° ====================
# â­ å¿…é¡»å­˜åœ¨! å¤„ç†å˜é•¿å›¾æ•°æ®batchingçš„å…³é”®å‡½æ•°

def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰æ‰¹æ¬¡æ•´ç†å‡½æ•°

    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‡½æ•°?
    å› ä¸ºæ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾è·¯å¾„é•¿åº¦ä¸åŒ(æœ‰çš„3å±‚,æœ‰çš„5å±‚),
    æ ‡å‡†çš„DataLoaderæ— æ³•è‡ªåŠ¨å¤„ç†å˜é•¿çš„å›¾æ•°æ®,
    æ‰€ä»¥éœ€è¦è¿™ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥æ­£ç¡®ç»„batch

    Args:
        batch: æ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬æ˜¯__getitem__è¿”å›çš„å­—å…¸

    Returns:
        æ•´ç†å¥½çš„batchå­—å…¸
    """
    # æå–batchä¸­çš„æ‰€æœ‰æ•°æ®
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    node_ids_list = [item['node_ids'] for item in batch]
    edges_list = [item['edges'] for item in batch]
    node_levels_list = [item['node_levels'] for item in batch]
    struct_features = torch.stack([item['struct_features'] for item in batch])

    # å¤„ç†target(å¦‚æœå­˜åœ¨)
    if 'target' in batch[0]:
        targets = torch.stack([item['target'] for item in batch])
    else:
        targets = None

    # å¯¹æ–‡æœ¬è¿›è¡Œpadding(ä½¿batchå†…æ‰€æœ‰åºåˆ—é•¿åº¦ç›¸åŒ)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)

    # ç»„è£…è¿”å›çš„batch
    batched_data = {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'node_ids': node_ids_list,  # ä¿æŒä¸ºlist,å› ä¸ºæ¯ä¸ªæ ·æœ¬é•¿åº¦ä¸åŒ
        'edges': edges_list,  # ä¿æŒä¸ºlist
        'node_levels': node_levels_list,  # ä¿æŒä¸ºlist
        'struct_features': struct_features
    }

    if targets is not None:
        batched_data['target'] = targets

    return batched_data


# ============================================================
# æ–°å¢: å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨
# ä½œç”¨: ç¡®ä¿æ¯ä¸ªbatchä¸­æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åˆç†
# ç”¨äº: Texté¢„è®­ç»ƒé˜¶æ®µ2 (å¯¹æ¯”å­¦ä¹ )
# ä½ç½®: åœ¨ if __name__ == "__main__": ä¹‹å‰æ’å…¥
# ============================================================

class BalancedBatchSampler:
    """
    å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨

    ç›®çš„:
        è§£å†³å¯¹æ¯”å­¦ä¹ ä¸­çš„æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜

    åŸå§‹æ•°æ®åˆ†å¸ƒ:
        - é‡å¤æŠ•è¯‰: 7.31% (17,000æ¡)
        - éé‡å¤: 92.69% (223,000æ¡)

    ä½¿ç”¨BalancedBatchSampler:
        - å¼ºåˆ¶: æ¯batch 30%é‡å¤ + 70%éé‡å¤
        - å®é™…: æ¯batch 19ä¸ªé‡å¤ + 45ä¸ªéé‡å¤
        - æ­£æ ·æœ¬å¯¹: C(19,2)=171å¯¹ (å……è¶³!)
    """

    def __init__(self, labels, batch_size=64, pos_ratio=0.3, shuffle=True):
        """
        Args:
            labels: æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾åˆ—è¡¨ (0æˆ–1)
            batch_size: æ‰¹æ¬¡å¤§å°
            pos_ratio: æ­£æ ·æœ¬(é‡å¤æŠ•è¯‰)å æ¯”ï¼Œæ¨è0.3
            shuffle: æ˜¯å¦æ¯ä¸ªepochæ‰“ä¹±
        """
        self.labels = np.array(labels)
        self.batch_size = batch_size
        self.pos_ratio = pos_ratio
        self.shuffle = shuffle

        # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬çš„ç´¢å¼•
        self.pos_indices = np.where(self.labels == 1)[0]  # é‡å¤æŠ•è¯‰ç´¢å¼•
        self.neg_indices = np.where(self.labels == 0)[0]  # éé‡å¤ç´¢å¼•

        self.num_pos = len(self.pos_indices)
        self.num_neg = len(self.neg_indices)

        # è®¡ç®—æ¯ä¸ªbatchéœ€è¦çš„æ­£è´Ÿæ ·æœ¬æ•°
        self.num_pos_per_batch = int(self.batch_size * self.pos_ratio)
        self.num_neg_per_batch = self.batch_size - self.num_pos_per_batch

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"âœ… BalancedBatchSampler åˆå§‹åŒ–:")
        print(f"  - æ€»æ ·æœ¬: {len(labels)}")
        print(f"  - æ­£æ ·æœ¬(é‡å¤): {self.num_pos} ({self.num_pos / len(labels) * 100:.2f}%)")
        print(f"  - è´Ÿæ ·æœ¬(éé‡å¤): {self.num_neg} ({self.num_neg / len(labels) * 100:.2f}%)")
        print(f"  - Batch size: {batch_size}")
        print(f"  - æ¯batchæ­£æ ·æœ¬: {self.num_pos_per_batch} ({pos_ratio * 100:.0f}%)")
        print(f"  - æ¯batchè´Ÿæ ·æœ¬: {self.num_neg_per_batch} ({(1 - pos_ratio) * 100:.0f}%)")

        # è®¡ç®—æ­£æ ·æœ¬å¯¹æ•°é‡
        pos_pairs = self.num_pos_per_batch * (self.num_pos_per_batch - 1) // 2
        neg_pairs = self.num_neg_per_batch * (self.num_neg_per_batch - 1) // 2
        cross_pairs = self.num_pos_per_batch * self.num_neg_per_batch

        print(f"  - æ¯batchæ ·æœ¬å¯¹:")
        print(f"    * æ­£ç±»å†…å¯¹: {pos_pairs}å¯¹")
        print(f"    * è´Ÿç±»å†…å¯¹: {neg_pairs}å¯¹")
        print(f"    * è·¨ç±»å¯¹: {cross_pairs}å¯¹")

    def __iter__(self):
        """ç”Ÿæˆbalanced batchçš„è¿­ä»£å™¨"""

        # æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±ç´¢å¼•
        if self.shuffle:
            np.random.shuffle(self.pos_indices)
            np.random.shuffle(self.neg_indices)

        pos_ptr = 0  # æ­£æ ·æœ¬æŒ‡é’ˆ
        neg_ptr = 0  # è´Ÿæ ·æœ¬æŒ‡é’ˆ

        # æŒç»­ç”Ÿæˆbatchç›´åˆ°æŸä¸€ç±»æ ·æœ¬ç”¨å®Œ
        while (pos_ptr + self.num_pos_per_batch <= self.num_pos and
               neg_ptr + self.num_neg_per_batch <= self.num_neg):
            batch_indices = []

            # é‡‡æ ·æ­£æ ·æœ¬
            pos_batch = self.pos_indices[pos_ptr:pos_ptr + self.num_pos_per_batch]
            batch_indices.extend(pos_batch.tolist())
            pos_ptr += self.num_pos_per_batch

            # é‡‡æ ·è´Ÿæ ·æœ¬
            neg_batch = self.neg_indices[neg_ptr:neg_ptr + self.num_neg_per_batch]
            batch_indices.extend(neg_batch.tolist())
            neg_ptr += self.num_neg_per_batch

            # æ‰“ä¹±batchå†…çš„é¡ºåºï¼ˆè®©æ­£è´Ÿæ ·æœ¬æ··åˆï¼‰
            np.random.shuffle(batch_indices)

            yield batch_indices

    def __len__(self):
        """ä¼°ç®—batchæ•°é‡"""
        # å–å†³äºå“ªä¸ªæ ·æœ¬ç±»å‹å…ˆç”¨å®Œ
        num_batches_pos = self.num_pos // self.num_pos_per_batch
        num_batches_neg = self.num_neg // self.num_neg_per_batch
        return min(num_batches_pos, num_batches_neg)


# ============================================================
# æ–°å¢: å¯¹æ¯”å­¦ä¹ æ–‡æœ¬æ•°æ®é›†
# ä½œç”¨: å°è£…æ–‡æœ¬æ•°æ®ç”¨äºå¯¹æ¯”å­¦ä¹ 
# ç”¨äº: Texté¢„è®­ç»ƒé˜¶æ®µ2
# ============================================================

class ContrastiveTextDataset(Dataset):
    """
    ç”¨äºå¯¹æ¯”å­¦ä¹ çš„æ–‡æœ¬æ•°æ®é›†

    ä¸æ™®é€šåˆ†ç±»Datasetçš„åŒºåˆ«:
        æ™®é€š: è¿”å› (text, label) ç”¨äºè®¡ç®—CE loss
        å¯¹æ¯”: è¿”å› (text, label) ç”¨äºæ„é€ æ­£è´Ÿæ ·æœ¬å¯¹

    labelçš„ä½œç”¨:
        - ä¸æ˜¯ç”¨äºè®¡ç®—åˆ†ç±»loss
        - è€Œæ˜¯ç”¨äºåˆ¤æ–­å“ªäº›æ ·æœ¬æ˜¯æ­£æ ·æœ¬å¯¹(åŒæ ‡ç­¾)
        - SupConLosså†…éƒ¨ä¼šç”¨labelæ„é€ mask
    """

    def __init__(self, texts, labels, tokenizer, max_length=256):
        """
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ (0æˆ–1)
            tokenizer: BERT tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

        print(f"âœ… ContrastiveTextDataset åˆå§‹åŒ–:")
        print(f"  - æ ·æœ¬æ•°: {len(texts)}")
        print(f"  - æœ€å¤§é•¿åº¦: {max_length}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬

        Returns:
            dict:
                'input_ids': [seq_len] - token ids
                'attention_mask': [seq_len] - attention mask
                'label': scalar - ç±»åˆ«æ ‡ç­¾ (0æˆ–1)
        """
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        # Tokenizeæ–‡æœ¬
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),  # [seq_len]
            'attention_mask': encoding['attention_mask'].squeeze(0),  # [seq_len]
            'label': torch.tensor(label, dtype=torch.long)  # scalar
        }


# ============================================================
# æ–°å¢: å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨
# ============================================================




class ContrastiveTextDataset(Dataset):
    """ç”¨äºå¯¹æ¯”å­¦ä¹ çš„æ–‡æœ¬æ•°æ®é›†"""

    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }
# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    """æµ‹è¯•æ•°æ®å¤„ç†å™¨"""
    print("æµ‹è¯•æ•°æ®å¤„ç†å™¨...")

    # åˆ›å»ºç®€å•çš„é…ç½®å¯¹è±¡
    class SimpleConfig:
        class Model:
            bert_model_name = 'bert-base-chinese'
        class Data:
            max_text_length = 256
        model = Model()
        data = Data()

    config = SimpleConfig()

    # åˆ›å»ºå¤„ç†å™¨
    processor = ComplaintDataProcessor(
        config,
        user_dict_file='new_user_dict.txt',
        stopword_file='new_stopword_dict.txt'
    )

    # æµ‹è¯•æ™ºèƒ½æ¸…æ´—
    test_texts = [
        "#åŒæ­¥è¦ç´ #å®¢æˆ·åæ˜ [101]ç§»åŠ¨4Gä¿¡å·å·®[/101]å¤šæ¬¡æŠ•è¯‰æœªè§£å†³#åŒæ­¥è¦ç´ #",
        "æµç¨‹è½¨è¿¹ï¼š(1)ã€è‡ªåŠ¨ã€‘æ˜¯å¦æœ‰åœ¨é€”å·¥å•ï¼šå¦(0) å®¢æˆ·è¦æ±‚ä¸Šé—¨å¤„ç†",
        "ã€äººå·¥ã€‘æ˜¯å¦éœ€è¦æ´¾å•ï¼šæ˜¯(1) å·¥å•å·ï¼šWO2024001234",
        "å®¢æˆ·å±äºç–‘ä¼¼æˆ·çº¿é—®é¢˜å…³æ€€ï¼Œè¯·æ™ºæ…§å®¶åº­å·¥ç¨‹å¸ˆå°½å¿«ä¸Šé—¨å¤„ç†ã€‚",
    ]

    print("\næ™ºèƒ½æ¸…æ´—æµ‹è¯•:")
    print("="*70)
    for i, text in enumerate(test_texts, 1):
        print(f"\nåŸå§‹æ–‡æœ¬ {i}:")
        print(f"  {text}")
        cleaned = processor.clean_text_smart(text)
        print(f"æ¸…æ´—å:")
        print(f"  {cleaned}")

        # æ£€æŸ¥å…³é”®ç‚¹
        has_brackets = '(' in cleaned or 'ã€' in cleaned or ')' in cleaned or 'ã€‘' in cleaned
        print(f"æ£€æŸ¥: æ‹¬å·={'âŒæ®‹ç•™' if has_brackets else 'âœ…æ¸…é™¤'}", end="")
        if '4G' in text:
            print(f" | 4G={'âœ…ä¿ç•™' if '4G' in cleaned else 'âŒä¸¢å¤±'}", end="")
        print()

    print("\nâœ… æ•°æ®å¤„ç†å™¨æµ‹è¯•å®Œæˆ!")