"""
å¤šæ¨¡æ€æŠ•è¯‰é¢„æµ‹æ¨¡å‹ - å®Œå…¨æ”¹è¿›ç‰ˆ
å®ç°å…­ä¸ªæ–¹å‘çš„æ‰€æœ‰æ”¹è¿›
âœ… ä¿®å¤ï¼štext_onlyæ¨¡å¼çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel
from torch_geometric.nn import GATConv
from torch_geometric.data import Data, Batch
import numpy as np
import os


# ============================================================
# æ–°å¢: BERTå¯¹æ¯”å­¦ä¹ æ¨¡å‹
# ============================================================

class BERTForContrastiveLearning(nn.Module):
    """BERT + Projection Head for Supervised Contrastive Learning"""

    def __init__(self, bert_model_name='bert-base-chinese', projection_dim=128):
        super().__init__()
        from transformers import BertModel

        self.bert = BertModel.from_pretrained(bert_model_name)
        hidden_size = self.bert.config.hidden_size  # 768

        # Projection Head: 768 â†’ 256 â†’ 128
        self.projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, projection_dim)
        )

    def forward(self, input_ids, attention_mask, return_projection=True):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # [batch, 768]

        if return_projection:
            projection = self.projection(pooled_output)
            features = F.normalize(projection, dim=1)  # L2å½’ä¸€åŒ–
            return features
        else:
            return pooled_output

    def get_bert_only(self):
        """æå–BERT (ä¸¢å¼ƒprojection)"""
        return self.bert


class SupConLoss(nn.Module):
    """Supervised Contrastive Learning Loss"""

    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, features, labels):
        """
        Args:
            features: [batch, dim] å½’ä¸€åŒ–ç‰¹å¾
            labels: [batch] ç±»åˆ«æ ‡ç­¾
        """
        device = features.device
        batch_size = features.shape[0]

        # 1. æ„é€ æ ‡ç­¾mask
        labels = labels.view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)

        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T)

        # 3. å»æ‰å¯¹è§’çº¿
        logits_mask = torch.ones_like(mask)
        logits_mask.fill_diagonal_(0)
        mask = mask * logits_mask

        # 4. æ¸©åº¦ç¼©æ”¾
        logits = similarity_matrix / self.temperature
        logits_max, _ = torch.max(logits, dim=1, keepdim=True)
        logits = logits - logits_max.detach()

        # 5. è®¡ç®—log-prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # 6. è®¡ç®—loss
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)
        loss = -mean_log_prob_pos.mean()

        return loss


class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å— - æ–¹å‘å››æ ¸å¿ƒåˆ›æ–°"""

    def __init__(self, dim, num_heads=4):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True)
        self.layer_norm1 = nn.LayerNorm(dim)
        self.layer_norm2 = nn.LayerNorm(dim)
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(dim * 4, dim),
            nn.Dropout(0.1)
        )

    def forward(self, query, key_value):
        """
        Args:
            query: [batch, 1, dim]
            key_value: [batch, 1, dim]
        Returns:
            enhanced: [batch, 1, dim]
            attn_weights: æ³¨æ„åŠ›æƒé‡
        """
        # Cross-Attention
        attn_output, attn_weights = self.attention(query, key_value, key_value)

        # æ®‹å·®è¿æ¥
        query = self.layer_norm1(query + attn_output)

        # FFN
        ffn_output = self.ffn(query)
        output = self.layer_norm2(query + ffn_output)

        return output, attn_weights


class TextMultiTokenGenerator(nn.Module):
    """ä»BERTå¤šå±‚è¾“å‡ºç”Ÿæˆå¤šä¸ªè¯­ä¹‰Token"""

    def __init__(self, bert_hidden_size=768, output_dim=256, num_tokens=4):
        super().__init__()
        self.num_tokens = num_tokens

        self.layer_projections = nn.ModuleList([
            nn.Sequential(
                nn.Linear(bert_hidden_size, output_dim),
                nn.LayerNorm(output_dim),
                nn.GELU()
            ) for _ in range(num_tokens)
        ])

        self.position_embeddings = nn.Parameter(
            torch.randn(1, num_tokens, output_dim) * 0.02
        )

    def forward(self, bert_hidden_states):
        """
        Args:
            bert_hidden_states: tupleï¼ŒBERTæ‰€æœ‰å±‚è¾“å‡º
        Returns:
            text_tokens: [batch, num_tokens, output_dim]
        """
        batch_size = bert_hidden_states[-1].size(0)

        tokens = []
        for i, proj in enumerate(self.layer_projections):
            layer_idx = -(self.num_tokens - i)  # -4, -3, -2, -1
            cls_token = bert_hidden_states[layer_idx][:, 0, :]  # [batch, 768]
            projected = proj(cls_token)  # [batch, 256]
            tokens.append(projected.unsqueeze(1))

        text_tokens = torch.cat(tokens, dim=1)  # [batch, 4, 256]
        text_tokens = text_tokens + self.position_embeddings.expand(batch_size, -1, -1)

        return text_tokens


class StructMultiTokenGenerator(nn.Module):
    """å°†ç»“æ„åŒ–ç‰¹å¾ç”Ÿæˆå¤šä¸ªToken"""

    def __init__(self, input_dim=53, output_dim=256, num_tokens=4):
        super().__init__()
        self.num_tokens = num_tokens

        self.token_generators = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(128, output_dim),
                nn.LayerNorm(output_dim)
            ) for _ in range(num_tokens)
        ])

        self.position_embeddings = nn.Parameter(
            torch.randn(1, num_tokens, output_dim) * 0.02
        )

    def forward(self, struct_features):
        """
        Args:
            struct_features: [batch, input_dim]
        Returns:
            struct_tokens: [batch, num_tokens, output_dim]
        """
        batch_size = struct_features.size(0)

        tokens = []
        for generator in self.token_generators:
            token = generator(struct_features)  # [batch, 256]
            tokens.append(token.unsqueeze(1))

        struct_tokens = torch.cat(tokens, dim=1)  # [batch, 4, 256]
        struct_tokens = struct_tokens + self.position_embeddings.expand(batch_size, -1, -1)

        return struct_tokens


class TextLedCrossModalAttention(nn.Module):
    """
    æ–‡æœ¬ä¸»å¯¼çš„è·¨æ¨¡æ€æ³¨æ„åŠ›

    è®¾è®¡ï¼š
    - æ–‡æœ¬å¯¹æ ‡ç­¾å’Œç»“æ„åšè·¨æ¨¡æ€æ³¨æ„åŠ›
    - å„æ¨¡æ€è‡ªæ³¨æ„åŠ›å¢å¼ºè¡¨ç¤º
    - é—¨æ§èåˆï¼Œæ–‡æœ¬æƒé‡æ›´é«˜
    """

    def __init__(self, dim=256, num_heads=4, dropout=0.1):
        super().__init__()
        self.dim = dim

        # æ–‡æœ¬å¯¹å…¶ä»–æ¨¡æ€çš„æ³¨æ„åŠ›
        self.text_to_label_attn = nn.MultiheadAttention(
            dim, num_heads, dropout=dropout, batch_first=True
        )
        self.text_to_struct_attn = nn.MultiheadAttention(
            dim, num_heads, dropout=dropout, batch_first=True
        )

        # å„æ¨¡æ€è‡ªæ³¨æ„åŠ›
        self.text_self_attn = nn.MultiheadAttention(
            dim, num_heads, dropout=dropout, batch_first=True
        )
        self.label_self_attn = nn.MultiheadAttention(
            dim, num_heads, dropout=dropout, batch_first=True
        )
        self.struct_self_attn = nn.MultiheadAttention(
            dim, num_heads, dropout=dropout, batch_first=True
        )

        # å±‚å½’ä¸€åŒ–
        self.text_norm = nn.LayerNorm(dim)
        self.label_norm = nn.LayerNorm(dim)
        self.struct_norm = nn.LayerNorm(dim)

        # é—¨æ§èåˆ
        self.modal_gate = nn.Sequential(
            nn.Linear(dim * 3, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3)
        )

        # æ–‡æœ¬åç½®ï¼ˆç¡®ä¿æ–‡æœ¬æƒé‡æ›´é«˜ï¼‰
        self.text_bias = nn.Parameter(torch.tensor(0.5))
        # å¯å­¦ä¹ çš„è·¨æ¨¡æ€èåˆæƒé‡
        self.cross_modal_weight_label = nn.Parameter(torch.tensor(0.0))
        self.cross_modal_weight_struct = nn.Parameter(torch.tensor(0.0))

    def forward(self, text_tokens, label_tokens, struct_tokens,
                label_mask=None, return_attention=True):
        """
        Args:
            text_tokens: [batch, 4, 256]
            label_tokens: [batch, N, 256]
            struct_tokens: [batch, 4, 256]
            label_mask: [batch, N] - Trueè¡¨ç¤ºpaddingä½ç½®
        """
        attention_weights = {}

        # 1. è‡ªæ³¨æ„åŠ›
        text_self, _ = self.text_self_attn(text_tokens, text_tokens, text_tokens)
        text_tokens = self.text_norm(text_tokens + text_self)

        label_self, attn_l = self.label_self_attn(
            label_tokens, label_tokens, label_tokens,
            key_padding_mask=label_mask,
            need_weights=return_attention,
            average_attn_weights=False
        )
        label_tokens = self.label_norm(label_tokens + label_self)

        struct_self, _ = self.struct_self_attn(struct_tokens, struct_tokens, struct_tokens)
        struct_tokens = self.struct_norm(struct_tokens + struct_self)

        # 2. æ–‡æœ¬ä¸»å¯¼çš„è·¨æ¨¡æ€æ³¨æ„åŠ›
        text_to_label, attn_t2l = self.text_to_label_attn(
            text_tokens, label_tokens, label_tokens,
            key_padding_mask=label_mask,
            need_weights=return_attention,
            average_attn_weights=False
        )

        text_to_struct, attn_t2s = self.text_to_struct_attn(
            text_tokens, struct_tokens, struct_tokens,
            need_weights=return_attention,
            average_attn_weights=False
        )

        # 3. ç‰¹å¾å¢å¼º
        weight_label = torch.sigmoid(self.cross_modal_weight_label)
        weight_struct = torch.sigmoid(self.cross_modal_weight_struct)
        text_enhanced = text_tokens + weight_label * text_to_label + weight_struct * text_to_struct
        label_enhanced = label_tokens
        struct_enhanced = struct_tokens

        # 4. æ± åŒ–
        text_pooled = text_enhanced.mean(dim=1)  # [batch, 256]
        label_pooled = label_enhanced.mean(dim=1)  # [batch, 256]
        struct_pooled = struct_enhanced.mean(dim=1)  # [batch, 256]

        # 5. é—¨æ§èåˆï¼ˆæ–‡æœ¬ä¸»å¯¼ï¼‰
        gate_input = torch.cat([text_pooled, label_pooled, struct_pooled], dim=-1)
        gate_logits = self.modal_gate(gate_input)  # [batch, 3]
        gate_logits[:, 0] = gate_logits[:, 0] + self.text_bias  # æ–‡æœ¬åç½®
        gate_weights = F.softmax(gate_logits, dim=-1)  # [batch, 3]

        if return_attention:
            attention_weights = {
                'text_to_label': attn_t2l,  # [batch, heads, 4, N]
                'text_to_struct': attn_t2s,  # [batch, heads, 4, 4]
                'label_self': attn_l,  # [batch, heads, N, N]
                'modal_weights': gate_weights,  # [batch, 3]
            }

        return text_pooled, label_pooled, struct_pooled, attention_weights


class GATLabelEncoder(nn.Module):
    """GATæ ‡ç­¾ç¼–ç å™¨ - æ”¯æŒå…¨å±€å›¾é¢„è®­ç»ƒ"""

    def __init__(self, vocab_size, hidden_dim=256, num_layers=3, num_heads=4, max_level=8):
        super().__init__()

        # èŠ‚ç‚¹åµŒå…¥
        self.node_embedding = nn.Embedding(vocab_size, 128)

        # å±‚çº§åµŒå…¥ - æ–¹å‘äºŒæ”¹è¿›
        self.level_embedding = nn.Embedding(max_level + 1, 32)

        # å±‚çº§æƒé‡ - ä½¿ç”¨é€’å¢åˆå§‹åŒ–ï¼Œè®©æ·±å±‚èŠ‚ç‚¹æœ‰æ›´å¤§åˆå§‹æƒé‡
        level_init = torch.zeros(max_level + 1)
        for i in range(max_level + 1):
            level_init[i] = i * 0.1  # æ·±å±‚æƒé‡æ›´å¤§
        self.level_weights = nn.Parameter(level_init)

        # GATå±‚
        self.gat_layers = nn.ModuleList()
        input_dim = 128 + 32

        for i in range(num_layers):
            if i == 0:
                self.gat_layers.append(
                    GATConv(input_dim, hidden_dim // num_heads, heads=num_heads, dropout=0.3)
                )
            else:
                self.gat_layers.append(
                    GATConv(hidden_dim, hidden_dim, heads=1, dropout=0.3)
                )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)

    def forward(self, node_ids, edge_index, node_levels, batch=None):
        """å‰å‘ä¼ æ’­"""
        # èŠ‚ç‚¹åµŒå…¥
        x = self.node_embedding(node_ids)  # [num_nodes, 128]

        # å±‚çº§åµŒå…¥
        level_emb = self.level_embedding(node_levels)  # [num_nodes, 32]

        # æ‹¼æ¥
        x = torch.cat([x, level_emb], dim=-1)  # [num_nodes, 160]

        # GATå±‚
        for gat_layer in self.gat_layers:
            x = gat_layer(x, edge_index)
            x = F.elu(x)

        # å±‚çº§åŠ æƒ - æ–¹å‘äºŒæ”¹è¿›
        level_weights = torch.softmax(self.level_weights, dim=0)
        weighted_x = x * level_weights[node_levels].unsqueeze(-1)

        # è¾“å‡ºæŠ•å½±å’Œå½’ä¸€åŒ–ï¼ˆå¯¹æ‰€æœ‰èŠ‚ç‚¹ï¼‰
        weighted_x = self.output_proj(weighted_x)
        weighted_x = self.layer_norm(weighted_x)

        # ========== è¿”å›æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼ˆä¿®å¤P01ï¼‰==========
        max_nodes = 8  # æ ‡ç­¾è·¯å¾„æœ€å¤§èŠ‚ç‚¹æ•°

        if batch is not None:
            batch_features = []
            batch_masks = []
            unique_batches = torch.unique(batch)

            for b in unique_batches:
                node_mask = (batch == b)
                nodes = weighted_x[node_mask]  # [num_nodes, hidden_dim]
                num_nodes = nodes.size(0)

                # Paddingåˆ°max_nodes
                if num_nodes < max_nodes:
                    pad_size = max_nodes - num_nodes
                    padding = torch.zeros(pad_size, nodes.size(-1), device=nodes.device)
                    nodes = torch.cat([nodes, padding], dim=0)
                    attn_mask = torch.cat([
                        torch.zeros(num_nodes, dtype=torch.bool, device=nodes.device),
                        torch.ones(pad_size, dtype=torch.bool, device=nodes.device)
                    ])
                else:
                    nodes = nodes[:max_nodes]
                    attn_mask = torch.zeros(max_nodes, dtype=torch.bool, device=nodes.device)

                batch_features.append(nodes.unsqueeze(0))
                batch_masks.append(attn_mask.unsqueeze(0))

            node_features = torch.cat(batch_features, dim=0)  # [batch, max_nodes, dim]
            node_masks = torch.cat(batch_masks, dim=0)  # [batch, max_nodes]
            return node_features, node_masks

        # å•æ ·æœ¬æƒ…å†µ
        num_nodes = weighted_x.size(0)
        if num_nodes < max_nodes:
            pad_size = max_nodes - num_nodes
            padding = torch.zeros(pad_size, weighted_x.size(-1), device=weighted_x.device)
            weighted_x = torch.cat([weighted_x, padding], dim=0)
            attn_mask = torch.cat([
                torch.zeros(num_nodes, dtype=torch.bool, device=weighted_x.device),
                torch.ones(pad_size, dtype=torch.bool, device=weighted_x.device)
            ])
        else:
            weighted_x = weighted_x[:max_nodes]
            attn_mask = torch.zeros(max_nodes, dtype=torch.bool, device=weighted_x.device)

        return weighted_x.unsqueeze(0), attn_mask.unsqueeze(0)


# ============================================================
# æ–°å¢: Labelå›å½’é¢„è®­ç»ƒæ¨¡å—
# ============================================================

class LabelRiskRegressor(nn.Module):
    """
    æ ‡ç­¾è·¯å¾„é£é™©å›å½’å™¨

    ç›®æ ‡: é¢„æµ‹ç»™å®šæ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰ç‡ (0-1ä¹‹é—´çš„è¿ç»­å€¼)

    æ¶æ„:
        GATLabelEncoder â†’ PathEmbedding [256ç»´] â†’ Regressor â†’ Risk Score [1ç»´]

    è®­ç»ƒç›®æ ‡:
        ç»™å®šæ ‡ç­¾è·¯å¾„ L1â†’L2â†’L3ï¼Œé¢„æµ‹è¯¥è·¯å¾„çš„å†å²é‡å¤æŠ•è¯‰ç‡
        ä¾‹å¦‚: "æœåŠ¡è´¨é‡â†’å“åº”é€Ÿåº¦â†’è¶…æ—¶" â†’ 0.23 (23%é‡å¤ç‡)
    """

    def __init__(self, label_encoder, hidden_dim=256):
        """
        Args:
            label_encoder: å·²æœ‰çš„GATLabelEncoder
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super().__init__()
        self.label_encoder = label_encoder

        # å›å½’å¤´
        self.regressor = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()  # è¾“å‡º0-1ä¹‹é—´
        )

    def forward(self, node_ids, edge_index, node_levels, batch=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            node_ids: èŠ‚ç‚¹ID
            edge_index: è¾¹ç´¢å¼•
            node_levels: èŠ‚ç‚¹å±‚çº§
            batch: batchç´¢å¼•

        Returns:
            risk_scores: [batch_size, 1] é¢„æµ‹çš„é£é™©åˆ†æ•°
        """
        # è·å–æ ‡ç­¾è·¯å¾„åµŒå…¥
        label_output = self.label_encoder(node_ids, edge_index, node_levels, batch)

        # å¤„ç†æ–°çš„è¿”å›æ ¼å¼ï¼ˆtupleï¼‰
        if isinstance(label_output, tuple):
            label_embedding = label_output[0].mean(dim=1)  # [batch, dim]
        else:
            label_embedding = label_output  # [batch, dim]

        # å›å½’é¢„æµ‹
        risk_scores = self.regressor(label_embedding)
        return risk_scores

    def compute_loss(self, predicted, target):
        """è®¡ç®—MSEæŸå¤±"""
        return F.mse_loss(predicted.squeeze(), target.float())


# ============================================================
# æ–°å¢: å¤šä»»åŠ¡é¢„è®­ç»ƒæ¨¡å—
# ============================================================

class MultiTaskPretrainer(nn.Module):
    """
    å¤šä»»åŠ¡é¢„è®­ç»ƒ: åŒæ—¶è¿›è¡Œæ–‡æœ¬å¯¹æ¯”å­¦ä¹  + æ ‡ç­¾å›å½’

    è®¾è®¡ç†å¿µ:
        - æ–‡æœ¬ç¼–ç å™¨é€šè¿‡å¯¹æ¯”å­¦ä¹ è·å¾—æ›´å¥½çš„è¯­ä¹‰è¡¨ç¤º
        - æ ‡ç­¾ç¼–ç å™¨é€šè¿‡å›å½’ä»»åŠ¡å­¦ä¹ æ ‡ç­¾è·¯å¾„çš„é£é™©æ¨¡å¼
        - ä¸¤è€…ç‹¬ç«‹é¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸»æ¨¡å‹ä¸­èåˆ
    """

    def __init__(self, text_model, label_regressor, text_weight=0.5, label_weight=0.5, temperature=0.5):
        super().__init__()
        self.text_model = text_model
        self.label_regressor = label_regressor
        self.text_weight = text_weight
        self.label_weight = label_weight
        self.temperature = temperature
        self.contrastive_loss = SupConLoss(temperature=self.temperature)

    def forward(self, text_inputs, label_inputs, text_labels, label_targets):
        """
        å‰å‘ä¼ æ’­

        Args:
            text_inputs: (input_ids, attention_mask)
            label_inputs: (node_ids, edge_index, node_levels, batch)
            text_labels: æ–‡æœ¬ç±»åˆ«æ ‡ç­¾
            label_targets: æ ‡ç­¾è·¯å¾„é£é™©åˆ†æ•°

        Returns:
            total_loss, text_loss, label_loss
        """
        # æ–‡æœ¬å¯¹æ¯”å­¦ä¹ 
        input_ids, attention_mask = text_inputs
        text_features = self.text_model(input_ids, attention_mask)
        text_loss = self.contrastive_loss(text_features, text_labels)

        # æ ‡ç­¾å›å½’
        node_ids, edge_index, node_levels, batch = label_inputs
        risk_pred = self.label_regressor(node_ids, edge_index, node_levels, batch)
        label_loss = self.label_regressor.compute_loss(risk_pred, label_targets)

        # åŠ æƒæ€»æŸå¤±
        total_loss = self.text_weight * text_loss + self.label_weight * label_loss

        return total_loss, text_loss, label_loss


# ============================================================
# æ–°å¢: å…¨å±€å›¾é¢„è®­ç»ƒæŸå¤±
# ============================================================

class GlobalGraphPretrainLoss(nn.Module):
    """
    å…¨å±€å›¾é¢„è®­ç»ƒæŸå¤±

    åŒ…å«ä¸‰ä¸ªå­ä»»åŠ¡:
    1. èŠ‚ç‚¹åˆ†ç±»: é¢„æµ‹èŠ‚ç‚¹çš„å±‚çº§
    2. è¾¹é¢„æµ‹: é¢„æµ‹ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´æ˜¯å¦å­˜åœ¨è¾¹
    3. å›¾å¯¹æ¯”å­¦ä¹ : å¢å¼ºå›¾çš„è¡¨ç¤ºèƒ½åŠ›
    """

    def __init__(self, hidden_dim=256, num_levels=8):
        super().__init__()

        # èŠ‚ç‚¹åˆ†ç±»å¤´
        self.level_classifier = nn.Linear(hidden_dim, num_levels + 1)

        # è¾¹é¢„æµ‹å¤´
        self.edge_predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, node_embeddings, edge_index, node_levels, batch=None):
        """
        è®¡ç®—é¢„è®­ç»ƒæŸå¤±

        Args:
            node_embeddings: [num_nodes, hidden_dim] èŠ‚ç‚¹åµŒå…¥
            edge_index: [2, num_edges] è¾¹ç´¢å¼•
            node_levels: [num_nodes] èŠ‚ç‚¹å±‚çº§
            batch: [num_nodes] batchç´¢å¼•

        Returns:
            total_loss, level_loss, edge_loss
        """
        # 1. èŠ‚ç‚¹å±‚çº§åˆ†ç±»æŸå¤±
        level_logits = self.level_classifier(node_embeddings)
        level_loss = F.cross_entropy(level_logits, node_levels)

        # 2. è¾¹é¢„æµ‹æŸå¤±
        # æ­£æ ·æœ¬: å®é™…å­˜åœ¨çš„è¾¹
        src, dst = edge_index
        pos_pairs = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=-1)
        pos_scores = self.edge_predictor(pos_pairs).squeeze()
        pos_labels = torch.ones_like(pos_scores)

        # è´Ÿæ ·æœ¬: éšæœºé‡‡æ ·ä¸å­˜åœ¨çš„è¾¹
        num_neg = edge_index.size(1)
        num_nodes = node_embeddings.size(0)
        neg_src = torch.randint(0, num_nodes, (num_neg,), device=edge_index.device)
        neg_dst = torch.randint(0, num_nodes, (num_neg,), device=edge_index.device)
        neg_pairs = torch.cat([node_embeddings[neg_src], node_embeddings[neg_dst]], dim=-1)
        neg_scores = self.edge_predictor(neg_pairs).squeeze()
        neg_labels = torch.zeros_like(neg_scores)

        all_scores = torch.cat([pos_scores, neg_scores])
        all_labels = torch.cat([pos_labels, neg_labels])
        edge_loss = F.binary_cross_entropy_with_logits(all_scores, all_labels)

        # æ€»æŸå¤±
        total_loss = level_loss + edge_loss

        return total_loss, level_loss, edge_loss


# ============================================================
# æ–°å¢: æ ‡ç­¾é¢„è®­ç»ƒ - å…±ç°é¢„æµ‹
# ============================================================

class LabelCooccurrenceLoss(nn.Module):
    """
    æ ‡ç­¾å…±ç°é¢„æµ‹æŸå¤±

    ä»»åŠ¡: ç»™å®šä¸€ä¸ªæ ‡ç­¾è·¯å¾„ï¼Œé¢„æµ‹å“ªäº›å…¶ä»–æ ‡ç­¾ç»å¸¸ä¸€èµ·å‡ºç°
    è¿™æœ‰åŠ©äºå­¦ä¹ æ ‡ç­¾ä¹‹é—´çš„è¯­ä¹‰å…³ç³»

    è®­ç»ƒæ•°æ®æ ¼å¼:
        è¾“å…¥: æ ‡ç­¾è·¯å¾„ L1â†’L2â†’L3 çš„åµŒå…¥
        ç›®æ ‡: å¤šæ ‡ç­¾åˆ†ç±»ï¼Œé¢„æµ‹ç›¸å…³æ ‡ç­¾
    """

    def __init__(self, hidden_dim=256, num_labels=1000):
        super().__init__()
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_labels)
        )

    def forward(self, label_embedding, cooccur_labels):
        """
        Args:
            label_embedding: [batch, hidden_dim] æ ‡ç­¾è·¯å¾„åµŒå…¥
            cooccur_labels: [batch, num_labels] å¤šçƒ­ç¼–ç çš„å…±ç°æ ‡ç­¾

        Returns:
            loss: BCEæŸå¤±
        """
        logits = self.predictor(label_embedding)
        loss = F.binary_cross_entropy_with_logits(logits, cooccur_labels.float())
        return loss


# ============================================================
# å¦ä¸€ç§å¯¹æ¯”å­¦ä¹ å®ç° - æ›´çµæ´»çš„ç‰ˆæœ¬
# ============================================================

class FlexibleSupConLoss(nn.Module):
    """
    çµæ´»çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤±

    æ”¯æŒ:
    - æ‰¹å†…å¯¹æ¯”
    - å¤šæ­£æ ·æœ¬å¯¹æ¯”
    - æ¸©åº¦å¯è°ƒ
    """

    def __init__(self, temperature=0.07, base_temperature=0.07):
        super().__init__()
        self.temperature = temperature
        self.base_temperature = base_temperature

    def forward(self, features, labels):
        """
        è®¡ç®—ç›‘ç£å¯¹æ¯”æŸå¤±

        Args:
            features: [batch, n_views, dim] æˆ– [batch, dim]
            labels: [batch]

        Returns:
            å¯¹æ¯”æŸå¤±
        """
        device = features.device

        # å¦‚æœæ˜¯2Dï¼Œæ·»åŠ viewç»´åº¦
        if features.dim() == 2:
            features = features.unsqueeze(1)

        batch_size = features.shape[0]
        contrast_count = features.shape[1]
        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)

        # 4.1 è®¡ç®—ç›¸ä¼¼åº¦
        anchor_dot_contrast = torch.div(
            torch.matmul(contrast_feature, contrast_feature.T),
            self.temperature
        )

        # 4.2 æ•°å€¼ç¨³å®šæ€§
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # 4.3 æ„å»ºmask
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)
        mask = mask.repeat(contrast_count, contrast_count)

        # 4.4 æ’é™¤è‡ªèº«
        logits_mask = torch.ones_like(mask)
        logits_mask.fill_diagonal_(0)
        mask = mask * logits_mask

        # 4.5 è®¡ç®—log-prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)

        # 4.6 æœ€ç»ˆloss: è´Ÿçš„å¹³å‡log-prob
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)
        loss = -mean_log_prob_pos.mean()

        return loss


class MultiModalComplaintModel(nn.Module):
    """å¤šæ¨¡æ€æŠ•è¯‰é¢„æµ‹æ¨¡å‹ - å®Œæ•´ç‰ˆ"""

    def __init__(self, config, vocab_size, mode='full', pretrained_path=None, No_pretrain_bert=False):
        """
        Args:
            config: é…ç½®å¯¹è±¡
            vocab_size: æ ‡ç­¾è¯æ±‡è¡¨å¤§å°
            mode: æ¨¡å‹æ¨¡å¼
                - 'full': å®Œæ•´ä¸‰æ¨¡æ€
                - 'text_only': ä»…æ–‡æœ¬
                - 'label_only': ä»…æ ‡ç­¾
                - 'struct_only': ä»…ç»“æ„åŒ–
                - 'text_label': æ–‡æœ¬+æ ‡ç­¾
                - 'text_struct': æ–‡æœ¬+ç»“æ„åŒ–
                - 'label_struct': æ ‡ç­¾+ç»“æ„åŒ–
            pretrained_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        """
        super().__init__()

        self.config = config
        self.mode = mode
        self.device = config.training.device

        # ========== æ–‡æœ¬ç¼–ç å™¨ (BERT) - æ–¹å‘ä¸€æ”¹è¿› ==========
        if mode in ['full', 'text_only', 'text_label', 'text_struct']:
            if No_pretrain_bert:
                # ã€æ–°å¢ã€‘å®Œå…¨éšæœºåˆå§‹åŒ–BERTï¼ˆçœŸæ­£ä»é›¶è®­ç»ƒï¼‰
                print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„BERTï¼ˆä»é›¶è®­ç»ƒï¼‰")
                from transformers import BertConfig
                bert_config = BertConfig.from_pretrained(config.model.bert_model_name)
                self.text_encoder = BertModel(bert_config)  # éšæœºåˆå§‹åŒ–æƒé‡
            elif pretrained_path and os.path.exists(pretrained_path):
                print(f"âœ… åŠ è½½é¢†åŸŸé¢„è®­ç»ƒBERT: {pretrained_path}")
                self.text_encoder = BertModel.from_pretrained(pretrained_path)
            else:
                print("ğŸ“¦ ä½¿ç”¨åŸå§‹BERTé¢„è®­ç»ƒæƒé‡ï¼ˆæ— é¢†åŸŸé¢„è®­ç»ƒï¼‰")
                self.text_encoder = BertModel.from_pretrained(config.model.bert_model_name)

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šæ·»åŠ æŠ•å½±å±‚ï¼ˆç»Ÿä¸€ç»´åº¦åˆ°256ï¼‰
            self.text_proj = nn.Linear(768, 256)

            # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
            self.text_contrast_proj = nn.Sequential(
                nn.Linear(768, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            )
        else:
            self.text_encoder = None
            self.text_proj = None  # âœ… æ˜ç¡®è®¾ç½®ä¸ºNone
            self.text_contrast_proj = None

        # ========== æ ‡ç­¾ç¼–ç å™¨ (GAT) - æ–¹å‘äºŒæ”¹è¿› ==========
        if mode in ['full', 'label_only', 'text_label', 'label_struct']:
            self.label_encoder = GATLabelEncoder(
                vocab_size=vocab_size,
                hidden_dim=256,
                num_layers=3,
                num_heads=4
            )

            # åŠ è½½å…¨å±€å›¾é¢„è®­ç»ƒæƒé‡
            if pretrained_path:
                label_pretrain_path = os.path.join(
                    config.training.label_pretrain_save_dir,
                    'label_global_pretrain.pth'
                )
                if os.path.exists(label_pretrain_path):
                    try:
                        state_dict = torch.load(label_pretrain_path, map_location=self.device)
                        self.label_encoder.load_state_dict(state_dict, strict=False)
                        print("âœ… åŠ è½½å…¨å±€å›¾é¢„è®­ç»ƒæƒé‡")
                    except Exception as e:
                        print(f"âš ï¸ å…¨å±€å›¾é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
        else:
            self.label_encoder = None

        # ========== ç»“æ„åŒ–ç‰¹å¾ç¼–ç å™¨ - æ–¹å‘ä¸‰æ”¹è¿› ==========
        if mode in ['full', 'struct_only', 'text_struct', 'label_struct']:
            assert config.model.struct_feat_dim == 53, \
                f"ç»“æ„åŒ–ç‰¹å¾å¿…é¡»æ˜¯53ç»´ï¼Œå½“å‰{config.model.struct_feat_dim}ç»´"

            self.struct_encoder = nn.Sequential(
                nn.Linear(53, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.3)
            )

            # ç‰¹å¾é‡è¦æ€§æƒé‡ - ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ä»¥æ‰“ç ´å¯¹ç§°æ€§
            self.feature_importance = nn.Parameter(torch.randn(53) * 0.1)
        else:
            self.struct_encoder = None
            self.feature_importance = None

        # ========== è·¨æ¨¡æ€æ³¨æ„åŠ› - æ–‡æœ¬ä¸»å¯¼æ–¹æ¡ˆï¼ˆä¿®å¤P01ï¼‰==========
        if mode == 'full':
            # Tokenç”Ÿæˆå™¨
            self.text_token_gen = TextMultiTokenGenerator(
                bert_hidden_size=768, output_dim=256, num_tokens=4
            )
            self.struct_token_gen = StructMultiTokenGenerator(
                input_dim=53, output_dim=256, num_tokens=4
            )
            # æ–‡æœ¬ä¸»å¯¼çš„è·¨æ¨¡æ€æ³¨æ„åŠ›
            self.text_led_cross_modal = TextLedCrossModalAttention(
                dim=256, num_heads=4, dropout=0.1
            )
        elif mode in ['text_label', 'text_struct', 'label_struct']:
            # åŒæ¨¡æ€äº¤äº’
            self.modal_attn_1 = CrossModalAttention(256, num_heads=4)
            self.modal_attn_2 = CrossModalAttention(256, num_heads=4)

        # ========== èåˆå±‚ ==========
        # è®¡ç®—èåˆå±‚è¾“å…¥ç»´åº¦
        if mode == 'full':
            fusion_input_dim = 256 * 3  # text + label + struct
        elif mode in ['text_label', 'text_struct', 'label_struct']:
            fusion_input_dim = 256 * 2
        else:
            fusion_input_dim = 256  # å•æ¨¡æ€ç»Ÿä¸€256ç»´

        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, config.model.fusion_dim),
            nn.LayerNorm(config.model.fusion_dim),
            nn.ReLU(),
            nn.Dropout(config.model.dropout),
            nn.Linear(config.model.fusion_dim, config.model.hidden_dim),
            nn.LayerNorm(config.model.hidden_dim),
            nn.ReLU(),
            nn.Dropout(config.model.dropout)
        )

        # ========== åˆ†ç±»å¤´ ==========
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(config.model.hidden_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 2)
        )
        # ========== æºå¤´é¢„é˜²: æƒé‡åˆå§‹åŒ– ==========
        self._initialize_weights()

    def _initialize_weights(self):
        """
        åˆå§‹åŒ–æ¨¡å‹æƒé‡ - æºå¤´é¢„é˜²æ•°å€¼é—®é¢˜
        ä½¿ç”¨Xavieråˆå§‹åŒ–ç¡®ä¿è®­ç»ƒç¨³å®š
        """
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                # Xavierå‡åŒ€åˆå§‹åŒ– (é€‚åˆtanh/sigmoidæ¿€æ´»)
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

            elif isinstance(module, nn.MultiheadAttention):
                # æ³¨æ„åŠ›å±‚çš„ç‰¹æ®Šåˆå§‹åŒ–
                for param_name, param in module.named_parameters():
                    if 'weight' in param_name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in param_name:
                        nn.init.zeros_(param)

            elif isinstance(module, nn.Embedding):
                # åµŒå…¥å±‚æ­£æ€åˆå§‹åŒ–
                nn.init.normal_(module.weight, mean=0.0, std=0.02)

            elif isinstance(module, nn.Parameter):
                # å‚æ•°åˆå§‹åŒ– (å¦‚feature_importance)
                if module.dim() > 0:
                    nn.init.ones_(module)

        print("âœ… æ¨¡å‹æƒé‡åˆå§‹åŒ–å®Œæˆ")

    def forward(self, input_ids=None, attention_mask=None,
                node_ids_list=None, edges_list=None, node_levels_list=None,
                struct_features=None, return_attention=False):
        """
        å‰å‘ä¼ æ’­
        Args:
            input_ids: æ–‡æœ¬è¾“å…¥ [batch, seq_len]
            attention_mask: æ³¨æ„åŠ›mask [batch, seq_len]
            node_ids_list: èŠ‚ç‚¹IDåˆ—è¡¨ (list of lists)
            edges_list: è¾¹åˆ—è¡¨ (list of lists)
            node_levels_list: èŠ‚ç‚¹å±‚çº§åˆ—è¡¨ (list of lists)
            struct_features: ç»“æ„åŒ–ç‰¹å¾ [batch, 53]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        Returns:
            logits: åˆ†ç±»logits [batch, 2]
            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸ï¼ˆå¦‚æœreturn_attention=Trueï¼‰
        """
        attention_weights = {}

        # ========== æ–‡æœ¬ç‰¹å¾ ==========
        bert_hidden_states = None  # æ–°å¢ï¼šä¿å­˜hidden_statesç”¨äºå¤šTokenç”Ÿæˆ
        if self.text_encoder is not None and input_ids is not None:
            text_output = self.text_encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )

            # ä¿å­˜æ‰€æœ‰å±‚çš„hidden_statesï¼ˆç”¨äºå¤šTokenç”Ÿæˆï¼‰
            bert_hidden_states = text_output.hidden_states  # tuple of [batch, seq, 768]

            # åŸæœ‰é€»è¾‘ä¿æŒï¼ˆç”¨äºéfullæ¨¡å¼çš„å…¼å®¹ï¼‰
            text_feat = text_output.last_hidden_state[:, 0, :]  # [batch, 768]

            if self.text_proj is not None:
                text_feat_proj = self.text_proj(text_feat)  # [batch, 256]
                text_feat_proj = text_feat_proj.unsqueeze(1)  # [batch, 1, 256]
            else:
                raise ValueError("text_projä¸åº”ä¸ºNoneï¼")
        else:
            text_feat_proj = None
            bert_hidden_states = None

        # ========== æ ‡ç­¾ç‰¹å¾ ==========
        if self.label_encoder is not None and node_ids_list is not None:
            batch_data = []
            for i in range(len(node_ids_list)):
                node_ids = torch.tensor(node_ids_list[i], dtype=torch.long, device=self.device)
                node_levels = torch.tensor(node_levels_list[i], dtype=torch.long, device=self.device)

                # æ„å»ºè¾¹
                if edges_list[i]:
                    edges = torch.tensor(edges_list[i], dtype=torch.long, device=self.device).t()
                else:
                    # è‡ªç¯
                    num_nodes = len(node_ids)
                    edges = torch.tensor([[j, j] for j in range(num_nodes)], device=self.device).t()

                data = Data(
                    x=node_ids,
                    edge_index=edges,
                    node_levels=node_levels,
                    batch=torch.full((len(node_ids),), i, dtype=torch.long, device=self.device)
                )
                batch_data.append(data)

            graph_batch = Batch.from_data_list(batch_data).to(self.device)
            # è·å–æ ‡ç­¾å¤šèŠ‚ç‚¹ç‰¹å¾ï¼ˆä¿®å¤P01ï¼‰
            label_feat = self.label_encoder(
                graph_batch.x,
                graph_batch.edge_index,
                graph_batch.node_levels,
                graph_batch.batch
            )  # è¿”å› (node_features, node_masks)
            # æ³¨æ„ï¼šlabel_featç°åœ¨æ˜¯tupleï¼Œåœ¨è·¨æ¨¡æ€äº¤äº’ä¸­å¤„ç†
        else:
            label_feat = None

        # ========== ç»“æ„åŒ–ç‰¹å¾ ==========
        if self.struct_encoder is not None and struct_features is not None:
            # ç‰¹å¾é‡è¦æ€§åŠ æƒ - æ–¹å‘ä¸‰
            if hasattr(self, 'feature_importance'):
                importance_weights = torch.softmax(self.feature_importance, dim=0)
                struct_features = struct_features * importance_weights

            struct_feat = self.struct_encoder(struct_features)  # [batch, 256]
            struct_feat = struct_feat.unsqueeze(1)  # [batch, 1, 256]
        else:
            struct_feat = None

        # ========== è·¨æ¨¡æ€äº¤äº’ - æ–‡æœ¬ä¸»å¯¼æ–¹æ¡ˆ ==========
        if self.mode == 'full':
            if text_feat_proj is not None and label_feat is not None and struct_feat is not None:
                # å¤„ç†æ ‡ç­¾è¿”å›å€¼ï¼ˆç°åœ¨æ˜¯å¤šèŠ‚ç‚¹ç‰ˆæœ¬ï¼‰
                if isinstance(label_feat, tuple):
                    label_node_feats, label_mask = label_feat
                else:
                    # å…¼å®¹æ—§ç‰ˆæœ¬
                    label_node_feats = label_feat
                    label_mask = None

                # ç”Ÿæˆæ–‡æœ¬å¤šToken
                if bert_hidden_states is not None:
                    text_tokens = self.text_token_gen(bert_hidden_states)  # [batch, 4, 256]
                else:
                    # å›é€€ï¼šå¤åˆ¶text_feat_proj
                    text_tokens = text_feat_proj.expand(-1, 4, -1)

                # ç”Ÿæˆç»“æ„å¤šToken
                struct_tokens = self.struct_token_gen(struct_features)  # [batch, 4, 256]

                # æ–‡æœ¬ä¸»å¯¼çš„è·¨æ¨¡æ€æ³¨æ„åŠ›
                text_pooled, label_pooled, struct_pooled, cross_attn = \
                    self.text_led_cross_modal(
                        text_tokens, label_node_feats, struct_tokens,
                        label_mask=label_mask,
                        return_attention=return_attention
                    )

                if return_attention and cross_attn:
                    attention_weights.update(cross_attn)

                # æ‹¼æ¥ä¸‰ä¸ªæ¨¡æ€çš„æ± åŒ–ç‰¹å¾
                combined_feat = torch.cat([text_pooled, label_pooled, struct_pooled], dim=-1)
            else:
                # å›é€€ï¼šéƒ¨åˆ†æ¨¡æ€ç¼ºå¤±
                features = []
                if text_feat_proj is not None:
                    features.append(text_feat_proj.squeeze(1))
                if label_feat is not None:
                    if isinstance(label_feat, tuple):
                        features.append(label_feat[0].mean(dim=1))
                    else:
                        features.append(label_feat.squeeze(1))
                if struct_feat is not None:
                    features.append(struct_feat.squeeze(1))
                combined_feat = torch.cat(features, dim=-1)

        elif self.mode in ['text_label', 'text_struct', 'label_struct']:
            # åŒæ¨¡æ€äº¤äº’
            feat1, feat2 = None, None

            if self.mode == 'text_label':
                feat1 = text_feat_proj
                # ä¿®å¤: label_encoderè¿”å›çš„æ˜¯å…ƒç»„(node_features, node_masks)
                if isinstance(label_feat, tuple):
                    node_feats, node_mask = label_feat
                    if node_mask is not None:
                        valid_mask = ~node_mask
                        mask_expanded = valid_mask.unsqueeze(-1).float()
                        label_pooled = (node_feats * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-8)
                    else:
                        label_pooled = node_feats.mean(dim=1)
                    feat2 = label_pooled.unsqueeze(1)
                else:
                    feat2 = label_feat
            elif self.mode == 'text_struct':
                feat1, feat2 = text_feat_proj, struct_feat
            elif self.mode == 'label_struct':
                # ä¿®å¤: label_encoderè¿”å›çš„æ˜¯å…ƒç»„(node_features, node_masks)
                if isinstance(label_feat, tuple):
                    node_feats, node_mask = label_feat
                    if node_mask is not None:
                        valid_mask = ~node_mask
                        mask_expanded = valid_mask.unsqueeze(-1).float()
                        label_pooled = (node_feats * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-8)
                    else:
                        label_pooled = node_feats.mean(dim=1)
                    feat1 = label_pooled.unsqueeze(1)
                else:
                    feat1 = label_feat
                feat2 = struct_feat

            if feat1 is not None and feat2 is not None:
                feat1_enhanced, attn1 = self.modal_attn_1(feat1, feat2)
                feat2_enhanced, attn2 = self.modal_attn_2(feat2, feat1)

                if return_attention:
                    attention_weights['modal1_to_modal2'] = attn1
                    attention_weights['modal2_to_modal1'] = attn2

                combined_feat = torch.cat([
                    feat1_enhanced.squeeze(1),
                    feat2_enhanced.squeeze(1)
                ], dim=-1)
            else:
                # å¦‚æœæŸä¸ªæ¨¡æ€ç¼ºå¤±ï¼Œä½¿ç”¨å¯ç”¨çš„
                available = [f for f in [feat1, feat2] if f is not None]
                if available:
                    combined_feat = torch.cat([f.squeeze(1) for f in available], dim=-1)
                else:
                    raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªæ¨¡æ€çš„è¾“å…¥")

        else:
            # å•æ¨¡æ€
            if text_feat_proj is not None:
                combined_feat = text_feat_proj.squeeze(1)
            elif label_feat is not None:
                if isinstance(label_feat, tuple):
                    node_feats, node_mask = label_feat
                    if node_mask is not None:
                        valid_mask = ~node_mask  # Trueè¡¨ç¤ºpaddingï¼Œå–åå¾—åˆ°æœ‰æ•ˆä½ç½®
                        mask_expanded = valid_mask.unsqueeze(-1).float()
                        combined_feat = (node_feats * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-8)
                    else:
                        combined_feat = node_feats.mean(dim=1)
                else:
                    combined_feat = label_feat.squeeze(1)
            elif struct_feat is not None:
                combined_feat = struct_feat.squeeze(1)
            else:
                raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªæ¨¡æ€çš„è¾“å…¥")

        # ========== èåˆå’Œåˆ†ç±» ==========
        fused_feat = self.fusion(combined_feat)  # [batch, hidden_dim]
        logits = self.classifier(fused_feat)  # [batch, 2]

        # ========== æºå¤´é¢„é˜²: ç»Ÿä¸€è¿”å›æ ¼å¼ ==========
        if return_attention:
            # ç¡®ä¿attention_weightsä¸ä¸ºç©º
            if not attention_weights:
                attention_weights = self._get_default_attention_weights()
            return logits, attention_weights
        else:
            # å³ä½¿ä¸éœ€è¦attentionï¼Œä¹Ÿè¿”å›Noneä¿æŒæ ¼å¼ç»Ÿä¸€
            return logits, None

    def _get_default_attention_weights(self):
        """è·å–é»˜è®¤çš„æ³¨æ„åŠ›æƒé‡ - ä½¿ç”¨æœ‰æ„ä¹‰çš„åˆå§‹åŒ–"""
        max_nodes = 8
        num_heads = 4
        text_seq_len = 4
        struct_seq_len = 4

        # æ–‡æœ¬å¯¹æ ‡ç­¾: é€’å‡æƒé‡ï¼ˆå‰é¢çš„æ ‡ç­¾èŠ‚ç‚¹æ›´é‡è¦ï¼‰
        t2l_weights = torch.zeros(1, num_heads, text_seq_len, max_nodes, device=self.device)
        for i in range(max_nodes):
            t2l_weights[:, :, :, i] = 1.0 / (i + 1)
        t2l_weights = t2l_weights / t2l_weights.sum(dim=-1, keepdim=True)

        # æ–‡æœ¬å¯¹ç»“æ„: é€’å‡æƒé‡
        t2s_weights = torch.zeros(1, num_heads, text_seq_len, struct_seq_len, device=self.device)
        for i in range(struct_seq_len):
            t2s_weights[:, :, :, i] = 1.0 / (i + 1)
        t2s_weights = t2s_weights / t2s_weights.sum(dim=-1, keepdim=True)

        # æ ‡ç­¾è‡ªæ³¨æ„åŠ›: å¯¹è§’çº¿å¼ºè°ƒï¼ˆè‡ªæ³¨æ„åŠ›æ›´å¼ºï¼‰
        l_self_weights = torch.zeros(1, num_heads, max_nodes, max_nodes, device=self.device)
        for i in range(max_nodes):
            l_self_weights[:, :, i, i] = 0.5  # å¯¹è§’çº¿æƒé‡
            for j in range(max_nodes):
                if i != j:
                    l_self_weights[:, :, i, j] = 0.5 / (max_nodes - 1)

        # é—¨æ§æƒé‡: æ–‡æœ¬ä¸»å¯¼
        gate_weights = torch.tensor([[0.5, 0.25, 0.25]], device=self.device)

        return {
            'text_to_label': t2l_weights,
            'text_to_struct': t2s_weights,
            'label_self': l_self_weights,
            'modal_weights': gate_weights,
        }


class ThresholdCalibrator:
    """
    é˜ˆå€¼æ ¡å‡†å™¨

    ä½œç”¨: åœ¨é¢„è®­ç»ƒåï¼Œæ ¹æ®éªŒè¯é›†ç»Ÿè®¡æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
    """

    def __init__(self):
        self.threshold = 0.5  # é»˜è®¤é˜ˆå€¼
        self.statistics = {}

    def calibrate(self, predicted_risks, true_labels):
        """
        æ ¡å‡†é˜ˆå€¼

        Args:
            predicted_risks: é¢„æµ‹çš„é£é™©åˆ†æ•° [N]
            true_labels: çœŸå®æ ‡ç­¾ [N], 0æˆ–1

        Returns:
            optimal_threshold: æœ€ä¼˜é˜ˆå€¼
        """
        from sklearn.metrics import roc_curve, auc

        # è®¡ç®—ROCæ›²çº¿
        fpr, tpr, thresholds = roc_curve(true_labels, predicted_risks)

        # YoudenæŒ‡æ•°: J = TPR - FPR
        youden_index = tpr - fpr
        optimal_idx = np.argmax(youden_index)
        optimal_threshold = thresholds[optimal_idx]

        self.threshold = optimal_threshold
        self.statistics = {
            'threshold': optimal_threshold,
            'tpr': tpr[optimal_idx],
            'fpr': fpr[optimal_idx],
            'auc': auc(fpr, tpr)
        }

        print(f"\nğŸ“Š é˜ˆå€¼æ ¡å‡†å®Œæˆ:")
        print(f"   æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
        print(f"   TPR: {tpr[optimal_idx]:.4f}")
        print(f"   FPR: {fpr[optimal_idx]:.4f}")
        print(f"   AUC: {auc(fpr, tpr):.4f}")

        return optimal_threshold

    def predict(self, risk_scores):
        """
        ä½¿ç”¨æ ¡å‡†åçš„é˜ˆå€¼è¿›è¡Œé¢„æµ‹

        Args:
            risk_scores: é£é™©åˆ†æ•° [N]

        Returns:
            predictions: é¢„æµ‹æ ‡ç­¾ [N], 0æˆ–1
        """
        return (risk_scores >= self.threshold).astype(int)


class FocalLoss(nn.Module):
    """Focal Loss - æ–¹å‘å…­"""

    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        """
        Args:
            inputs: [batch, num_classes]
            targets: [batch]
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()


class DiceLoss(nn.Module):
    """Dice Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""

    def __init__(self, smooth=1.0):
        super().__init__()
        self.smooth = smooth

    def forward(self, inputs, targets):
        probs = F.softmax(inputs, dim=1)
        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()

        intersection = (probs * targets_one_hot).sum(dim=0)
        union = probs.sum(dim=0) + targets_one_hot.sum(dim=0)

        dice = (2 * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice.mean()


class CombinedLoss(nn.Module):
    """ç»„åˆæŸå¤±å‡½æ•°"""

    def __init__(self, focal_weight=0.5, dice_weight=0.3, ce_weight=0.2):
        super().__init__()
        self.focal = FocalLoss()
        self.dice = DiceLoss()
        self.focal_weight = focal_weight
        self.dice_weight = dice_weight
        self.ce_weight = ce_weight

    def forward(self, inputs, targets):
        focal_loss = self.focal(inputs, targets)
        dice_loss = self.dice(inputs, targets)
        ce_loss = F.cross_entropy(inputs, targets)

        return (self.focal_weight * focal_loss +
                self.dice_weight * dice_loss +
                self.ce_weight * ce_loss)


class ModalBalanceLoss(nn.Module):
    """
    æ¨¡æ€å¹³è¡¡æŸå¤± - æ–¹å‘å…­ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    åŸºäºæ³¨æ„åŠ›æƒé‡çš„ç†µæœ€å¤§åŒ–ï¼Œå¼ºåˆ¶ä¸‰ä¸ªæ¨¡æ€è´¡çŒ®å‡è¡¡

    åŸç†ï¼š
    1. ä»è·¨æ¨¡æ€æ³¨æ„åŠ›ä¸­æå–æ¯ä¸ªæ¨¡æ€çš„è´¡çŒ®åº¦æƒé‡
    2. è®¡ç®—æƒé‡åˆ†å¸ƒçš„ç†µ
    3. ç†µè¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå‡åŒ€ï¼Œæ¨¡æ€è¶Šå¹³è¡¡
    4. é€šè¿‡æœ€å¤§åŒ–ç†µï¼Œå¼ºåˆ¶å„æ¨¡æ€å¹³è¡¡è´¡çŒ®

    æ•ˆæœï¼š
    - é˜²æ­¢Structä¸»å¯¼
    - è®©Textå…³æ³¨æ–‡æœ¬ç»†èŠ‚
    - è®©Labelå…³æ³¨ç±»åˆ«ä¿¡æ¯
    - è®©Structå…³æ³¨å®¢æˆ·ç”»åƒ
    """

    def __init__(self, weight=0.1):
        super().__init__()
        self.weight = weight
        self.epsilon = 1e-8  # é˜²æ­¢log(0)

    def forward(self, attention_weights_dict):
        """
        è®¡ç®—åŸºäºæ³¨æ„åŠ›æƒé‡çš„å¹³è¡¡æŸå¤±

        Args:
            attention_weights_dict: æ³¨æ„åŠ›æƒé‡å­—å…¸ï¼Œæ ¼å¼ï¼š
                {
                    'text_to_label': [batch, num_heads, 1, 1],
                    'label_to_text': [batch, num_heads, 1, 1],
                    'semantic_to_struct': [batch, num_heads, 1, 1],
                    'struct_to_semantic': [batch, num_heads, 1, 1]
                }

        Returns:
            balance_loss: æ¨¡æ€å¹³è¡¡æŸå¤±ï¼ˆæ ‡é‡ï¼‰
        """
        # ========== æå–å„æ¨¡æ€çš„æ³¨æ„åŠ›æƒé‡ï¼ˆå¢å¼ºç‰ˆï¼‰ ==========

        # è·å–deviceï¼ˆç”¨äºåˆ›å»ºtensorï¼‰
        device = next(iter(attention_weights_dict.values())).device if attention_weights_dict else torch.device('cpu')

        # Textçš„è´¡çŒ®åº¦ï¼šä»text_to_labelä¸­æå–
        if 'text_to_label' in attention_weights_dict and attention_weights_dict['text_to_label'] is not None:
            text_attn = attention_weights_dict['text_to_label']
            w_text = text_attn.mean()
            # æºå¤´é¢„é˜²: æ£€æŸ¥æ•°å€¼å¼‚å¸¸
            if torch.isnan(w_text) or torch.isinf(w_text):
                print("  âš ï¸ text_to_labelæƒé‡å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                w_text = torch.tensor(0.33, device=device)
        else:
            w_text = torch.tensor(0.33, device=device)  # é»˜è®¤å‡åŒ€åˆ†å¸ƒ

        # Labelçš„è´¡çŒ®åº¦ï¼šä»label_to_textä¸­æå–
        if 'label_to_text' in attention_weights_dict and attention_weights_dict['label_to_text'] is not None:
            label_attn = attention_weights_dict['label_to_text']
            w_label = label_attn.mean()
            if torch.isnan(w_label) or torch.isinf(w_label):
                print("  âš ï¸ label_to_textæƒé‡å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                w_label = torch.tensor(0.33, device=device)
        else:
            w_label = torch.tensor(0.33, device=device)

        # Structçš„è´¡çŒ®åº¦ï¼šä»struct_to_semanticä¸­æå–
        if 'struct_to_semantic' in attention_weights_dict and attention_weights_dict['struct_to_semantic'] is not None:
            struct_attn = attention_weights_dict['struct_to_semantic']
            w_struct = struct_attn.mean()
            if torch.isnan(w_struct) or torch.isinf(w_struct):
                print("  âš ï¸ struct_to_semanticæƒé‡å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                w_struct = torch.tensor(0.33, device=device)
        else:
            w_struct = torch.tensor(0.33, device=device)

        # ========== å½’ä¸€åŒ–æƒé‡ï¼ˆå˜æˆæ¦‚ç‡åˆ†å¸ƒï¼‰- å¢å¼ºç‰ˆ ==========
        weights = torch.stack([w_text, w_label, w_struct])  # [3]

        # æºå¤´é¢„é˜²: æ£€æŸ¥æƒé‡æ˜¯å¦å¼‚å¸¸
        if torch.isnan(weights).any() or torch.isinf(weights).any():
            print("  âš ï¸ æ£€æµ‹åˆ°NaNæˆ–Infï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
            weights = torch.ones(3, device=weights.device) / 3
        else:
            # ç¡®ä¿éè´Ÿ
            weights = torch.abs(weights)

            # ç¡®ä¿æƒé‡ä¸ä¼šå¤ªå°ï¼ˆé¿å…æ•°å€¼é—®é¢˜ï¼‰
            weights = weights + 1e-8

            # Softmaxå½’ä¸€åŒ–ï¼ˆç¡®ä¿å’Œä¸º1ï¼‰
            weights = F.softmax(weights, dim=0)

        # ========== æ–¹æ³•1ï¼šç†µæœ€å¤§åŒ–ï¼ˆé¼“åŠ±å‡åŒ€åˆ†å¸ƒï¼‰- å¢å¼ºç‰ˆ ==========
        # ç†µå…¬å¼ï¼šH = -Î£ p_i * log(p_i)
        # ç†æƒ³æƒ…å†µï¼šp_text = p_label = p_struct = 1/3ï¼Œæ­¤æ—¶ç†µæœ€å¤§

        entropy = -torch.sum(weights * torch.log(weights + self.epsilon))

        # æºå¤´é¢„é˜²: æ£€æŸ¥ç†µæ˜¯å¦å¼‚å¸¸
        if torch.isnan(entropy) or torch.isinf(entropy):
            print("  âš ï¸ ç†µè®¡ç®—å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            max_entropy = torch.log(torch.tensor(3.0, device=weights.device))
            entropy = max_entropy  # ä½¿ç”¨æœ€å¤§ç†µ

        # å½’ä¸€åŒ–ç†µï¼ˆæœ€å¤§ç†µä¸ºlog(3)=1.0986ï¼‰
        max_entropy = torch.log(torch.tensor(3.0, device=weights.device))
        normalized_entropy = entropy / (max_entropy + self.epsilon)

        # æŸå¤± = -ç†µï¼ˆå› ä¸ºè¦æœ€å¤§åŒ–ç†µï¼Œæ‰€ä»¥åŠ è´Ÿå·ï¼‰
        balance_loss_entropy = -entropy

        # ========== æ–¹æ³•2ï¼šMSEçº¦æŸï¼ˆå¼ºåˆ¶æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼‰==========
        # ç›®æ ‡ï¼šæ¯ä¸ªæ¨¡æ€è´¡çŒ®1/3
        target = torch.tensor(1.0 / 3.0, device=weights.device)
        balance_loss_mse = ((weights - target) ** 2).sum()

        # ========== ç»„åˆä¸¤ç§æ–¹æ³• ==========
        # ç†µæœ€å¤§åŒ–ï¼ˆä¸»è¦ï¼‰+ MSEçº¦æŸï¼ˆè¾…åŠ©ï¼‰
        balance_loss = balance_loss_entropy + 0.5 * balance_loss_mse

        # ========== å¯é€‰ï¼šæ·»åŠ æ­£åˆ™é¡¹ï¼ˆé˜²æ­¢æƒé‡è¿‡å°ï¼‰==========
        # ç¡®ä¿æ¯ä¸ªæ¨¡æ€è‡³å°‘æœ‰æœ€å°è´¡çŒ®ï¼ˆä¾‹å¦‚10%ï¼‰
        min_weight = 0.1
        penalty = torch.relu(min_weight - weights).sum()
        balance_loss = balance_loss + 0.1 * penalty

        return self.weight * balance_loss