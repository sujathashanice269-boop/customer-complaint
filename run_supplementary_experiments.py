"""
============================================================
è¡¥å……å®éªŒè„šæœ¬ - å®Œå…¨ç¬¦åˆå‚è€ƒæ–‡çŒ®æ ¼å¼ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰
============================================================

å‚è€ƒæ–‡çŒ®å¯¹ç…§ï¼š
- AAFHA Fig.3: Learning Rate Sensitivity (ç®€æ´æŠ˜çº¿å›¾ï¼Œåªæœ‰Accuracy)
- AAFHA Fig.4: Dropout Sensitivity (ç®€æ´æŠ˜çº¿å›¾)
- AAFHA Fig.5: Time Complexity (è¡¨æ ¼)
- AAFHA Fig.7: ROC Curves
- AAFHA Fig.9: Confusion Matrix
- AAFHA Fig.10-11: Integration Analysis (LIMEç‰¹å¾æƒé‡)
- AAFHA Table 5: Ablation Study (è¡¨æ ¼)
- å‡æ–°é—»è®ºæ–‡ Table 16/17: Fusion Comparison (è¡¨æ ¼)
- å‡æ–°é—»è®ºæ–‡ Table 11: Cosine Similarity (è¡¨æ ¼)

ä½¿ç”¨æ–¹æ³•ï¼š
    python run_supplementary_experiments.py --exp all
    python run_supplementary_experiments.py --exp lr_sensitivity
    ...

ä¿®å¤å†…å®¹ï¼š
- ç»´åº¦åŒ¹é…ï¼štext_featé€šè¿‡text_projä»768->256ï¼Œä¸struct_feat (256)åŒ¹é…
- ç©ºåˆ—è¡¨æ£€æŸ¥ï¼šsemantic_alignmentä¸­æ·»åŠ ç©ºåˆ—è¡¨ä¿æŠ¤
- ç‰¹å¾åç§°ï¼šç¡®ä¿53ä¸ªç‰¹å¾åç§°
"""

import os
import sys
import json
import time
import gc
import argparse
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score,
    confusion_matrix, roc_curve, auc,
    precision_score, recall_score
)
from tqdm import tqdm

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# =============================================================================
# ä¸­æ–‡å­—ä½“è®¾ç½®
# =============================================================================
def setup_font():
    """è®¾ç½®å­—ä½“ï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º"""
    font_list = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial']
    for font in font_list:
        try:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            return
        except Exception:
            continue
    plt.rcParams['axes.unicode_minus'] = False

setup_font()

# =============================================================================
# å¯¼å…¥é¡¹ç›®æ¨¡å—
# =============================================================================
try:
    from config import Config
    from data_processor import ComplaintDataProcessor, ComplaintDataset, custom_collate_fn
    from model import MultiModalComplaintModel, FocalLoss
    print("âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿è„šæœ¬æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¸config.pyåŒçº§ï¼‰")
    sys.exit(1)


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°æ€§"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å¯è®­ç»ƒå‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
    os.makedirs(path, exist_ok=True)


def safe_mean(lst):
    """å®‰å…¨è®¡ç®—å‡å€¼ï¼Œå¤„ç†ç©ºåˆ—è¡¨"""
    if not lst:
        return 0.0
    return float(np.mean(lst))


def safe_std(lst):
    """å®‰å…¨è®¡ç®—æ ‡å‡†å·®ï¼Œå¤„ç†ç©ºåˆ—è¡¨"""
    if not lst or len(lst) < 2:
        return 0.0
    return float(np.std(lst))


def safe_min(lst):
    """å®‰å…¨è®¡ç®—æœ€å°å€¼ï¼Œå¤„ç†ç©ºåˆ—è¡¨"""
    if not lst:
        return 0.0
    return float(np.min(lst))


def safe_max(lst):
    """å®‰å…¨è®¡ç®—æœ€å¤§å€¼ï¼Œå¤„ç†ç©ºåˆ—è¡¨"""
    if not lst:
        return 0.0
    return float(np.max(lst))


# =============================================================================
# æ•°æ®å‡†å¤‡
# =============================================================================
def prepare_data(config, pretrained_path=None):
    """
    å‡†å¤‡æ•°æ®é›†

    Args:
        config: é…ç½®å¯¹è±¡
        pretrained_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„

    Returns:
        train_loader, val_loader, test_loader, vocab_size, processor
    """
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = ComplaintDataProcessor(
        config=config,
        user_dict_file=config.data.user_dict_file
    )

    # å°è¯•åŠ è½½å¤„ç†å™¨çŠ¶æ€
    processor_paths = [
        './processor.pkl',
        './pretrained_complaint_bert_improved/processor.pkl',
        './pretrained_complaint_bert_improved/stage2/processor.pkl'
    ]
    if pretrained_path:
        processor_paths.insert(0, os.path.join(os.path.dirname(pretrained_path), 'processor.pkl'))

    for path in processor_paths:
        if os.path.exists(path):
            try:
                processor.load(path)
                print(f"âœ… åŠ è½½å¤„ç†å™¨: {path}")
                break
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤„ç†å™¨å¤±è´¥ {path}: {e}")

    # å‡†å¤‡æ•°æ®
    data = processor.prepare_datasets(
        train_file=config.training.data_file,
        for_pretrain=False
    )

    vocab_size = data.get('vocab_size', len(processor.node_to_id) + 1)

    # åˆ’åˆ†æ•°æ® (60% è®­ç»ƒ, 20% éªŒè¯, 20% æµ‹è¯•)
    total_size = len(data['targets'])
    indices = torch.randperm(total_size).tolist()

    train_size = int(total_size * 0.6)
    val_size = int(total_size * 0.2)

    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]

    def split_data(data_dict, idx_list):
        """æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®"""
        return {
            'text_data': {
                'input_ids': data_dict['text_data']['input_ids'][idx_list],
                'attention_mask': data_dict['text_data']['attention_mask'][idx_list]
            },
            'node_ids_list': [data_dict['node_ids_list'][i] for i in idx_list],
            'edges_list': [data_dict['edges_list'][i] for i in idx_list],
            'node_levels_list': [data_dict['node_levels_list'][i] for i in idx_list],
            'struct_features': data_dict['struct_features'][idx_list],
            'targets': data_dict['targets'][idx_list]
        }

    train_data = split_data(data, train_indices)
    val_data = split_data(data, val_indices)
    test_data = split_data(data, test_indices)

    # åˆ›å»ºDataset
    train_dataset = ComplaintDataset(
        train_data['text_data'], train_data['node_ids_list'],
        train_data['edges_list'], train_data['node_levels_list'],
        train_data['struct_features'], train_data['targets']
    )
    val_dataset = ComplaintDataset(
        val_data['text_data'], val_data['node_ids_list'],
        val_data['edges_list'], val_data['node_levels_list'],
        val_data['struct_features'], val_data['targets']
    )
    test_dataset = ComplaintDataset(
        test_data['text_data'], test_data['node_ids_list'],
        test_data['edges_list'], test_data['node_levels_list'],
        test_data['struct_features'], test_data['targets']
    )

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.training.batch_size,
        shuffle=True,
        collate_fn=custom_collate_fn,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.training.batch_size,
        collate_fn=custom_collate_fn
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.training.batch_size,
        collate_fn=custom_collate_fn
    )

    print(f"  è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}, æµ‹è¯•é›†: {len(test_dataset)}")

    return train_loader, val_loader, test_loader, vocab_size, processor


def quick_train_and_evaluate(model, train_loader, val_loader, test_loader, config, num_epochs=10):
    """
    å¿«é€Ÿè®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        config: é…ç½®
        num_epochs: è®­ç»ƒè½®æ•°

    Returns:
        metrics, all_preds, all_probs, all_targets
    """
    device = config.training.device
    model = model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)

    if config.training.use_focal_loss:
        criterion = FocalLoss()
    else:
        criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒ
    for epoch in range(num_epochs):
        model.train()
        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            struct_features = batch['struct_features'].to(device)
            targets = batch['target'].to(device)

            logits, _ = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                node_ids_list=batch['node_ids'],
                edges_list=batch['edges'],
                node_levels_list=batch['node_levels'],
                struct_features=struct_features
            )

            loss = criterion(logits, targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
            optimizer.step()

    # æµ‹è¯•
    model.eval()
    all_preds = []
    all_probs = []
    all_targets = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            struct_features = batch['struct_features'].to(device)

            logits, _ = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                node_ids_list=batch['node_ids'],
                edges_list=batch['edges'],
                node_levels_list=batch['node_levels'],
                struct_features=struct_features
            )

            probs = torch.softmax(logits, dim=1)
            all_probs.extend(probs[:, 1].cpu().numpy())
            all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            all_targets.extend(batch['target'].numpy())

    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'accuracy': accuracy_score(all_targets, all_preds),
        'precision': precision_score(all_targets, all_preds, zero_division=0),
        'recall': recall_score(all_targets, all_preds, zero_division=0),
        'f1': f1_score(all_targets, all_preds, zero_division=0),
        'auc': roc_auc_score(all_targets, all_probs) if len(set(all_targets)) > 1 else 0.5
    }

    return metrics, all_preds, all_probs, all_targets


# =============================================================================
# å®éªŒ1: å­¦ä¹ ç‡æ•æ„Ÿæ€§ (å‚è€ƒ AAFHA Fig.3)
# =============================================================================
def run_lr_sensitivity(config, pretrained_path, save_dir):
    """
    å­¦ä¹ ç‡æ•æ„Ÿæ€§åˆ†æ
    å‚è€ƒAAFHA Fig.3æ ¼å¼ï¼šç®€æ´æŠ˜çº¿å›¾ï¼Œåªå±•ç¤ºAccuracy
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Learning Rate Sensitivity (å‚è€ƒAAFHA Fig.3)")
    print("=" * 60)

    set_seed(42)

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    learning_rates = [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]
    results = {}

    # ä¿å­˜åŸå§‹å­¦ä¹ ç‡
    original_lr = config.training.learning_rate

    for lr in learning_rates:
        print(f"\n>>> Learning Rate: {lr}")
        config.training.learning_rate = lr

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode='full',
            pretrained_path=pretrained_path
        )

        metrics, _, _, _ = quick_train_and_evaluate(
            model, train_loader, val_loader, test_loader, config, num_epochs=10
        )

        results[lr] = metrics
        print(f"  Accuracy: {metrics['accuracy']:.4f}")

        del model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # æ¢å¤åŸå§‹å­¦ä¹ ç‡
    config.training.learning_rate = original_lr

    # ========== ç»˜å›¾ (AAFHA Fig.3 é£æ ¼) ==========
    fig, ax = plt.subplots(figsize=(8, 5))

    lrs = list(results.keys())
    accs = [results[lr]['accuracy'] for lr in lrs]

    ax.plot(range(len(lrs)), accs, 'b-o', linewidth=2, markersize=8, label='Our Dataset')

    ax.set_xlabel('Learning Rate', fontsize=12)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_title('Impact of Learning Rate on Model Accuracy', fontsize=14)
    ax.set_xticks(range(len(lrs)))
    ax.set_xticklabels([f'{lr:.0e}' for lr in lrs])
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # è®¾ç½®Yè½´èŒƒå›´
    y_min = min(accs) - 0.05
    y_max = max(accs) + 0.02
    ax.set_ylim(max(0, y_min), min(1, y_max))

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'lr_sensitivity.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"\nâœ… ä¿å­˜: lr_sensitivity.png")

    return results


# =============================================================================
# å®éªŒ2: Dropoutæ•æ„Ÿæ€§ (å‚è€ƒ AAFHA Fig.4)
# =============================================================================
def run_dropout_sensitivity(config, pretrained_path, save_dir):
    """
    Dropoutæ•æ„Ÿæ€§åˆ†æ
    å‚è€ƒAAFHA Fig.4æ ¼å¼ï¼šç®€æ´æŠ˜çº¿å›¾
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Dropout Sensitivity (å‚è€ƒAAFHA Fig.4)")
    print("=" * 60)

    set_seed(42)

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]
    results = {}

    # ä¿å­˜åŸå§‹dropout
    original_dropout = config.model.dropout

    for dropout in dropout_rates:
        print(f"\n>>> Dropout Rate: {dropout}")
        config.model.dropout = dropout

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode='full',
            pretrained_path=pretrained_path
        )

        metrics, _, _, _ = quick_train_and_evaluate(
            model, train_loader, val_loader, test_loader, config, num_epochs=10
        )

        results[dropout] = metrics
        print(f"  Accuracy: {metrics['accuracy']:.4f}")

        del model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # æ¢å¤åŸå§‹dropout
    config.model.dropout = original_dropout

    # ========== ç»˜å›¾ (AAFHA Fig.4 é£æ ¼) ==========
    fig, ax = plt.subplots(figsize=(8, 5))

    dropouts = list(results.keys())
    accs = [results[d]['accuracy'] for d in dropouts]

    ax.plot(dropouts, accs, 'b-o', linewidth=2, markersize=8, label='Our Dataset')

    ax.set_xlabel('Dropout Rate', fontsize=12)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_title('Impact of Dropout Rates on Accuracy', fontsize=14)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    y_min = min(accs) - 0.05
    y_max = max(accs) + 0.02
    ax.set_ylim(max(0, y_min), min(1, y_max))

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'dropout_sensitivity.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"\nâœ… ä¿å­˜: dropout_sensitivity.png")

    return results


# =============================================================================
# å®éªŒ3: èåˆæ–¹å¼æ¯”è¾ƒ (å‚è€ƒå‡æ–°é—»è®ºæ–‡ Table 16/17)
# =============================================================================
def run_fusion_comparison(config, pretrained_path, save_dir):
    """
    èåˆæ–¹å¼æ¯”è¾ƒ
    å‚è€ƒå‡æ–°é—»è®ºæ–‡Table 16/17æ ¼å¼ï¼š**è¡¨æ ¼å½¢å¼**
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Fusion Method Comparison (å‚è€ƒTable 16/17)")
    print("=" * 60)

    set_seed(42)

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    # æµ‹è¯•ä¸åŒèåˆæ–¹å¼
    fusion_methods = {
        'Text+Label': 'text_label',
        'Text+Struct': 'text_struct',
        'Label+Struct': 'label_struct',
        'Full Model (Cross-Attention)': 'full'
    }

    results = {}

    for name, mode in fusion_methods.items():
        print(f"\n>>> Testing: {name}")

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode=mode,
            pretrained_path=pretrained_path
        )

        metrics, _, _, _ = quick_train_and_evaluate(
            model, train_loader, val_loader, test_loader, config, num_epochs=10
        )

        results[name] = metrics
        print(f"  Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, AUC: {metrics['auc']:.4f}")

        del model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ========== ç”Ÿæˆè¡¨æ ¼ (Table 16/17 é£æ ¼) ==========
    table_data = []
    for method, metrics in results.items():
        table_data.append({
            'Model': method,
            'Accuracy': f"{metrics['accuracy'] * 100:.2f}",
            'Precision': f"{metrics['precision'] * 100:.2f}",
            'Recall': f"{metrics['recall'] * 100:.2f}",
            'F1 Score': f"{metrics['f1'] * 100:.2f}",
            'AUC': f"{metrics['auc'] * 100:.2f}"
        })

    df = pd.DataFrame(table_data)

    # ä¿å­˜ä¸ºCSV
    df.to_csv(os.path.join(save_dir, 'fusion_comparison_table.csv'), index=False)

    # ç”ŸæˆLaTeXè¡¨æ ¼
    latex_table = df.to_latex(
        index=False,
        caption='Comparative study on different fusion models',
        label='tab:fusion_comparison'
    )
    with open(os.path.join(save_dir, 'fusion_comparison_table.tex'), 'w', encoding='utf-8') as f:
        f.write(latex_table)

    print(f"\nâœ… ä¿å­˜: fusion_comparison_table.csv, fusion_comparison_table.tex")
    print("\nğŸ“‹ èåˆæ–¹å¼æ¯”è¾ƒè¡¨æ ¼:")
    print(df.to_string(index=False))

    return results


# =============================================================================
# å®éªŒ4: æ—¶é—´å¤æ‚åº¦åˆ†æ (å‚è€ƒ AAFHA 4.8èŠ‚)
# =============================================================================
def run_time_complexity(config, pretrained_path, save_dir):
    """
    æ—¶é—´å¤æ‚åº¦åˆ†æ
    å‚è€ƒAAFHA 4.8èŠ‚æ ¼å¼ï¼š**è¡¨æ ¼å½¢å¼**
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Time Complexity Analysis (å‚è€ƒAAFHA 4.8èŠ‚)")
    print("=" * 60)

    set_seed(42)
    device = config.training.device

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    modes = {
        'Text Only': 'text_only',
        'Label Only': 'label_only',
        'Struct Only': 'struct_only',
        'Text+Label': 'text_label',
        'Text+Struct': 'text_struct',
        'Full Model': 'full'
    }

    results = {}

    for name, mode in modes.items():
        print(f"\n>>> Testing: {name}")

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode=mode,
            pretrained_path=pretrained_path
        )
        model = model.to(device)
        model.eval()

        # å‚æ•°é‡
        num_params = count_parameters(model)

        # æ¨ç†æ—¶é—´æµ‹é‡
        inference_times = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                struct_features = batch['struct_features'].to(device)

                # é¢„çƒ­
                _ = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=batch['node_ids'],
                    edges_list=batch['edges'],
                    node_levels_list=batch['node_levels'],
                    struct_features=struct_features
                )

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                # è®¡æ—¶
                start_time = time.time()
                for _ in range(5):
                    _ = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        node_ids_list=batch['node_ids'],
                        edges_list=batch['edges'],
                        node_levels_list=batch['node_levels'],
                        struct_features=struct_features
                    )

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                end_time = time.time()
                batch_size = input_ids.shape[0]
                avg_time = (end_time - start_time) / 5 / batch_size * 1000  # ms per sample
                inference_times.append(avg_time)

                if batch_idx >= 2:  # åªæµ‹3ä¸ªbatch
                    break

        results[name] = {
            'parameters': num_params,
            'parameters_M': num_params / 1e6,
            'inference_time_ms': safe_mean(inference_times)
        }

        print(f"  Parameters: {num_params / 1e6:.2f}M, Inference: {safe_mean(inference_times):.2f}ms")

        del model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ========== ç”Ÿæˆè¡¨æ ¼ ==========
    table_data = []
    for name, data in results.items():
        table_data.append({
            'Model': name,
            'Parameters (M)': f"{data['parameters_M']:.2f}",
            'Inference Time (ms)': f"{data['inference_time_ms']:.2f}"
        })

    df = pd.DataFrame(table_data)
    df.to_csv(os.path.join(save_dir, 'time_complexity_table.csv'), index=False)

    # LaTeX
    latex_table = df.to_latex(
        index=False,
        caption='Time complexity analysis',
        label='tab:time_complexity'
    )
    with open(os.path.join(save_dir, 'time_complexity_table.tex'), 'w', encoding='utf-8') as f:
        f.write(latex_table)

    print(f"\nâœ… ä¿å­˜: time_complexity_table.csv, time_complexity_table.tex")
    print("\nğŸ“‹ æ—¶é—´å¤æ‚åº¦è¡¨æ ¼:")
    print(df.to_string(index=False))

    return results


# =============================================================================
# å®éªŒ5: æ··æ·†çŸ©é˜µä¸ROCæ›²çº¿ (å‚è€ƒ AAFHA Fig.7, Fig.9)
# =============================================================================
def run_confusion_matrix_roc(config, pretrained_path, save_dir):
    """
    æ··æ·†çŸ©é˜µä¸ROCæ›²çº¿
    å‚è€ƒAAFHA Fig.7 (ROC) å’Œ Fig.9 (Confusion Matrix)
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Confusion Matrix & ROC Curve (å‚è€ƒAAFHA Fig.7, Fig.9)")
    print("=" * 60)

    set_seed(42)

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    model = MultiModalComplaintModel(
        config=config,
        vocab_size=vocab_size,
        mode='full',
        pretrained_path=pretrained_path
    )

    metrics, all_preds, all_probs, all_targets = quick_train_and_evaluate(
        model, train_loader, val_loader, test_loader, config, num_epochs=10
    )

    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  Accuracy: {metrics['accuracy']:.4f}")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  Recall: {metrics['recall']:.4f}")
    print(f"  F1: {metrics['f1']:.4f}")
    print(f"  AUC: {metrics['auc']:.4f}")

    # ========== ç»˜å›¾ ==========
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_targets, all_preds)
    ax = axes[0]
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues', ax=ax,
        xticklabels=['Non-Repeat', 'Repeat'],
        yticklabels=['Non-Repeat', 'Repeat']
    )
    ax.set_xlabel('Predicted', fontsize=11)
    ax.set_ylabel('Actual', fontsize=11)
    ax.set_title('Confusion Matrix', fontsize=12)

    # ROCæ›²çº¿
    ax = axes[1]
    fpr, tpr, _ = roc_curve(all_targets, all_probs)
    roc_auc = auc(fpr, tpr)

    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {roc_auc:.4f}')
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)
    ax.fill_between(fpr, tpr, alpha=0.2)

    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate', fontsize=11)
    ax.set_ylabel('True Positive Rate', fontsize=11)
    ax.set_title('ROC Curve', fontsize=12)
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'confusion_matrix_roc.png'), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"\nâœ… ä¿å­˜: confusion_matrix_roc.png")

    del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return metrics


# =============================================================================
# å®éªŒ6: æ¨¡æ€è¯­ä¹‰å¯¹é½ (å‚è€ƒå‡æ–°é—»è®ºæ–‡ Table 11)
# ç»´åº¦è¯´æ˜ï¼š
#   - text_feat_raw: [batch, 768] (BERT CLSè¾“å‡º)
#   - text_feat: [batch, 256] (é€šè¿‡text_projæŠ•å½±)
#   - struct_feat: [batch, 256] (é€šè¿‡struct_encoderç¼–ç )
#   - ä¸¤è€…ç»´åº¦åŒ¹é…ï¼Œå¯ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
# =============================================================================
def run_semantic_alignment(config, pretrained_path, save_dir):
    """
    æ¨¡æ€è¯­ä¹‰å¯¹é½åˆ†æ
    å‚è€ƒå‡æ–°é—»è®ºæ–‡Table 11æ ¼å¼ï¼š**ä½™å¼¦ç›¸ä¼¼åº¦è¡¨æ ¼**
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Semantic Alignment (å‚è€ƒTable 11)")
    print("=" * 60)

    set_seed(42)
    device = config.training.device

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    model = MultiModalComplaintModel(
        config=config,
        vocab_size=vocab_size,
        mode='full',
        pretrained_path=pretrained_path
    )
    model = model.to(device)
    model.eval()

    # æ”¶é›†ç›¸ä¼¼åº¦
    similarities_repeat = []
    similarities_non_repeat = []

    def cosine_sim(a, b):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a < 1e-8 or norm_b < 1e-8:
            return 0.0
        return float(np.dot(a, b) / (norm_a * norm_b))

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            struct_features = batch['struct_features'].to(device)
            targets = batch['target'].numpy()

            # è·å–æ–‡æœ¬ç‰¹å¾ (BERTè¾“å‡º)
            # text_output.last_hidden_state: [batch, seq_len, 768]
            # text_feat_raw: [batch, 768] (CLS token)
            text_output = model.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
            text_feat_raw = text_output.last_hidden_state[:, 0, :]  # [batch, 768]

            # æŠ•å½±åˆ°256ç»´ (ä½¿ç”¨æ¨¡å‹çš„text_projå±‚: 768 -> 256)
            # text_feat: [batch, 256]
            text_feat = model.text_proj(text_feat_raw).cpu().numpy()

            # è·å–ç»“æ„åŒ–ç‰¹å¾ (é€šè¿‡struct_encoder: 53 -> 256)
            # struct_feat: [batch, 256]
            struct_feat = model.struct_encoder(struct_features).cpu().numpy()

            # é€æ ·æœ¬è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç»´åº¦éƒ½æ˜¯256ï¼ŒåŒ¹é…ï¼ï¼‰
            for i, target in enumerate(targets):
                sim = cosine_sim(text_feat[i], struct_feat[i])

                if target == 1:
                    similarities_repeat.append(sim)
                else:
                    similarities_non_repeat.append(sim)

    # ========== ç”Ÿæˆè¡¨æ ¼ (Table 11 é£æ ¼) ==========
    # ä½¿ç”¨å®‰å…¨å‡½æ•°å¤„ç†å¯èƒ½çš„ç©ºåˆ—è¡¨
    table_data = [
        {
            'Category': 'Repeat Complaint',
            'Mean Similarity': f"{safe_mean(similarities_repeat):.4f}",
            'Std': f"{safe_std(similarities_repeat):.4f}",
            'Min': f"{safe_min(similarities_repeat):.4f}",
            'Max': f"{safe_max(similarities_repeat):.4f}",
            'Count': len(similarities_repeat)
        },
        {
            'Category': 'Non-Repeat Complaint',
            'Mean Similarity': f"{safe_mean(similarities_non_repeat):.4f}",
            'Std': f"{safe_std(similarities_non_repeat):.4f}",
            'Min': f"{safe_min(similarities_non_repeat):.4f}",
            'Max': f"{safe_max(similarities_non_repeat):.4f}",
            'Count': len(similarities_non_repeat)
        }
    ]

    df = pd.DataFrame(table_data)
    df.to_csv(os.path.join(save_dir, 'semantic_alignment_table.csv'), index=False)

    # LaTeX
    latex_table = df.to_latex(
        index=False,
        caption='Cosine similarity between text and structured features',
        label='tab:semantic_alignment'
    )
    with open(os.path.join(save_dir, 'semantic_alignment_table.tex'), 'w', encoding='utf-8') as f:
        f.write(latex_table)

    print(f"\nâœ… ä¿å­˜: semantic_alignment_table.csv, semantic_alignment_table.tex")
    print("\nğŸ“‹ è¯­ä¹‰å¯¹é½è¡¨æ ¼:")
    print(df.to_string(index=False))

    del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return {
        'repeat': {'mean': safe_mean(similarities_repeat), 'std': safe_std(similarities_repeat)},
        'non_repeat': {'mean': safe_mean(similarities_non_repeat), 'std': safe_std(similarities_non_repeat)}
    }


# =============================================================================
# å®éªŒ7: LIMEå¯è§£é‡Šæ€§åˆ†æ (å‚è€ƒ AAFHA Fig.10-11)
# =============================================================================
# ç»“æ„åŒ–ç‰¹å¾åç§°ï¼ˆå…±53ä¸ªï¼Œä¸config.model.struct_feat_dim=53å¯¹åº”ï¼‰
STRUCT_FEATURE_NAMES = [
    'Channel', 'Credit', 'Global_Level', 'Upgrade', 'Satisfaction_Time',
    'Urgency_Time', 'Urgency_Accept', 'Transparency', 'Old_User_Online',
    'Policy_Satisfaction', 'New_User_Online', 'New_User_Store', 'Promotion',
    'Network_Satisfaction', 'Performance', 'Service_Usage', 'New_User_Hotline',
    'Expectation', 'Old_User_Hotline', 'Old_User_Store', 'Network_Complaint',
    'NPS_Score', 'Channel_Complaint', 'Other_Complaint', 'No_Complaint',
    'Marketing_Complaint', 'Professionalism', 'Timeliness', 'Result_Satisfaction',
    'Overall_Satisfaction', 'Phone_Status', 'Package_Brand', 'Age', 'Tenure_Months',
    'VIP_Level', 'DND', 'Dual_Card', 'Phone_Brand', 'Campus_User', 'Volte_Potential',
    'Price_Sensitive', 'No_Broadband', 'Competitor_Broadband', 'Card_Apply',
    'Card_Potential', 'Migrant_Worker', 'Other_Return', 'Return_User',
    'Respondent', 'Customer_Segment', 'Gender', 'Feature_52', 'Feature_53'
]  # å…±53ä¸ª

def run_lime_analysis(config, pretrained_path, save_dir):
    """
    LIMEå¯è§£é‡Šæ€§åˆ†æ
    å‚è€ƒAAFHA Fig.10-11æ ¼å¼ï¼šå±•ç¤ºtop Kç‰¹å¾çš„æƒé‡æ¡å½¢å›¾
    """
    print("\n" + "=" * 60)
    print("å®éªŒ: Integration Analysis / LIME (å‚è€ƒAAFHA Fig.10-11)")
    print("=" * 60)

    set_seed(42)
    device = config.training.device

    train_loader, val_loader, test_loader, vocab_size, processor = prepare_data(config, pretrained_path)

    # ä½¿ç”¨å•æ ·æœ¬batch
    test_loader_single = DataLoader(
        test_loader.dataset,
        batch_size=1,
        collate_fn=custom_collate_fn
    )

    model = MultiModalComplaintModel(
        config=config,
        vocab_size=vocab_size,
        mode='full',
        pretrained_path=pretrained_path
    )
    model = model.to(device)
    model.eval()

    def compute_feature_contributions(batch):
        """è®¡ç®—ç‰¹å¾è´¡çŒ®åº¦ï¼ˆæ‰°åŠ¨æ³•ï¼‰"""
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        struct_features = batch['struct_features'].to(device)

        # è·å–åŸå§‹é¢„æµ‹æ¦‚ç‡
        with torch.no_grad():
            logits, _ = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                node_ids_list=batch['node_ids'],
                edges_list=batch['edges'],
                node_levels_list=batch['node_levels'],
                struct_features=struct_features
            )
            orig_probs = torch.softmax(logits, dim=1)
            orig_prob = orig_probs[0, 1].item()  # é‡å¤æŠ•è¯‰çš„æ¦‚ç‡

        contributions = []
        num_features = struct_features.shape[1]  # åº”è¯¥æ˜¯53

        # æ‰°åŠ¨æ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—è´¡çŒ®åº¦
        for i in range(num_features):
            # å…‹éš†å¹¶æ‰°åŠ¨
            perturbed = struct_features.clone()
            perturbed[0, i] = 0  # å°†è¯¥ç‰¹å¾ç½®é›¶

            with torch.no_grad():
                logits_p, _ = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=batch['node_ids'],
                    edges_list=batch['edges'],
                    node_levels_list=batch['node_levels'],
                    struct_features=perturbed
                )
                new_prob = torch.softmax(logits_p, dim=1)[0, 1].item()

            contribution = orig_prob - new_prob  # æ­£å€¼è¡¨ç¤ºè¯¥ç‰¹å¾å¢åŠ é‡å¤æŠ•è¯‰æ¦‚ç‡

            # è·å–ç‰¹å¾åç§°
            if i < len(STRUCT_FEATURE_NAMES):
                name = STRUCT_FEATURE_NAMES[i]
            else:
                name = f'Feature_{i}'

            contributions.append((name, contribution))

        # æŒ‰è´¡çŒ®åº¦ç»å¯¹å€¼æ’åº
        contributions_sorted = sorted(contributions, key=lambda x: abs(x[1]), reverse=True)
        return orig_prob, contributions_sorted

    # å¯»æ‰¾å…¸å‹æ¡ˆä¾‹
    print("\nå¯»æ‰¾å…¸å‹æ¡ˆä¾‹...")
    repeat_case = None
    non_repeat_case = None

    for batch in test_loader_single:
        target = batch['target'].item()
        if target == 1 and repeat_case is None:
            orig_prob, contribs = compute_feature_contributions(batch)
            repeat_case = {'prob': orig_prob, 'contributions': contribs}
            print(f"  æ‰¾åˆ°é‡å¤æŠ•è¯‰æ¡ˆä¾‹, prob={orig_prob:.4f}")
        elif target == 0 and non_repeat_case is None:
            orig_prob, contribs = compute_feature_contributions(batch)
            non_repeat_case = {'prob': orig_prob, 'contributions': contribs}
            print(f"  æ‰¾åˆ°éé‡å¤æŠ•è¯‰æ¡ˆä¾‹, prob={orig_prob:.4f}")

        if repeat_case and non_repeat_case:
            break

    # ========== ç»˜å›¾ (AAFHA Fig.10-11 é£æ ¼) ==========
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    cases = [
        (axes[0], repeat_case, 'Repeat Complaint'),
        (axes[1], non_repeat_case, 'Non-Repeat Complaint')
    ]

    for ax, case, title in cases:
        if case is None:
            ax.text(0.5, 0.5, 'No data', ha='center', va='center', fontsize=14)
            ax.set_title(title, fontsize=12)
            continue

        top_k = 10
        top_features = case['contributions'][:top_k]

        names = [f[0][:15] for f in top_features]  # æˆªæ–­è¿‡é•¿çš„åç§°
        values = [f[1] for f in top_features]
        colors = ['#e74c3c' if v > 0 else '#3498db' for v in values]

        y_pos = np.arange(len(names))
        ax.barh(y_pos, values, color=colors, edgecolor='white')

        ax.set_yticks(y_pos)
        ax.set_yticklabels(names, fontsize=9)
        ax.set_xlabel('Contribution Weight', fontsize=11)
        ax.set_title(f'{title}\n(Pred Prob: {case["prob"]:.4f})', fontsize=12)
        ax.axvline(x=0, color='black', linewidth=0.8)
        ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'lime_integration_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"\nâœ… ä¿å­˜: lime_integration_analysis.png")

    del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return {'repeat': repeat_case, 'non_repeat': non_repeat_case}


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================
def main():
    parser = argparse.ArgumentParser(description='è¡¥å……å®éªŒè„šæœ¬ï¼ˆç¬¦åˆå‚è€ƒæ–‡çŒ®æ ¼å¼ï¼‰')
    parser.add_argument(
        '--exp',
        type=str,
        default='all',
        choices=['all', 'lr_sensitivity', 'dropout_sensitivity',
                 'fusion', 'time_complexity', 'confusion_matrix',
                 'semantic_alignment', 'lime'],
        help='è¦è¿è¡Œçš„å®éªŒ'
    )
    parser.add_argument(
        '--pretrained_path',
        type=str,
        default='./pretrained_complaint_bert_improved/stage2',
        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„'
    )
    parser.add_argument(
        '--save_dir',
        type=str,
        default='./supplementary_results',
        help='ç»“æœä¿å­˜ç›®å½•'
    )

    args = parser.parse_args()

    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    ensure_dir(args.save_dir)

    # åŠ è½½é…ç½®
    config = Config()

    print("\n" + "=" * 70)
    print("ğŸ§ª è¡¥å……å®éªŒè„šæœ¬ï¼ˆå‚è€ƒæ–‡çŒ®æ ¼å¼ï¼‰")
    print("=" * 70)
    print(f"è®¾å¤‡: {config.training.device}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print(f"å®éªŒ: {args.exp}")
    print("=" * 70)

    # å®éªŒæ˜ å°„
    experiments = {
        'lr_sensitivity': ('å­¦ä¹ ç‡æ•æ„Ÿæ€§ (AAFHA Fig.3)', run_lr_sensitivity),
        'dropout_sensitivity': ('Dropoutæ•æ„Ÿæ€§ (AAFHA Fig.4)', run_dropout_sensitivity),
        'fusion': ('èåˆæ–¹å¼æ¯”è¾ƒ (Table 16/17)', run_fusion_comparison),
        'time_complexity': ('æ—¶é—´å¤æ‚åº¦ (AAFHA 4.8èŠ‚)', run_time_complexity),
        'confusion_matrix': ('æ··æ·†çŸ©é˜µ+ROC (AAFHA Fig.7,9)', run_confusion_matrix_roc),
        'semantic_alignment': ('è¯­ä¹‰å¯¹é½ (Table 11)', run_semantic_alignment),
        'lime': ('LIMEåˆ†æ (AAFHA Fig.10-11)', run_lime_analysis),
    }

    all_results = {}

    # ç¡®å®šè¦è¿è¡Œçš„å®éªŒ
    if args.exp == 'all':
        exp_list = list(experiments.keys())
    else:
        exp_list = [args.exp]

    # è¿è¡Œå®éªŒ
    for exp_name in exp_list:
        if exp_name in experiments:
            desc, func = experiments[exp_name]
            print(f"\n{'=' * 60}")
            print(f"ğŸ”¬ è¿è¡Œ: {desc}")
            print(f"{'=' * 60}")
            try:
                result = func(config, args.pretrained_path, args.save_dir)
                all_results[exp_name] = result
            except Exception as e:
                print(f"âŒ å®éªŒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    # ä¿å­˜æ‰€æœ‰ç»“æœ
    def convert_serializable(obj):
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_serializable(i) for i in obj]
        return obj

    results_path = os.path.join(args.save_dir, 'all_results.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(convert_serializable(all_results), f, ensure_ascii=False, indent=2, default=str)
    
    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
    print("=" * 70)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for filename in sorted(os.listdir(args.save_dir)):
        print(f"  - {filename}")


if __name__ == "__main__":
    main()