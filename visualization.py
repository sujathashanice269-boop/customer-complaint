"""
Visualization Tools - visualization.py
For visualizing model attention weights, feature importance, etc.
Complete English version with all functions
"""

import torch
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import networkx as nx
import os
import warnings
# ===== å¤åˆ¶å¼€å§‹ =====
from functools import lru_cache
from transformers import BertTokenizer
import jieba

# ============================================================
# ç¿»è¯‘æ¨¡å— - ç¦»çº¿ç¿»è¯‘å™¨
# ============================================================

# ============================================================
# ç¿»è¯‘æ¨¡å— - æ‰‹åŠ¨å­—å…¸ä¼˜å…ˆ + MarianMTè‡ªåŠ¨ç¿»è¯‘å…œåº•
# ============================================================

class SmartTranslator:
    """æ™ºèƒ½ç¿»è¯‘å™¨ - æ‰‹åŠ¨å­—å…¸ä¼˜å…ˆï¼Œå­—å…¸å¤–ä½¿ç”¨MarianMTè‡ªåŠ¨ç¿»è¯‘"""

    BUILTIN_DICT = {
        # åŸºç¡€ç½‘ç»œç›¸å…³
        "ç½‘ç»œ": "Network", "ä¿¡å·": "Signal", "ç½‘é€Ÿ": "Speed",
        "æ–­ç½‘": "Offline", "æ‰çº¿": "Drop", "å¡é¡¿": "Lag",
        "å»¶è¿Ÿ": "Delay", "è¶…æ—¶": "Timeout", "è¿æ¥": "Connect",
        "ä¸Šç½‘": "Internet", "å®½å¸¦": "Broadband", "WiFi": "WiFi",
        "4G": "4G", "5G": "5G", "åŸºç«™": "Station", "è¦†ç›–": "Coverage",
        "ç½‘ç»œé—®é¢˜": "NetworkIssue", "ä¿¡å·å·®": "WeakSignal",
        "ç½‘ç»œæ•…éšœ": "NetworkFault", "ç½‘ç»œæ…¢": "SlowNetwork",
        # è´¹ç”¨ç›¸å…³
        "é€€è´¹": "Refund", "é€€æ¬¾": "Refund", "æ‰£è´¹": "Charge",
        "è´¹ç”¨": "Fee", "è¯è´¹": "Bill", "æµé‡": "Data",
        "å¥—é¤": "Plan", "èµ„è´¹": "Tariff", "è´¦å•": "Bill",
        "ä½™é¢": "Balance", "æ¬ è´¹": "Arrears", "å……å€¼": "Topup",
        "è®¡è´¹": "Billing", "å¤šæ‰£": "Overcharge", "ä¹±æ‰£": "WrongCharge",
        "èµ„è´¹äº‰è®®": "BillingDispute", "å¥—é¤é—®é¢˜": "PlanIssue",
        "è´¦æˆ·é—®é¢˜": "AccountIssue", "é€€è´¹ç”³è¯·": "RefundRequest",
        # æœåŠ¡ç›¸å…³
        "æœåŠ¡": "Service", "å®¢æœ": "Support", "æŠ•è¯‰": "Complaint",
        "å¤„ç†": "Handle", "è§£å†³": "Resolve", "å›å¤": "Reply",
        "å“åº”": "Response", "ç­‰å¾…": "Wait", "æ€åº¦": "Attitude",
        "æœåŠ¡è´¨é‡": "ServiceQuality", "å“åº”é€Ÿåº¦": "ResponseSpeed",
        "å¤„ç†è¶…æ—¶": "HandleTimeout", "æœåŠ¡æ€åº¦": "ServiceAttitude",
        # é—®é¢˜çŠ¶æ€
        "é—®é¢˜": "Issue", "æ•…éšœ": "Fault", "é”™è¯¯": "Error",
        "å¼‚å¸¸": "Abnormal", "æ— æ³•": "Cannot", "ä¸èƒ½": "Cannot",
        "å¤±è´¥": "Fail", "æ…¢": "Slow", "å·®": "Poor", "å¼±": "Weak",
        # æ“ä½œåŠ¨ä½œ
        "åæ˜ ": "Report", "å’¨è¯¢": "Consult", "æŸ¥è¯¢": "Query",
        "åŠç†": "Apply", "ç”³è¯·": "Request", "å–æ¶ˆ": "Cancel",
        "å˜æ›´": "Change", "è¦æ±‚": "Demand", "éœ€è¦": "Need",
        "ä¸šåŠ¡åŠç†": "Business", "ä¸šåŠ¡": "Business",
        # ç”¨æˆ·è®¾å¤‡
        "ç”¨æˆ·": "User", "å®¢æˆ·": "Customer", "æ‰‹æœº": "Phone",
        "ç”µè¯": "Call", "çŸ­ä¿¡": "SMS", "å·ç ": "Number", "è®¾å¤‡": "Device",
        "ç§»åŠ¨": "Mobile", "è”é€š": "Unicom", "ç”µä¿¡": "Telecom",
        # ä¸šåŠ¡ç±»å‹
        "å®½å¸¦æ•…éšœ": "BroadbandFault", "ç§»åŠ¨ç½‘ç»œ": "MobileNetwork",
        "å›ºç½‘": "FixedNetwork", "å¢å€¼ä¸šåŠ¡": "VAS", "å›½é™…æ¼«æ¸¸": "Roaming",
        "æºå·è½¬ç½‘": "PortNumber", "å®åè®¤è¯": "RealName",
        "åˆçº¦": "Contract", "è¿çº¦é‡‘": "Penalty", "æŠ¼é‡‘": "Deposit",
        "å‘ç¥¨": "Invoice", "ç§¯åˆ†": "Points", "ä¼˜æƒ ": "Discount",
        "ä¿ƒé”€": "Promotion", "æ´»åŠ¨": "Campaign",
        # æ“ä½œçŠ¶æ€
        "ä½¿ç”¨": "Use", "å¼€é€š": "Activate", "å…³é—­": "Close",
        "å‡çº§": "Upgrade", "é™çº§": "Downgrade", "ç»­è´¹": "Renew",
        "ç¼´è´¹": "Payment", "æ¬ è´¹åœæœº": "Suspended",
        "åŸå› ": "Reason", "ç»“æœ": "Result", "å»ºè®®": "Suggest",
        "æ»¡æ„": "Satisfied", "ä¸æ»¡": "Unsatisfied", "é‡å¤": "Repeat",
        # æ–°å¢ï¼šæ ‡ç­¾å±‚çº§å¸¸ç”¨è¯
        "ä¸€çº§": "Level1", "äºŒçº§": "Level2", "ä¸‰çº§": "Level3", "å››çº§": "Level4",
        "ç±»åˆ«": "Category", "åˆ†ç±»": "Class", "ç±»å‹": "Type",
        "ä¸»é¢˜": "Topic", "åŸå› åˆ†æ": "RootCause", "å¤„ç†ç»“æœ": "Resolution",
        "ç´§æ€¥": "Urgent", "ä¸€èˆ¬": "Normal", "é‡è¦": "Important",
        # æ–°å¢ï¼šæŠ•è¯‰æ–‡æœ¬å¸¸ç”¨è¯
        "ä¸å¥½": "Bad", "å¤ªæ…¢": "TooSlow", "ä¸è¡Œ": "NotWork",
        "æ‰“ä¸é€š": "CannotCall", "ä¸Šä¸äº†": "CannotAccess", "æ”¶ä¸åˆ°": "CannotReceive",
        "å‘ä¸å‡º": "CannotSend", "çœ‹ä¸äº†": "CannotView", "ç”¨ä¸äº†": "CannotUse",
        "ç»å¸¸": "Often", "ä¸€ç›´": "Always", "æœ‰æ—¶": "Sometimes",
        "çªç„¶": "Suddenly", "æ€»æ˜¯": "Always", "ä»ä¸": "Never",
        "å·²ç»": "Already", "è¿˜æ˜¯": "Still", "ä»ç„¶": "Still",
        "å¸Œæœ›": "Hope", "éº»çƒ¦": "Trouble",
        "æ„Ÿè°¢": "Thanks", "è°¢è°¢": "Thanks", "æŠ±æ­‰": "Sorry",
        # æ–°å¢ï¼šæ•°å­—å’Œå•ä½
        "å¤©": "Days", "å°æ—¶": "Hours", "åˆ†é’Ÿ": "Minutes",
        "å…ƒ": "Yuan", "å—": "Yuan", "é’±": "Money",
        "æ¬¡": "Times", "ä¸ª": "Count", "æ¡": "Items",
        # æ–°å¢ï¼šæ›´å¤šä¸šåŠ¡æœ¯è¯­
        "æœˆç§Ÿ": "MonthlyFee", "åŒ…æœˆ": "Monthly", "æ—¥ç§Ÿ": "DailyFee",
        "æœ¬åœ°": "Local", "é•¿é€”": "LongDistance", "æ¼«æ¸¸è´¹": "RoamingFee",
        "å›½å†…": "Domestic", "å›½é™…": "International", "æ¸¯æ¾³å°": "HMT",
        "è¯­éŸ³": "Voice", "è§†é¢‘": "Video",
        "å½©ä¿¡": "MMS", "æ¥ç”µæ˜¾ç¤º": "CallerID", "å‘¼å«è½¬ç§»": "CallForward",
        "ç•™è¨€": "Voicemail", "é»‘åå•": "Blacklist", "ç™½åå•": "Whitelist",
    }

    def __init__(self):
        self.translator_model = None
        self.translator_tokenizer = None
        self.auto_translate_cache = {}
        self.model_loaded = False
        self._try_init_translator()

    def _try_init_translator(self):
        try:
            from transformers import MarianMTModel, MarianTokenizer
            model_name = 'Helsinki-NLP/opus-mt-zh-en'
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½è‡ªåŠ¨ç¿»è¯‘æ¨¡å‹: {model_name}")
            self.translator_tokenizer = MarianTokenizer.from_pretrained(model_name)
            self.translator_model = MarianMTModel.from_pretrained(model_name)
            self.model_loaded = True
            print("âœ… è‡ªåŠ¨ç¿»è¯‘æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"âœ… å†…ç½®å­—å…¸è¯æ±‡æ•°: {len(self.BUILTIN_DICT)}")
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨ç¿»è¯‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"âœ… å°†ä»…ä½¿ç”¨å†…ç½®å­—å…¸ç¿»è¯‘ (è¯æ±‡æ•°: {len(self.BUILTIN_DICT)})")
            self.model_loaded = False

    def _auto_translate(self, text: str) -> str:
        if not self.model_loaded:
            return text
        if text in self.auto_translate_cache:
            return self.auto_translate_cache[text]
        try:
            inputs = self.translator_tokenizer(text, return_tensors="pt", padding=True)
            translated = self.translator_model.generate(**inputs)
            result = self.translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()
            if ' ' in result and len(text) <= 4:
                result = result.split()[0]
            self.auto_translate_cache[text] = result
            return result
        except:
            return text

    def translate(self, text: str) -> str:
        if not text or not text.strip():
            return text
        text = text.strip().replace("##", "")
        if text in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']:
            return ""
        if text.replace(' ', '').isascii():
            return text
        if text in self.BUILTIN_DICT:
            return self.BUILTIN_DICT[text]
        return self._auto_translate(text)

    def translate_batch(self, texts: List[str]) -> List[str]:
        results = []
        to_auto_translate = []
        to_auto_translate_indices = []
        for i, text in enumerate(texts):
            text = text.strip().replace("##", "") if text else ""
            if not text or text in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']:
                results.append("")
            elif text.replace(' ', '').isascii():
                results.append(text)
            elif text in self.BUILTIN_DICT:
                results.append(self.BUILTIN_DICT[text])
            elif text in self.auto_translate_cache:
                results.append(self.auto_translate_cache[text])
            else:
                results.append(None)
                to_auto_translate.append(text)
                to_auto_translate_indices.append(i)
        if to_auto_translate and self.model_loaded:
            try:
                inputs = self.translator_tokenizer(to_auto_translate, return_tensors="pt", padding=True)
                translated = self.translator_model.generate(**inputs)
                for j, idx in enumerate(to_auto_translate_indices):
                    result = self.translator_tokenizer.decode(translated[j], skip_special_tokens=True).strip()
                    original = to_auto_translate[j]
                    if ' ' in result and len(original) <= 4:
                        result = result.split()[0]
                    self.auto_translate_cache[original] = result
                    results[idx] = result
            except:
                for j, idx in enumerate(to_auto_translate_indices):
                    results[idx] = to_auto_translate[j]
        else:
            for j, idx in enumerate(to_auto_translate_indices):
                results[idx] = to_auto_translate[j]
        return results


_global_translator = None


def get_translator():
    global _global_translator
    if _global_translator is None:
        _global_translator = SmartTranslator()
    return _global_translator


def translate_label_path(label_path: str) -> list:
    if 'â†’' in label_path:
        parts = label_path.split('â†’')
    elif '->' in label_path:
        parts = label_path.split('->')
    else:
        parts = [label_path]
    translator = get_translator()
    clean_parts = [p.strip() for p in parts if p.strip()]
    return translator.translate_batch(clean_parts)


# ============================================================
# Jiebaåˆ†è¯æ”¯æŒ - ç”¨äºè¯çº§åˆ«attentionèšåˆ
# ============================================================

_jieba_initialized = False


def init_jieba_with_user_dict(user_dict_file='new_user_dict.txt'):
    """åˆå§‹åŒ–jiebaå¹¶åŠ è½½ç”¨æˆ·è¯å…¸ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼‰"""
    global _jieba_initialized
    if not _jieba_initialized:
        if os.path.exists(user_dict_file):
            jieba.load_userdict(user_dict_file)
            print(f"âœ… Visualization: å·²åŠ è½½ç”¨æˆ·è¯å…¸ {user_dict_file}")
        else:
            print(f"âš ï¸ Visualization: ç”¨æˆ·è¯å…¸ä¸å­˜åœ¨ {user_dict_file}")
        _jieba_initialized = True


def aggregate_tokens_to_words(tokens, attention_scores, text, user_dict_file='new_user_dict.txt'):
    """
    å°†BERTå­—çº§åˆ«çš„tokenså’Œattentionèšåˆä¸ºjiebaè¯çº§åˆ«

    Args:
        tokens: BERTåˆ†è¯åçš„tokenåˆ—è¡¨ ['[CLS]', 'ç½‘', 'ç»œ', 'ä¿¡', 'å·', ...]
        attention_scores: æ¯ä¸ªtokençš„attentionåˆ†æ•° (1D array)
        text: åŸå§‹ä¸­æ–‡æ–‡æœ¬
        user_dict_file: ç”¨æˆ·è¯å…¸æ–‡ä»¶è·¯å¾„

    Returns:
        word_list: jiebaåˆ†è¯åçš„è¯è¯­åˆ—è¡¨
        word_attention: æ¯ä¸ªè¯è¯­çš„èšåˆattentionåˆ†æ•°
        word_token_indices: æ¯ä¸ªè¯è¯­å¯¹åº”çš„åŸå§‹tokenç´¢å¼•èŒƒå›´ [(start, end), ...]
    """
    # åˆå§‹åŒ–jieba
    init_jieba_with_user_dict(user_dict_file)

    # ç”¨jiebaå¯¹åŸæ–‡åˆ†è¯
    jieba_words = list(jieba.cut(text))

    # è¿‡æ»¤æ‰ç©ºç™½è¯
    jieba_words = [w for w in jieba_words if w.strip()]

    # æ¸…ç†tokensï¼šå»æ‰[CLS]ã€[SEP]ç­‰ç‰¹æ®Štokenï¼Œå»ºç«‹ä½ç½®æ˜ å°„
    # token_map: [(åŸå§‹ç´¢å¼•, æ¸…ç†åçš„å­—ç¬¦), ...]
    special_tokens = {'[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]', ''}
    token_map = []
    for i, token in enumerate(tokens):
        clean_token = token.replace('##', '')
        if token not in special_tokens and clean_token.strip():
            token_map.append((i, clean_token))

    # å°†jiebaè¯è¯­ä¸BERT tokenså¯¹é½
    word_list = []
    word_attention = []
    word_token_indices = []

    token_ptr = 0  # å½“å‰åœ¨token_mapä¸­çš„ä½ç½®

    for word in jieba_words:
        word_chars = list(word)
        word_len = len(word_chars)

        # æŸ¥æ‰¾è¿™ä¸ªè¯åœ¨token_mapä¸­çš„èµ·å§‹ä½ç½®
        start_ptr = token_ptr
        matched_indices = []
        matched_attentions = []

        # å°è¯•åŒ¹é…è¿™ä¸ªè¯çš„æ‰€æœ‰å­—ç¬¦
        temp_ptr = token_ptr
        match_success = True

        for char in word_chars:
            if temp_ptr >= len(token_map):
                match_success = False
                break

            orig_idx, token_char = token_map[temp_ptr]

            # æ£€æŸ¥å­—ç¬¦æ˜¯å¦åŒ¹é…ï¼ˆå¤„ç†å¯èƒ½çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
            if token_char == char or char in token_char:
                matched_indices.append(orig_idx)
                matched_attentions.append(attention_scores[orig_idx] if orig_idx < len(attention_scores) else 0)
                temp_ptr += 1
            else:
                # å­—ç¬¦ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šå­—ç¬¦ï¼Œè·³è¿‡
                match_success = False
                break

        if match_success and matched_indices:
            # æˆåŠŸåŒ¹é…ï¼Œèšåˆattentionï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼‰
            word_list.append(word)
            word_attention.append(np.mean(matched_attentions))
            word_token_indices.append((matched_indices[0], matched_indices[-1]))
            token_ptr = temp_ptr
        else:
            # åŒ¹é…å¤±è´¥ï¼Œå°è¯•è·³è¿‡å½“å‰tokenç»§ç»­
            if token_ptr < len(token_map):
                token_ptr += 1

    return word_list, np.array(word_attention), word_token_indices

def select_top_attention_tokens(attention_matrix, tokens, top_k=8, text=None, user_dict_file='new_user_dict.txt'):
    """
    é€‰æ‹©æ³¨æ„åŠ›æƒé‡æœ€é«˜çš„tokensï¼ˆæ”¯æŒè¯çº§åˆ«èšåˆï¼‰

    æ”¹è¿›ï¼šå½“æä¾›textå‚æ•°æ—¶ï¼Œä½¿ç”¨jiebaåˆ†è¯è¿›è¡Œè¯çº§åˆ«èšåˆ
         å¦åˆ™å›é€€åˆ°åŸæ¥çš„å­—çº§åˆ«é€‰æ‹©

    Args:
        attention_matrix: attentionæƒé‡çŸ©é˜µ [seq_len, key_len] æˆ– [seq_len]
        tokens: BERTåˆ†è¯åçš„tokenåˆ—è¡¨
        top_k: é€‰æ‹©top-kä¸ªè¯/å­—
        text: åŸå§‹æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œæä¾›æ—¶å¯ç”¨è¯çº§åˆ«èšåˆï¼‰
        user_dict_file: ç”¨æˆ·è¯å…¸æ–‡ä»¶è·¯å¾„

    Returns:
        final_indices: é€‰ä¸­çš„tokenç´¢å¼•åˆ—è¡¨
        final_orig: åŸå§‹è¯è¯­/å­—ç¬¦åˆ—è¡¨
        final_trans: ç¿»è¯‘åçš„è¯è¯­åˆ—è¡¨
    """
    # ç¡®ä¿attention_matrixæ˜¯2Dçš„
    if attention_matrix.ndim == 1:
        attention_matrix = attention_matrix.reshape(-1, 1)

    # è®¡ç®—æ¯ä¸ªtokençš„importanceï¼ˆå¯¹æ‰€æœ‰keyä½ç½®æ±‚å¹³å‡ï¼‰
    importance = attention_matrix.mean(axis=1)

    # éœ€è¦è¿‡æ»¤çš„ç‰¹æ®Šç¬¦å·å’Œæ— æ„ä¹‰å­—ç¬¦
    special_chars = {'#', '##', '@', '$', '%', '^', '&', '*', '(', ')',
                     '-', '_', '+', '=', '[', ']', '{', '}', '|', '\\',
                     '/', '<', '>', ',', '.', '?', '!', '~', '`', '"', "'",
                     'ï¼š', 'ï¼›', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', '"', '"', ''', ''',
                     'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹', 'â€¦', 'â€”', 'Â·'}

    # ========== æ–°å¢ï¼šè¯çº§åˆ«èšåˆæ¨¡å¼ ==========
    if text is not None and len(text.strip()) > 0:
        try:
            # ä½¿ç”¨jiebaåˆ†è¯è¿›è¡Œè¯çº§åˆ«èšåˆ
            word_list, word_attention, word_token_indices = aggregate_tokens_to_words(
                tokens, importance, text, user_dict_file
            )

            if len(word_list) > 0:
                # è¿‡æ»¤æ— æ„ä¹‰çš„è¯
                valid_words = []
                for i, (word, attn, indices) in enumerate(zip(word_list, word_attention, word_token_indices)):
                    # è¿‡æ»¤å•å­—ç¬¦ä¸”ä¸ºç‰¹æ®Šç¬¦å·çš„
                    if len(word) == 1 and word in special_chars:
                        continue
                    # è¿‡æ»¤çº¯ç‰¹æ®Šç¬¦å·ç»„æˆçš„è¯
                    if all(c in special_chars or c.isspace() for c in word):
                        continue
                    # è¿‡æ»¤attentionè¿‡ä½çš„
                    if attn <= 0.001:
                        continue
                    valid_words.append((i, word, attn, indices))

                # æŒ‰attentionæ’åºï¼Œé€‰æ‹©top-k
                valid_words.sort(key=lambda x: x[2], reverse=True)
                selected = valid_words[:top_k]

                # æŒ‰åŸå§‹é¡ºåºæ’åº
                selected.sort(key=lambda x: x[3][0])

                # ç¿»è¯‘
                translator = get_translator()
                final_indices = [s[3][0] for s in selected]  # ä½¿ç”¨è¯çš„èµ·å§‹tokenç´¢å¼•
                final_orig = [s[1] for s in selected]
                final_trans = translator.translate_batch(final_orig)

                # è¿‡æ»¤ç¿»è¯‘åä¸ºç©ºçš„ç»“æœ
                result_indices = []
                result_orig = []
                result_trans = []
                for idx, orig, trans in zip(final_indices, final_orig, final_trans):
                    if trans and trans.strip() and trans not in special_chars:
                        result_indices.append(idx)
                        result_orig.append(orig)
                        result_trans.append(trans)

                if result_trans:
                    print(f"   [è¯çº§åˆ«èšåˆ] æå–åˆ° {len(result_trans)} ä¸ªå…³é”®è¯")
                    return result_indices, result_orig, result_trans

        except Exception as e:
            print(f"   âš ï¸ è¯çº§åˆ«èšåˆå¤±è´¥ï¼Œå›é€€åˆ°å­—çº§åˆ«: {e}")

    # ========== åŸæœ‰é€»è¾‘ï¼šå­—çº§åˆ«é€‰æ‹©ï¼ˆä½œä¸ºfallbackï¼‰ ==========
    valid = []

    for i, (token, score) in enumerate(zip(tokens, importance)):
        # è¿‡æ»¤BERTç‰¹æ®Štoken
        if token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '', '[MASK]']:
            continue
        # è¿‡æ»¤ä½æƒé‡token
        if score <= 0.001:
            continue
        # å»æ‰##å‰ç¼€
        clean_token = token.replace('##', '')
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not clean_token or clean_token.strip() == '':
            continue
        # è¿‡æ»¤çº¯ç‰¹æ®Šç¬¦å·
        if clean_token in special_chars:
            continue
        # è¿‡æ»¤åªåŒ…å«ç‰¹æ®Šç¬¦å·çš„token
        if all(c in special_chars or c.isspace() for c in clean_token):
            continue
        valid.append((i, token, score))

    valid.sort(key=lambda x: x[2], reverse=True)
    selected = valid[:top_k]
    selected.sort(key=lambda x: x[0])

    translator = get_translator()
    indices = [s[0] for s in selected]
    orig_tokens = [s[1].replace('##', '') for s in selected]
    trans_tokens = translator.translate_batch(orig_tokens)

    # è¿‡æ»¤ç¿»è¯‘åä¸ºç©ºæˆ–ä»æ˜¯ç‰¹æ®Šç¬¦å·çš„ç»“æœ
    final_indices = []
    final_orig = []
    final_trans = []
    for idx, orig, trans in zip(indices, orig_tokens, trans_tokens):
        # è·³è¿‡ç¿»è¯‘åä¸ºç©ºçš„
        if not trans or trans.strip() == '':
            continue
        # è·³è¿‡ç¿»è¯‘åä»æ˜¯ç‰¹æ®Šç¬¦å·çš„
        if trans in special_chars or all(c in special_chars or c.isspace() for c in trans):
            continue
        final_indices.append(idx)
        final_orig.append(orig)
        final_trans.append(trans)

    return final_indices, final_orig, final_trans


warnings.filterwarnings('ignore')

# ============================================================
# Matplotlib ä¸­æ–‡å­—ä½“é…ç½® - SimHei
# ============================================================

# æ–¹æ³•1: å°è¯•ç›´æ¥ä½¿ç”¨å­—ä½“åç§°
plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.unicode_minus'] = False

# æ–¹æ³•2: å¦‚æœæ–¹æ³•1ä¸è¡Œï¼Œç›´æ¥æŒ‡å®šå­—ä½“æ–‡ä»¶è·¯å¾„
import os
simhei_paths = [
    '/usr/share/fonts/chinese/simhei.ttf',
    '/usr/share/fonts/truetype/simhei.ttf',
    '/usr/local/share/fonts/simhei.ttf',
    os.path.expanduser('~/.fonts/simhei.ttf'),
]

simhei_path = None
for path in simhei_paths:
    if os.path.exists(path):
        simhei_path = path
        break

if simhei_path:
    # æ³¨å†Œå­—ä½“
    fm.fontManager.addfont(simhei_path)
    prop = fm.FontProperties(fname=simhei_path)
    plt.rcParams['font.sans-serif'] = [prop.get_name()] + plt.rcParams['font.sans-serif']
    print(f"âœ… SimHei font loaded from: {simhei_path}")
else:
    print("âš ï¸ SimHei font not found, using fallback fonts")

# å…¶ä»–é…ç½®
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10
plt.rcParams['figure.titlesize'] = 16

sns.set_style("whitegrid")
sns.set_context("paper", font_scale=1.2)

# éªŒè¯å­—ä½“æ˜¯å¦å¯ç”¨
print("=" * 50)
print("Font Configuration Check:")
available_fonts = set([f.name for f in fm.fontManager.ttflist])
chinese_fonts = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC']
for font in chinese_fonts:
    if font in available_fonts:
        print(f"  âœ… {font} - Available")
    else:
        print(f"  âŒ {font} - Not Found")
print("=" * 50)


class AttentionVisualizer:
    """Attention Weight Visualizer"""

    def __init__(self, figsize: Tuple[int, int] = (12, 8)):
        self.figsize = figsize

    def visualize_cross_modal_attention(self, attention_weights: Dict[str, torch.Tensor],
                                      sample_text: str = None, save_path: str = None):
        """Visualize cross-modal attention weights"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()

        attention_types = [
            ('text_to_label', 'Text â†’ Label'),
            ('text_to_struct', 'Text â†’ Struct'),
            ('label_to_text', 'Label â†’ Text'),
            ('label_to_struct', 'Label â†’ Struct'),
            ('struct_to_text', 'Struct â†’ Text'),
            ('struct_to_label', 'Struct â†’ Label')
        ]

        for idx, (attn_key, title) in enumerate(attention_types):
            if attn_key in attention_weights:
                attn = attention_weights[attn_key][0, 0].cpu().numpy()
                ax = axes[idx]
                sns.heatmap(attn, cmap='Blues', ax=ax, cbar=True,
                           xticklabels=False, yticklabels=False)
                ax.set_title(title, fontsize=14)

                if idx == 0 and sample_text:
                    ax.text(0.5, -0.15, f"Text: {sample_text[:30]}...",
                           transform=ax.transAxes, ha='center', fontsize=10)

        plt.suptitle('Cross-Modal Attention Weights Visualization', fontsize=16)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def visualize_token_attention(self, text: str, attention_scores: np.ndarray,
                                save_path: str = None):
        """Visualize token-level attention scores"""
        tokens = list(text)[:50]
        fig, ax = plt.subplots(figsize=(15, 3))

        if len(attention_scores) > len(tokens):
            attention_scores = attention_scores[:len(tokens)]

        norm = plt.Normalize(vmin=attention_scores.min(), vmax=attention_scores.max())
        colors = plt.cm.Blues(norm(attention_scores))

        for i, (token, color) in enumerate(zip(tokens, colors)):
            ax.text(i, 0, token, ha='center', va='center',
                   color='black', backgroundcolor=color, fontsize=12)

        ax.set_xlim(-1, len(tokens))
        ax.set_ylim(-0.5, 0.5)
        ax.axis('off')
        ax.set_title('Token-Level Attention Visualization', fontsize=14)

        sm = plt.cm.ScalarMappable(cmap='Blues', norm=norm)
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', pad=0.1)
        cbar.set_label('Attention Intensity')

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


class FeatureImportanceVisualizer:
    """Feature Importance Visualizer"""

    def __init__(self):
        pass

    def visualize_structured_features(self, feature_names: List[str],
                                    feature_importance: np.ndarray,
                                    save_path: str = None):
        """Visualize structured feature importance"""
        indices = np.argsort(feature_importance)[::-1]
        sorted_features = [feature_names[i] for i in indices]
        sorted_importance = feature_importance[indices]

        plt.figure(figsize=(10, 6))
        bars = plt.barh(sorted_features, sorted_importance)

        colors = plt.cm.RdYlBu_r(sorted_importance / sorted_importance.max())
        for bar, color in zip(bars, colors):
            bar.set_color(color)

        plt.xlabel('Feature Importance')
        plt.title('Structured Feature Importance Ranking')
        plt.grid(axis='x', alpha=0.3)

        for i, v in enumerate(sorted_importance):
            plt.text(v + 0.01, i, f'{v:.3f}', va='center')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


class LabelTreeVisualizer:
    """Label Tree Visualizer"""

    def __init__(self):
        pass

    def visualize_label_tree(self, label_tree, save_path: str = None):
        """Visualize complaint label hierarchy"""
        G = nx.DiGraph()

        for node_id, label in label_tree.id_to_label.items():
            G.add_node(node_id, label=label)

        for parent, child in label_tree.edges:
            G.add_edge(parent, child)

        pos = nx.spring_layout(G, k=2, iterations=50)

        plt.figure(figsize=(15, 10))

        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True,
                              arrowsize=20, alpha=0.6)

        node_colors = []
        for node in G.nodes():
            level = len(nx.ancestors(G, node))
            node_colors.append(level)

        nx.draw_networkx_nodes(G, pos, node_color=node_colors,
                              cmap='viridis', node_size=1000, alpha=0.8)

        labels = nx.get_node_attributes(G, 'label')
        short_labels = {k: v[:10] + '...' if len(v) > 10 else v
                       for k, v in labels.items()}
        nx.draw_networkx_labels(G, pos, short_labels, font_size=8)

        plt.title('Complaint Label Hierarchy Structure', fontsize=16)
        plt.axis('off')

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


class PredictionAnalyzer:
    """Prediction Result Analyzer"""

    def __init__(self):
        pass

    def visualize_prediction_distribution(self, predictions: pd.DataFrame,
                                        save_path: str = None):
        """Visualize prediction result distribution"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. Prediction class distribution
        ax = axes[0, 0]
        predictions['prediction_text'].value_counts().plot(kind='bar', ax=ax)
        ax.set_title('Prediction Class Distribution')
        ax.set_xlabel('Prediction Class')
        ax.set_ylabel('Count')

        # 2. Confidence distribution
        ax = axes[0, 1]
        ax.hist(predictions['confidence'], bins=20, alpha=0.7, color='blue', edgecolor='black')
        ax.set_title('Prediction Confidence Distribution')
        ax.set_xlabel('Confidence')
        ax.set_ylabel('Frequency')
        ax.axvline(0.8, color='red', linestyle='--', label='High Confidence Threshold')
        ax.legend()

        # 3. Repeat complaint probability distribution
        ax = axes[1, 0]
        ax.hist(predictions['repeat_probability'], bins=20, alpha=0.7,
               color='red', edgecolor='black')
        ax.set_title('Repeat Complaint Probability Distribution')
        ax.set_xlabel('Probability')
        ax.set_ylabel('Frequency')
        ax.axvline(0.5, color='black', linestyle='--', label='Decision Threshold')
        ax.legend()

        # 4. Empty subplot for future use
        axes[1, 1].axis('off')

        plt.suptitle('Prediction Results Analysis', fontsize=16)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


class TextAnalyzer:
    """Text Analysis and Visualization"""

    def __init__(self):
        pass

    def visualize_wordcloud(self, texts_by_category: Dict[str, List[str]],
                           save_path: str = None):
        """Visualize word clouds by category"""
        categories = list(texts_by_category.keys())
        n_categories = len(categories)

        fig, axes = plt.subplots(1, n_categories, figsize=(6*n_categories, 5))
        if n_categories == 1:
            axes = [axes]

        for idx, (category, texts) in enumerate(texts_by_category.items()):
            if texts:
                combined_text = ' '.join(texts)

                wordcloud = WordCloud(
                    width=800, height=400,
                    background_color='white',
                    max_words=100,
                    relative_scaling=0.5,
                    min_font_size=10
                ).generate(combined_text)

                ax = axes[idx]
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.set_title(f'{category} Word Cloud', fontsize=16)
                ax.axis('off')
            else:
                axes[idx].text(0.5, 0.5, f'No {category} Data',
                             ha='center', va='center', fontsize=14)
                axes[idx].axis('off')

        plt.suptitle('Complaint Text Word Cloud Analysis', fontsize=18)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


def visualize_training_curves(history: Dict[str, List[float]], save_path: str = None):
    """Visualize training curves"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    metrics = [
        ('loss', 'Loss'),
        ('accuracy', 'Accuracy'),
        ('f1', 'F1 Score'),
        ('precision', 'Precision'),
        ('recall', 'Recall'),
        ('auc', 'AUC')
    ]

    for idx, (metric, title) in enumerate(metrics):
        row = idx // 3
        col = idx % 3
        ax = axes[row, col]

        if f'train_{metric}' in history:
            ax.plot(history[f'train_{metric}'], label='Train', marker='o')
        if f'val_{metric}' in history:
            ax.plot(history[f'val_{metric}'], label='Validation', marker='s')

        ax.set_title(title)
        ax.set_xlabel('Epoch')
        ax.set_ylabel(title)
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.suptitle('Training Progress Metrics', fontsize=16)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


# ============================================================
# Enhanced Visualization Classes for Paper-Quality Figures
# ============================================================

class EnhancedAttentionVisualizer:
    """
    Enhanced Attention Visualizer - For paper-quality figure generation
    Supports cross-modal attention heatmaps, case decision tracing, etc.
    """

    def __init__(self, tokenizer=None, save_dir: str = './outputs/figures'):
        self.tokenizer = tokenizer
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def plot_cross_modal_attention_heatmap(self,
                                           attention_weights: Dict[str, torch.Tensor],
                                           struct_features: List[str] = None,
                                           struct_feature_names: List[str] = None,
                                           sample_id: str = "sample",
                                           label_path: str = None,
                                           tokenizer=None,
                                           text: str = None,
                                           new_code: str = None,
                                           save_path: str = None) -> plt.Figure:
        """
        Plot cross-modal attention heatmap
        ä¿®å¤: æ·»åŠ Yè½´æ–‡æœ¬å…³é”®è¯æ˜¾ç¤º
        """
        if new_code:
            print(f"ğŸ“Œ Processing sample new_code: {new_code}")

        attn_types = []
        titles = []
        if 'text_to_label' in attention_weights and attention_weights['text_to_label'] is not None:
            attn_types.append(('text_to_label', attention_weights['text_to_label']))
            titles.append('Text -> Label')
        if 'label_to_text' in attention_weights and attention_weights['label_to_text'] is not None:
            attn_types.append(('label_to_text', attention_weights['label_to_text']))
            titles.append('Label -> Text')
        if 'text_to_struct' in attention_weights and attention_weights['text_to_struct'] is not None:
            attn_types.append(('text_to_struct', attention_weights['text_to_struct']))
            titles.append('Text -> Structured')

        if len(attn_types) == 0:
            print("Warning: No available attention weights")
            return None

        # ç¿»è¯‘æ ‡ç­¾è·¯å¾„
        translated_labels = None
        if label_path:
            translated_labels = translate_label_path(label_path)
            print(f"   Label: {' -> '.join(translated_labels)}")

        # ã€ä¿®å¤é—®é¢˜2ã€‘æå–æ–‡æœ¬å…³é”®è¯ç”¨äºYè½´æ˜¾ç¤º
        text_keywords = None
        text_keyword_indices = None
        if tokenizer is not None and text is not None:
            try:
                for attn_name, attn_tensor in attn_types:
                    if 'text_to' in attn_name:
                        if isinstance(attn_tensor, torch.Tensor):
                            attn_for_keywords = attn_tensor.detach().cpu()
                            if attn_for_keywords.dim() == 4:
                                attn_for_keywords = attn_for_keywords[0].mean(dim=0).numpy()
                            elif attn_for_keywords.dim() == 3:
                                attn_for_keywords = attn_for_keywords[0].numpy()
                            else:
                                attn_for_keywords = attn_for_keywords.numpy()
                        else:
                            attn_for_keywords = np.array(attn_tensor)

                        if attn_for_keywords.ndim == 1:
                            attn_for_keywords = attn_for_keywords.reshape(1, -1)

                        encoding = tokenizer(text, max_length=256, truncation=True, return_tensors='pt')
                        tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])

                        text_keyword_indices, orig_tokens, text_keywords = select_top_attention_tokens(
                            attn_for_keywords, tokens, top_k=min(8, attn_for_keywords.shape[0]), text=text
                        )

                        if text_keywords:
                            print(f"   Text Keywords (Y-axis): {', '.join(text_keywords)}")
                        break
            except Exception as e:
                print(f"   Warning: Failed to extract text keywords: {e}")
                text_keywords = None

        n_plots = len(attn_types)
        n_cols = min(2, n_plots)
        n_rows = (n_plots + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(7 * n_cols, 6 * n_rows))
        if n_plots == 1:
            axes = [axes]
        else:
            axes = axes.flatten()

        for idx, ((attn_name, attn_tensor), title) in enumerate(zip(attn_types, titles)):
            ax = axes[idx]
            if isinstance(attn_tensor, torch.Tensor):
                attn = attn_tensor.detach().cpu()
                if attn.dim() == 4:
                    attn = attn[0].mean(dim=0).numpy()
                elif attn.dim() == 3:
                    attn = attn[0].numpy()
                elif attn.dim() == 2:
                    attn = attn.numpy()
                else:
                    attn = attn.numpy()
            else:
                attn = np.array(attn_tensor)

            if attn.ndim == 1:
                attn = attn.reshape(1, -1)

            # ã€ä¿®å¤é—®é¢˜2ã€‘å¦‚æœæœ‰æ–‡æœ¬å…³é”®è¯ï¼Œå¯¹attentionçŸ©é˜µè¿›è¡Œè¡Œé€‰æ‹©
            y_labels = None
            if 'text_to' in attn_name and text_keywords and text_keyword_indices:
                valid_indices = [i for i in text_keyword_indices if i < attn.shape[0]]
                if valid_indices:
                    attn = attn[valid_indices, :]
                    y_labels = text_keywords[:len(valid_indices)]

            im = ax.imshow(attn, cmap='YlOrRd', aspect='auto')
            ax.set_title(title, fontsize=14, fontweight='bold')
            cbar = plt.colorbar(im, ax=ax, shrink=0.8)
            cbar.set_label('Attention Weight', fontsize=10)

            # è®¾ç½®Xè½´æ ‡ç­¾
            n_cols_attn = attn.shape[1]
            if 'label' in attn_name and translated_labels:
                if len(translated_labels) >= n_cols_attn:
                    x_labels = translated_labels[:n_cols_attn]
                else:
                    x_labels = translated_labels + [f"L{i + 1}" for i in range(len(translated_labels), n_cols_attn)]
                ax.set_xticks(range(n_cols_attn))
                ax.set_xticklabels(x_labels, rotation=30, ha='right', fontsize=9)
                ax.set_xlabel('Label Hierarchy', fontsize=11)
            elif 'struct' in attn_name and struct_feature_names:
                if len(struct_feature_names) >= n_cols_attn:
                    x_labels = struct_feature_names[:n_cols_attn]
                else:
                    x_labels = struct_feature_names + [f"F{i + 1}" for i in
                                                       range(len(struct_feature_names), n_cols_attn)]
                ax.set_xticks(range(n_cols_attn))
                ax.set_xticklabels(x_labels, rotation=45, ha='right', fontsize=8)
                ax.set_xlabel('Structured Features', fontsize=11)
            else:
                ax.set_xlabel('Key Position', fontsize=11)

            # ã€ä¿®å¤é—®é¢˜2ã€‘è®¾ç½®Yè½´æ ‡ç­¾ - æ˜¾ç¤ºæ–‡æœ¬å…³é”®è¯
            if y_labels and 'text_to' in attn_name:
                ax.set_yticks(range(len(y_labels)))
                ax.set_yticklabels(y_labels, fontsize=9)
                ax.set_ylabel('Text Keywords', fontsize=11)
            else:
                ax.set_ylabel('Query Position', fontsize=11)

        for idx in range(len(attn_types), len(axes)):
            axes[idx].axis('off')

        plt.suptitle(f'Cross-Modal Attention (Sample: {sample_id})', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()

        if save_path is None:
            save_path = os.path.join(self.save_dir, f'attention_heatmap_{sample_id}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Attention heatmap saved: {save_path}")

        return fig

    def plot_attention_with_text(self,
                                 attention_weights: Dict[str, torch.Tensor],
                                 text: str,
                                 label_path: str,
                                 prediction: int,
                                 confidence: float,
                                 true_label: int = None,
                                 sample_id: str = "case",
                                 new_code: str = None,
                                 tokenizer=None,
                                 struct_feature_names: List[str] = None,
                                 save_path: str = None) -> plt.Figure:
        """Plot attention heatmap with text and label - å¸¦ç¿»è¯‘å’Œå…³é”®è¯æŒ‘é€‰"""
        plt.rcParams['font.family'] = 'DejaVu Sans'

        if new_code:
            print(f"ğŸ“Œ Case study sample new_code: {new_code}")

        fig = plt.figure(figsize=(16, 10))
        gs = fig.add_gridspec(3, 2, height_ratios=[0.8, 2.2, 1], hspace=0.3, wspace=0.3)

        ax_text = fig.add_subplot(gs[0, :])
        ax_text.axis('off')

        # ã€ä¿®å¤é—®é¢˜3ã€‘æ˜¾ç¤ºæ–‡æœ¬æ‘˜è¦ï¼ˆå‰100ä¸ªå­—ç¬¦ï¼‰
        display_id = new_code if new_code else sample_id
        text_preview = text[:100] + "..." if len(text) > 100 else text
        translator = get_translator()
        translated_preview = translator.translate(text_preview)
        info_text = f"Sample: {display_id}\nText Preview: {translated_preview}"
        ax_text.text(0.5, 0.5, info_text, ha='center', va='center',
                     fontsize=10, wrap=True,
                     bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))

        translated_labels = translate_label_path(label_path)
        n_labels = len(translated_labels)
        print(f"   Label Path: {' â†’ '.join(translated_labels)}")

        ax_label = fig.add_subplot(gs[2, 0])
        ax_label.axis('off')
        label_display = " â†’ ".join(translated_labels)
        ax_label.text(0.5, 0.5, f"Label Path:\n{label_display}", ha='center', va='center',
                      fontsize=10, wrap=True,
                      bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.3))

        ax_pred = fig.add_subplot(gs[2, 1])
        ax_pred.axis('off')
        pred_text = "Repeat Complaint" if prediction == 1 else "Non-Repeat"
        true_text = ""
        if true_label is not None:
            true_text = f"\nTrue: {'Repeat' if true_label == 1 else 'Non-Repeat'}"
            is_correct = prediction == true_label
            status = " (Correct)" if is_correct else " (Wrong)"
            true_text += status

        pred_content = f"Prediction: {pred_text}\nConfidence: {confidence:.2%}{true_text}"
        color = 'lightcoral' if prediction == 1 else 'lightgreen'
        ax_pred.text(0.5, 0.5, pred_content, ha='center', va='center',
                     fontsize=12, fontweight='bold',
                     bbox=dict(boxstyle='round,pad=0.5', facecolor=color, alpha=0.5))

        ax_attn1 = fig.add_subplot(gs[1, 0])
        ax_attn2 = fig.add_subplot(gs[1, 1])

        if 'text_to_label' in attention_weights and attention_weights['text_to_label'] is not None:
            attn = attention_weights['text_to_label']
            if isinstance(attn, torch.Tensor):
                attn = attn.detach().cpu()
                if attn.dim() >= 3:
                    attn = attn[0].mean(dim=0).numpy() if attn.dim() == 4 else attn[0].numpy()
                else:
                    attn = attn.numpy()
            if attn.ndim == 1:
                attn = attn.reshape(1, -1)

            y_labels = None
            top_k = 10
            if tokenizer is not None:
                try:
                    encoding = tokenizer(text, max_length=256, truncation=True, return_tensors='pt')
                    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])
                    indices, orig_tokens, trans_tokens = select_top_attention_tokens(attn, tokens, top_k=top_k, text=text)

                    print(f"   Top-{len(indices)} keywords:")
                    for orig, trans in zip(orig_tokens, trans_tokens):
                        print(f"      {orig} â†’ {trans}")

                    attn = attn[indices, :]
                    if attn.shape[1] > n_labels:
                        attn = attn[:, :n_labels]
                    y_labels = trans_tokens
                except Exception as e:
                    print(f"   âš ï¸ Token selection error: {e}")

            im1 = ax_attn1.imshow(attn, cmap='Blues', aspect='auto')
            ax_attn1.set_title('Text â†’ Label Attention', fontsize=12, fontweight='bold')

            n_cols = attn.shape[1]
            if len(translated_labels) >= n_cols:
                x_labels = translated_labels[:n_cols]
            else:
                x_labels = translated_labels + [f"L{i + 1}" for i in range(len(translated_labels), n_cols)]
            ax_attn1.set_xticks(range(n_cols))
            ax_attn1.set_xticklabels(x_labels, rotation=25, ha='right', fontsize=9)
            ax_attn1.set_xlabel('Label Hierarchy', fontsize=10)

            if y_labels:
                ax_attn1.set_yticks(range(len(y_labels)))
                ax_attn1.set_yticklabels(y_labels, fontsize=9)
                ax_attn1.set_ylabel('Text Keywords', fontsize=10)
            else:
                ax_attn1.set_ylabel('Text Position', fontsize=10)

            plt.colorbar(im1, ax=ax_attn1, shrink=0.6)
        else:
            ax_attn1.text(0.5, 0.5, 'No Data Available', ha='center', va='center')
            ax_attn1.set_title('Text â†’ Label Attention', fontsize=12)

        if 'text_to_struct' in attention_weights and attention_weights['text_to_struct'] is not None:
            attn = attention_weights['text_to_struct']
            if isinstance(attn, torch.Tensor):
                attn = attn.detach().cpu()
                if attn.dim() >= 3:
                    attn = attn[0].mean(dim=0).numpy() if attn.dim() == 4 else attn[0].numpy()
                else:
                    attn = attn.numpy()
            if attn.ndim == 1:
                attn = attn.reshape(1, -1)

            # ã€ä¿®å¤é—®é¢˜2ã€‘æ·»åŠ Yè½´æ–‡æœ¬å…³é”®è¯
            y_labels_struct = None
            if tokenizer is not None:
                try:
                    encoding = tokenizer(text, max_length=256, truncation=True, return_tensors='pt')
                    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])
                    indices_struct, orig_tokens_struct, trans_tokens_struct = select_top_attention_tokens(
                        attn, tokens, top_k=min(10, attn.shape[0]), text=text
                    )

                    if indices_struct and trans_tokens_struct:
                        # å¯¹attentionçŸ©é˜µè¿›è¡Œè¡Œé€‰æ‹©
                        valid_indices = [i for i in indices_struct if i < attn.shape[0]]
                        if valid_indices:
                            attn = attn[valid_indices, :]
                            y_labels_struct = trans_tokens_struct[:len(valid_indices)]
                except Exception as e:
                    print(f"   âš ï¸ Textâ†’Struct keyword extraction error: {e}")

            im2 = ax_attn2.imshow(attn, cmap='Oranges', aspect='auto')
            # ã€ä¿®å¤é—®é¢˜2ã€‘æ ‡é¢˜ä»"Semantic"æ”¹ä¸º"Text"
            ax_attn2.set_title('Text â†’ Structured Attention', fontsize=12, fontweight='bold')

            # ã€ä¿®å¤é—®é¢˜2ã€‘Xè½´æ˜¾ç¤ºç»“æ„åŒ–ç‰¹å¾åç§°
            n_struct_cols = attn.shape[1]
            if struct_feature_names and len(struct_feature_names) > 0:
                if len(struct_feature_names) >= n_struct_cols:
                    x_labels_struct = struct_feature_names[:n_struct_cols]
                else:
                    x_labels_struct = struct_feature_names + [f"F{i + 1}" for i in
                                                              range(len(struct_feature_names), n_struct_cols)]
                ax_attn2.set_xticks(range(n_struct_cols))
                ax_attn2.set_xticklabels(x_labels_struct, rotation=45, ha='right', fontsize=8)
                ax_attn2.set_xlabel('Structured Features', fontsize=10)
            else:
                ax_attn2.set_xlabel('Feature Index', fontsize=10)

            # ã€ä¿®å¤é—®é¢˜2ã€‘Yè½´æ˜¾ç¤ºæ–‡æœ¬å…³é”®è¯
            if y_labels_struct:
                ax_attn2.set_yticks(range(len(y_labels_struct)))
                ax_attn2.set_yticklabels(y_labels_struct, fontsize=9)
                ax_attn2.set_ylabel('Text Keywords', fontsize=10)
            else:
                ax_attn2.set_ylabel('Text Position', fontsize=10)

            plt.colorbar(im2, ax=ax_attn2, shrink=0.6)
        else:
            ax_attn2.text(0.5, 0.5, 'No Data Available', ha='center', va='center')
            ax_attn2.set_title('Text â†’ Structured Attention', fontsize=12)

        plt.suptitle(f'Case Study: {sample_id}', fontsize=16, fontweight='bold')

        if save_path is None:
            save_path = os.path.join(self.save_dir, f'case_study_{sample_id}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f"âœ… Case study figure saved: {save_path}")

        return fig


class ModalityContributionAnalyzer:
    """
    Modality Contribution Analyzer - Quantify each modality's contribution
    """

    def __init__(self, save_dir: str = './outputs/figures'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def plot_ablation_comparison(self,
                                  ablation_results: Dict[str, Dict[str, float]],
                                  save_path: str = None) -> plt.Figure:
        """
        Plot ablation study comparison - Modality contribution bar chart
        """
        models = list(ablation_results.keys())
        metrics = ['accuracy', 'f1', 'auc']

        model_names = {
            'full_model': 'Full Model',
            'text_only': 'Text Only',
            'label_only': 'Label Only',
            'struct_only': 'Struct Only',
            'text_label': 'Text+Label',
            'text_struct': 'Text+Struct',
            'label_struct': 'Label+Struct',
            'No_pretrain': 'No_Pretrain'
        }

        fig, ax = plt.subplots(figsize=(14, 7))

        x = np.arange(len(models))
        width = 0.25

        colors = ['#2ecc71', '#3498db', '#e74c3c']

        for i, (metric, color) in enumerate(zip(metrics, colors)):
            values = [ablation_results[m].get(metric, 0) for m in models]
            offset = (i - 1) * width
            bars = ax.bar(x + offset, values, width, label=metric.upper(), color=color, alpha=0.85)

            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.annotate(f'{val:.3f}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom', fontsize=8)

        ax.set_xlabel('Model Configuration', fontsize=12)
        ax.set_ylabel('Performance Score', fontsize=12)
        ax.set_title('Ablation Study Results Comparison', fontsize=14, fontweight='bold')

        ax.set_xticks(x)
        display_names = [model_names.get(m, m) for m in models]
        ax.set_xticklabels(display_names, rotation=30, ha='right', fontsize=10)

        ax.legend(loc='upper right', fontsize=10)
        ax.set_ylim(0, 1.15)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_axisbelow(True)

        plt.tight_layout()

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'ablation_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Ablation comparison figure saved: {save_path}")

        return fig

    def plot_modality_contribution_pie(self,
                                        contributions: Dict[str, float],
                                        save_path: str = None) -> plt.Figure:
        """
        Plot modality contribution pie chart
        """
        fig, ax = plt.subplots(figsize=(8, 8))

        labels = list(contributions.keys())
        sizes = list(contributions.values())
        colors = ['#3498db', '#2ecc71', '#e74c3c']
        explode = (0.05, 0.05, 0.05)

        wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels,
                                           colors=colors, autopct='%1.1f%%',
                                           shadow=True, startangle=90,
                                           textprops={'fontsize': 12})

        for autotext in autotexts:
            autotext.set_fontweight('bold')
            autotext.set_fontsize(14)

        ax.set_title('Modality Contribution to Prediction', fontsize=16, fontweight='bold')

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'modality_contribution_pie.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Modality contribution pie chart saved: {save_path}")

        return fig

    def plot_radar_comparison(self,
                               model_results: Dict[str, Dict[str, float]],
                               save_path: str = None) -> plt.Figure:
        """
        Plot model performance radar chart comparison
        """
        metrics = ['AUC', 'F1', 'Precision', 'Recall', 'Accuracy']
        metric_keys = ['auc', 'f1', 'precision', 'recall', 'accuracy']

        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))

        colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12', '#1abc9c']

        model_display_names = {
            'full_model': 'Full Model',
            'text_only': 'Text Only',
            'label_only': 'Label Only',
            'text_label': 'Text+Label'
        }

        for idx, (model_name, results) in enumerate(model_results.items()):
            values = [results.get(k, 0) for k in metric_keys]
            values += values[:1]

            display_name = model_display_names.get(model_name, model_name)

            ax.plot(angles, values, 'o-', linewidth=2,
                   label=display_name, color=colors[idx % len(colors)])
            ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics, fontsize=12)
        ax.set_ylim(0, 1)
        ax.set_title('Model Performance Radar Chart Comparison', fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1))

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'radar_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Radar chart saved: {save_path}")

        return fig


class TrainingCurveVisualizer:
    """
    Training Curve Visualizer - Show curriculum learning three stages
    """

    def __init__(self, save_dir: str = './outputs/figures'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def plot_curriculum_learning_curves(self,
                                         stage1_history: Dict[str, List[float]],
                                         stage2_history: Dict[str, List[float]],
                                         stage3_history: Dict[str, List[float]],
                                         save_path: str = None) -> plt.Figure:
        """
        Plot curriculum learning three-stage training curves
        """
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        stages = [
            ('Stage 1: Single-Modal Pre-training', stage1_history, '#3498db'),
            ('Stage 2: Dual-Modal Interaction', stage2_history, '#2ecc71'),
            ('Stage 3: Tri-Modal Fusion', stage3_history, '#e74c3c')
        ]

        for ax, (title, history, color) in zip(axes, stages):
            if 'train_loss' in history:
                epochs = range(1, len(history['train_loss']) + 1)
                ax.plot(epochs, history['train_loss'], 'o-', color=color,
                       label='Train Loss', linewidth=2, markersize=4)
            if 'val_loss' in history:
                epochs = range(1, len(history['val_loss']) + 1)
                ax.plot(epochs, history['val_loss'], 's--', color=color,
                       alpha=0.7, label='Val Loss', linewidth=2, markersize=4)
            if 'val_auc' in history:
                ax2 = ax.twinx()
                epochs = range(1, len(history['val_auc']) + 1)
                ax2.plot(epochs, history['val_auc'], '^-', color='purple',
                        label='Val AUC', linewidth=2, markersize=4)
                ax2.set_ylabel('AUC', color='purple')
                ax2.tick_params(axis='y', labelcolor='purple')
                ax2.set_ylim(0.5, 1.0)

            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title(title, fontsize=12, fontweight='bold')
            ax.legend(loc='upper right')
            ax.grid(True, alpha=0.3)

        plt.suptitle('Curriculum Learning Three-Stage Training Curves', fontsize=16, fontweight='bold')
        plt.tight_layout()

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'curriculum_learning_curves.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Curriculum learning curves saved: {save_path}")

        return fig

    def plot_feature_importance(self,
                                 feature_names: List[str],
                                 importance_scores: np.ndarray,
                                 top_k: int = 15,
                                 save_path: str = None) -> plt.Figure:
        """
        Plot structured feature importance bar chart
        """
        indices = np.argsort(importance_scores)[::-1][:top_k]
        top_names = [feature_names[i] if i < len(feature_names) else f'Feature_{i}' for i in indices]
        top_scores = importance_scores[indices]

        fig, ax = plt.subplots(figsize=(10, 8))

        colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(top_scores)))
        bars = ax.barh(range(len(top_names)), top_scores, color=colors)

        ax.set_yticks(range(len(top_names)))
        ax.set_yticklabels(top_names)
        ax.invert_yaxis()
        ax.set_xlabel('Importance Score', fontsize=12)
        ax.set_title(f'Structured Feature Importance Top-{top_k}', fontsize=14, fontweight='bold')

        for bar, score in zip(bars, top_scores):
            ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                   f'{score:.4f}', va='center', fontsize=9)

        ax.grid(axis='x', alpha=0.3)
        plt.tight_layout()

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'feature_importance.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… Feature importance figure saved: {save_path}")

        return fig


# ============================================================
# Statistical Analysis
# ============================================================

from scipy import stats

class StatisticalAnalyzer:
    """Statistical Significance Analyzer"""

    def __init__(self, save_dir: str = './outputs/figures'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def paired_t_test(self, scores1: List[float], scores2: List[float],
                      name1: str = "Model1", name2: str = "Model2"):
        """Paired t-test"""
        t_stat, p_value = stats.ttest_rel(scores1, scores2)

        result = {
            'model1': name1,
            'model2': name2,
            'mean1': np.mean(scores1),
            'mean2': np.mean(scores2),
            'std1': np.std(scores1),
            'std2': np.std(scores2),
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05
        }

        print(f"\nğŸ“Š Paired t-test: {name1} vs {name2}")
        print(f"  {name1}: {result['mean1']:.4f} Â± {result['std1']:.4f}")
        print(f"  {name2}: {result['mean2']:.4f} Â± {result['std2']:.4f}")
        print(f"  t-statistic: {t_stat:.4f}")
        print(f"  p-value: {p_value:.4f}")
        print(f"  Significant (p<0.05): {'Yes' if result['significant'] else 'No'}")

        return result

    def wilcoxon_test(self, scores1: List[float], scores2: List[float],
                      name1: str = "Model1", name2: str = "Model2"):
        """Wilcoxon signed-rank test (non-parametric)"""
        stat, p_value = stats.wilcoxon(scores1, scores2)

        result = {
            'model1': name1,
            'model2': name2,
            'statistic': stat,
            'p_value': p_value,
            'significant': p_value < 0.05
        }

        print(f"\nğŸ“Š Wilcoxon test: {name1} vs {name2}")
        print(f"  statistic: {stat:.4f}")
        print(f"  p-value: {p_value:.4f}")
        print(f"  Significant (p<0.05): {'Yes' if result['significant'] else 'No'}")

        return result

    def generate_significance_table(self,
                                    all_results: Dict[str, List[float]],
                                    baseline_name: str = "Ours (Full)",
                                    save_path: str = None):
        """
        Generate significance test results table

        Args:
            all_results: Results from multiple runs for all models
                Format: {'model_name': [score1, score2, ...], ...}
            baseline_name: Name of baseline model for comparison
            save_path: Path to save results

        Returns:
            DataFrame with significance test results
        """
        if baseline_name not in all_results:
            print(f"Warning: Baseline model {baseline_name} not found")
            return None

        baseline_scores = all_results[baseline_name]
        results = []

        for model_name, scores in all_results.items():
            if model_name == baseline_name:
                continue

            if len(scores) != len(baseline_scores):
                print(f"Warning: {model_name} sample count mismatch, skipping")
                continue

            t_stat, p_value = stats.ttest_rel(baseline_scores, scores)

            results.append({
                'Model': model_name,
                'Mean': np.mean(scores),
                'Std': np.std(scores),
                'p-value': p_value,
                'Significant': 'Yes' if p_value < 0.05 else 'No'
            })

        # Create DataFrame
        df = pd.DataFrame(results)
        df = df.sort_values('Mean', ascending=False)

        # Print table
        print("\n" + "=" * 60)
        print(f"Significance Test Results (Baseline: {baseline_name})")
        print("=" * 60)
        print(df.to_string(index=False))

        # Save to CSV
        if save_path is None:
            save_path = os.path.join(self.save_dir, 'significance_test.csv')
        df.to_csv(save_path, index=False)
        print(f"\nâœ… Results saved: {save_path}")

        # Generate LaTeX table
        latex_path = save_path.replace('.csv', '.tex')
        latex_content = df.to_latex(index=False, caption='Statistical Significance Test Results',
                                    label='tab:significance')
        with open(latex_path, 'w') as f:
            f.write(latex_content)
        print(f"âœ… LaTeX table saved: {latex_path}")

        return df
    def plot_confidence_intervals(self,
                                   results: Dict[str, Dict[str, float]],
                                   metric: str = 'auc',
                                   save_path: str = None):
        """Plot confidence intervals"""
        fig, ax = plt.subplots(figsize=(12, 6))

        models = list(results.keys())
        means = [results[m].get(metric, results[m].get('mean', 0)) for m in models]
        stds = [results[m].get(f'{metric}_std', results[m].get('std', 0.01)) for m in models]

        ci_95 = [1.96 * s for s in stds]

        colors = ['#e74c3c' if 'Ours' in m or 'full' in m.lower() else '#3498db' for m in models]

        y_pos = np.arange(len(models))
        ax.barh(y_pos, means, xerr=ci_95, color=colors, alpha=0.7, capsize=5)

        ax.set_yticks(y_pos)
        ax.set_yticklabels(models)
        ax.set_xlabel(f'{metric.upper()} (95% CI)', fontsize=12)
        ax.set_title('Model Performance Comparison with Confidence Intervals', fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)

        for i, (mean, ci) in enumerate(zip(means, ci_95)):
            ax.text(mean + ci + 0.01, i, f'{mean:.4f}Â±{ci:.4f}', va='center', fontsize=9)

        plt.tight_layout()

        if save_path is None:
            save_path = os.path.join(self.save_dir, 'confidence_intervals.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Confidence intervals figure saved: {save_path}")

        return fig


if __name__ == "__main__":
    print("Testing visualization tools...")

    # Create sample data
    attention_weights = {
        'text_to_label': torch.randn(1, 8, 10, 10),
        'label_to_text': torch.randn(1, 8, 10, 10)
    }

    # Test attention visualization
    visualizer = AttentionVisualizer()
    visualizer.visualize_cross_modal_attention(
        attention_weights,
        sample_text="Test complaint text content"
    )

    # Test feature importance visualization
    feature_viz = FeatureImportanceVisualizer()
    feature_names = ['Tenure', 'Monthly_Spend', 'Complaint_Count', 'Satisfaction', 'Plan_Type']
    feature_importance = np.random.rand(5)
    feature_viz.visualize_structured_features(feature_names, feature_importance)

    print("Visualization test completed!")