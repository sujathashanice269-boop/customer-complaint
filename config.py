"""
é…ç½®æ–‡ä»¶ - å®Œå…¨æ”¹è¿›ç‰ˆ
æ”¯æŒå…­ä¸ªæ–¹å‘çš„æ‰€æœ‰æ–°å‚æ•°
"""

import json
import os
from typing import Dict, Any
from dataclasses import dataclass, field, asdict


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    # BERTç›¸å…³
    bert_model_name: str = 'bert-base-chinese'
    bert_max_length: int = 256

    # GATæ ‡ç­¾ç¼–ç å™¨
    label_embedding_dim: int = 128
    label_hidden_dim: int = 256
    num_gat_layers: int = 3
    num_gat_heads: int = 4
    max_label_depth: int = 8

    # è·¨æ¨¡æ€æ³¨æ„åŠ› - æ–¹å‘å››
    use_cross_attention: bool = True
    cross_attn_heads: int = 4

    # èåˆå±‚
    fusion_dim: int = 256
    hidden_dim: int = 256
    dropout: float = 0.3

    # ç»“æ„åŒ–ç‰¹å¾ - æ–¹å‘ä¸‰
    struct_feat_dim: int = 53
    use_feature_importance: bool = True


@dataclass
class PretrainConfig:
    """é¢„è®­ç»ƒé…ç½® - æ–¹å‘ä¸€å’Œæ–¹å‘äºŒ"""

    # ===== æ–¹å‘ä¸€: Texté¢„è®­ç»ƒ =====
    # é˜¶æ®µ1: çº¯MLMé¢†åŸŸé€‚åº”
    stage1_epochs: int = 30
    stage1_lr: float = 5e-5
    stage1_mask_prob: float = 0.15

    # Span Maskingç­–ç•¥
    use_span_masking: bool = True
    span_mask_length: int = 3
    span_mask_prob: float = 0.3

    # é˜¶æ®µ2: å¯¹æ¯”å­¦ä¹ 
    stage2_epochs: int = 20
    stage2_lr: float = 3e-5
    use_contrastive: bool = False
    contrastive_temperature: float = 0.5
    contrastive_loss_weight: float = 0.3

    # ===== æ–¹å‘äºŒ: Labelå…¨å±€å›¾é¢„è®­ç»ƒ =====
    use_global_graph_pretrain: bool = False
    global_graph_epochs: int = 10
    global_graph_lr: float = 1e-4

    # å…¨å±€å›¾é¢„è®­ç»ƒä»»åŠ¡
    use_node_prediction: bool = True
    use_link_prediction: bool = True
    use_subgraph_classification: bool = True

    # é€šç”¨å‚æ•°
    pretrain_batch_size: int = 32
    save_steps: int = 500
    eval_steps: int = 500


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½® - æ–¹å‘äº”è¯¾ç¨‹å­¦ä¹ """

    # æ•°æ®
    data_file: str = 'å°æ¡ˆä¾‹aié—®è¯¢.xlsx'
    large_data_file: str = 'å¤šæ¨¡æ€åˆå§‹è¡¨_æ•°æ®æ ‡ç­¾.xlsx'
    test_size: float = 0.2
    val_size: float = 0.1
    random_state: int = 42

    # ===== æ–¹å‘äº”: è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ =====
    use_curriculum_learning: bool = True

    # é˜¶æ®µ1: å•æ¨¡æ€é¢„è®­ç»ƒ
    stage1_single_modal_epochs: int = 10
    stage1_lr: float = 2e-5

    # é˜¶æ®µ2: åŒæ¨¡æ€äº¤äº’
    stage2_dual_modal_epochs: int = 10
    stage2_lr: float = 1e-5

    # é˜¶æ®µ3: ä¸‰æ¨¡æ€èåˆ
    stage3_full_epochs: int = 20
    stage3_lr: float = 5e-6

    # å¦‚æœä¸ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
    num_epochs: int = 30
    learning_rate: float = 2e-5

    # ä¼˜åŒ–å™¨
    batch_size: int = 16
    weight_decay: float = 0.01
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 5.0

    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_steps: int = 500
    scheduler_type: str = 'cosine'

    # æ—©åœ
    early_stopping_patience: int = 5
    early_stopping_delta: float = 0.001

    # ===== æ–¹å‘å…­: æ¨¡æ€å¹³è¡¡æŸå¤± =====
    use_modal_balance_loss: bool = False
    modal_balance_weight: float = 0.1

    # æŸå¤±å‡½æ•°
    use_focal_loss: bool = True
    focal_alpha: float = 0.25
    focal_gamma: float = 2.0

    # æ ‡ç­¾å¹³æ»‘
    use_label_smoothing: bool = False
    label_smoothing: float = 0.1

    # æ•°æ®å¢å¼º
    use_data_augmentation: bool = True
    augmentation_prob: float = 0.5

    # è®¾å¤‡
    device: str = 'cuda'
    num_workers: int = 0

    # ä¿å­˜å’Œæ—¥å¿—
    save_dir: str = './models'
    log_dir: str = './logs'
    save_steps: int = 1000
    log_interval: int = 10

    # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    pretrain_save_dir: str = './pretrained_complaint_bert_improved'
    label_pretrain_save_dir: str = './pretrained_label_graph'


@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    max_text_length: int = 256
    max_label_nodes: int = 10

    # ç”¨æˆ·è¯å…¸ - åªç”¨äºæ–‡æœ¬
    user_dict_file: str = 'new_user_dict.txt'

    # ç»“æ„åŒ–ç‰¹å¾åˆ—ï¼ˆFåˆ—åˆ°CRåˆ—ï¼Œå…±53åˆ—ï¼‰
    struct_start_col: int = 5
    struct_end_col: int = 57

    # æ•°æ®å¢å¼ºå‚æ•°
    text_augment_prob: float = 0.5
    synonym_replace_prob: float = 0.3
    random_delete_prob: float = 0.1
    random_swap_prob: float = 0.1


@dataclass
class Config:
    """å®Œæ•´é…ç½®"""
    model: ModelConfig = field(default_factory=ModelConfig)
    pretrain: PretrainConfig = field(default_factory=PretrainConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    data: DataConfig = field(default_factory=DataConfig)

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        import torch
        if self.training.device == 'cuda' and not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
            self.training.device = 'cpu'

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.training.save_dir, exist_ok=True)
        os.makedirs(self.training.log_dir, exist_ok=True)
        os.makedirs(self.training.pretrain_save_dir, exist_ok=True)
        os.makedirs(self.training.label_pretrain_save_dir, exist_ok=True)

    def save_config(self, path: str):
        """ä¿å­˜é…ç½®"""
        config_dict = {
            'model': asdict(self.model),
            'pretrain': asdict(self.pretrain),
            'training': asdict(self.training),
            'data': asdict(self.data)
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {path}")

    @classmethod
    def load_config(cls, path: str):
        """åŠ è½½é…ç½®"""
        with open(path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)

        config = cls()
        if 'model' in config_dict:
            config.model = ModelConfig(**config_dict['model'])
        if 'pretrain' in config_dict:
            config.pretrain = PretrainConfig(**config_dict['pretrain'])
        if 'training' in config_dict:
            config.training = TrainingConfig(**config_dict['training'])
        if 'data' in config_dict:
            config.data = DataConfig(**config_dict['data'])

        print(f"âœ… é…ç½®å·²åŠ è½½: {path}")
        return config

    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("é…ç½®æ‘˜è¦")
        print("="*60)

        print("\nğŸ“Š æ¨¡å‹é…ç½®:")
        print(f"  - BERTæœ€å¤§é•¿åº¦: {self.model.bert_max_length}")
        print(f"  - GATå±‚æ•°: {self.model.num_gat_layers}")
        print(f"  - è·¨æ¨¡æ€æ³¨æ„åŠ›: {'âœ“' if self.model.use_cross_attention else 'âœ—'}")
        print(f"  - ç‰¹å¾é‡è¦æ€§åŠ æƒ: {'âœ“' if self.model.use_feature_importance else 'âœ—'}")
        print(f"  - ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {self.model.struct_feat_dim}")

        print("\nğŸ¯ é¢„è®­ç»ƒé…ç½®:")
        print(f"  - Texté˜¶æ®µ1 (MLM): {self.pretrain.stage1_epochs}è½®")
        print(f"  - Texté˜¶æ®µ2 (å¯¹æ¯”): {self.pretrain.stage2_epochs}è½®")
        print(f"  - Span Masking: {'âœ“' if self.pretrain.use_span_masking else 'âœ—'}")
        print(f"  - å¯¹æ¯”å­¦ä¹ : {'âœ“' if self.pretrain.use_contrastive else 'âœ—'}")
        print(f"  - Labelå…¨å±€å›¾é¢„è®­ç»ƒ: {'âœ“' if self.pretrain.use_global_graph_pretrain else 'âœ—'}")

        print("\nğŸš€ è®­ç»ƒé…ç½®:")
        if self.training.use_curriculum_learning:
            print(f"  - è¯¾ç¨‹å­¦ä¹ : âœ“")
            print(f"    â€¢ é˜¶æ®µ1 (å•æ¨¡æ€): {self.training.stage1_single_modal_epochs}è½®")
            print(f"    â€¢ é˜¶æ®µ2 (åŒæ¨¡æ€): {self.training.stage2_dual_modal_epochs}è½®")
            print(f"    â€¢ é˜¶æ®µ3 (ä¸‰æ¨¡æ€): {self.training.stage3_full_epochs}è½®")
        else:
            print(f"  - æ ‡å‡†è®­ç»ƒ: {self.training.num_epochs}è½®")

        print(f"  - æ¨¡æ€å¹³è¡¡æŸå¤±: {'âœ“' if self.training.use_modal_balance_loss else 'âœ—'}")
        print(f"  - Focal Loss: {'âœ“' if self.training.use_focal_loss else 'âœ—'}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {self.training.batch_size}")
        print(f"  - å­¦ä¹ ç‡: {self.training.learning_rate}")
        print(f"  - è®¾å¤‡: {self.training.device}")

        print("\nğŸ“ æ•°æ®é…ç½®:")
        print(f"  - è®­ç»ƒæ•°æ®: {self.training.data_file}")
        print(f"  - å¤§è§„æ¨¡æ•°æ®: {self.training.large_data_file}")
        print(f"  - ç”¨æˆ·è¯å…¸: {self.data.user_dict_file}")
        print(f"  - ç»“æ„åŒ–ç‰¹å¾: {self.model.struct_feat_dim}ç»´")

        print("\nğŸ’¾ ä¿å­˜è·¯å¾„:")
        print(f"  - æ¨¡å‹ä¿å­˜: {self.training.save_dir}")
        print(f"  - Texté¢„è®­ç»ƒ: {self.training.pretrain_save_dir}")
        print(f"  - Labelé¢„è®­ç»ƒ: {self.training.label_pretrain_save_dir}")
        print("="*60 + "\n")


def get_default_config() -> Config:
    """è·å–é»˜è®¤é…ç½®"""
    return Config()


def get_quick_test_config() -> Config:
    """è·å–å¿«é€Ÿæµ‹è¯•é…ç½®"""
    config = Config()

    # å‡å°‘è®­ç»ƒè½®æ•°
    config.pretrain.stage1_epochs = 1
    config.pretrain.stage2_epochs = 1
    config.pretrain.global_graph_epochs = 1

    config.training.stage1_single_modal_epochs = 1
    config.training.stage2_dual_modal_epochs = 1
    config.training.stage3_full_epochs = 1
    config.training.num_epochs = 5

    # â­ å…³é”®ä¿®æ”¹5: è°ƒæ•´batch sizeå’Œå­¦ä¹ ç‡
    config.training.batch_size = 8  # ä»4æ”¹æˆ8,æ›´ç¨³å®š
    config.pretrain.pretrain_batch_size = 8

    # é™ä½å­¦ä¹ ç‡,ç‰¹åˆ«æ˜¯å¯¹æ–°è¯
    config.pretrain.stage1_lr = 2e-5  # ä»5e-5é™ä½åˆ°2e-5
    config.pretrain.stage2_lr = 1e-5  # ä»3e-5é™ä½åˆ°1e-5

    print("âš¡ ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½® (å·²ä¼˜åŒ–ç¨³å®šæ€§)")
    return config


def get_production_config() -> Config:
    """è·å–ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    config = Config()

    # å®Œæ•´çš„é¢„è®­ç»ƒè½®æ•°
    config.pretrain.stage1_epochs = 30
    config.pretrain.stage2_epochs = 20
    config.pretrain.global_graph_epochs = 20

    # å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ 
    config.training.use_curriculum_learning = True
    config.training.stage1_single_modal_epochs = 10
    config.training.stage2_dual_modal_epochs = 10
    config.training.stage3_full_epochs = 20

    # å¤§æ‰¹é‡
    config.training.batch_size = 32
    config.pretrain.pretrain_batch_size = 32

    print("ğŸ­ ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒé…ç½®")
    return config


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print("æµ‹è¯•é»˜è®¤é…ç½®:")
    config = get_default_config()
    config.print_summary()