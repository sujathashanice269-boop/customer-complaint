"""
ä¸»è®­ç»ƒè„šæœ¬ - å®Œå…¨æ”¹è¿›ç‰ˆ
å®ç°ç­–ç•¥Aï¼šé¢„è®­ç»ƒç”¨24ä¸‡ï¼Œè®­ç»ƒç”¨3.5ä¸‡
âœ… ä¿®å¤ï¼šLabelå…¨å±€å›¾é¢„è®­ç»ƒçš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import warnings
warnings.filterwarnings('ignore')
import torch.nn.functional as F
from config import Config, get_quick_test_config, get_production_config
from data_processor import ComplaintDataProcessor, ComplaintDataset, custom_collate_fn
from model import MultiModalComplaintModel, FocalLoss, ModalBalanceLoss


# ========== æºå¤´é¢„é˜²: è®­ç»ƒç›‘æ§ç±» ==========
class TrainingMonitor:
    """
    è®­ç»ƒç›‘æ§ç±» - å®æ—¶æ£€æµ‹è®­ç»ƒå¼‚å¸¸
    é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€Losså¼‚å¸¸ã€æƒé‡NaNç­‰é—®é¢˜
    """

    def __init__(self, window_size=10):
        self.loss_history = []
        self.gradient_norms = []
        self.window_size = window_size
        self.nan_count = 0
        self.inf_count = 0

    def check_loss(self, loss):
        """æ£€æŸ¥lossæ˜¯å¦å¼‚å¸¸"""
        # æ£€æŸ¥NaN
        if torch.isnan(loss):
            self.nan_count += 1
            print(f"âŒ æ£€æµ‹åˆ°NaN Loss! (ç¬¬{self.nan_count}æ¬¡)")
            return False

        # æ£€æŸ¥Inf
        if torch.isinf(loss):
            self.inf_count += 1
            print(f"âŒ æ£€æµ‹åˆ°Inf Loss! (ç¬¬{self.inf_count}æ¬¡)")
            return False

        # æ£€æŸ¥æ˜¯å¦çªç„¶æš´æ¶¨
        if len(self.loss_history) >= self.window_size:
            recent_avg = sum(self.loss_history[-self.window_size:]) / self.window_size
            if loss.item() > recent_avg * 10:
                print(f"âš ï¸ Lossæš´æ¶¨: {recent_avg:.4f} â†’ {loss.item():.4f}")
                # ä¸åœæ­¢è®­ç»ƒï¼Œåªæ˜¯è­¦å‘Š

        self.loss_history.append(loss.item())
        return True

    def check_gradients(self, model, max_norm=10.0):
        """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸"""
        total_norm = 0.0
        nan_params = []
        inf_params = []

        for name, param in model.named_parameters():
            if param.grad is not None:
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰NaNæˆ–Inf
                if torch.isnan(param.grad).any():
                    nan_params.append(name)
                if torch.isinf(param.grad).any():
                    inf_params.append(name)

                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                param_norm = param.grad.data.norm(2).item()
                total_norm += param_norm ** 2

        total_norm = total_norm ** 0.5
        self.gradient_norms.append(total_norm)

        # æŠ¥å‘Šå¼‚å¸¸
        if nan_params:
            print(f"âŒ ä»¥ä¸‹å‚æ•°çš„æ¢¯åº¦åŒ…å«NaN: {nan_params[:3]}...")
            return False

        if inf_params:
            print(f"âŒ ä»¥ä¸‹å‚æ•°çš„æ¢¯åº¦åŒ…å«Inf: {inf_params[:3]}...")
            return False

        if total_norm > max_norm * 2:
            print(f"âš ï¸ æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {total_norm:.2f} (é˜ˆå€¼: {max_norm})")

        return True

    def check_model_weights(self, model):
        """æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦å¼‚å¸¸"""
        nan_weights = []
        inf_weights = []

        for name, param in model.named_parameters():
            if torch.isnan(param).any():
                nan_weights.append(name)
            if torch.isinf(param).any():
                inf_weights.append(name)

        if nan_weights:
            print(f"âŒ ä»¥ä¸‹æƒé‡åŒ…å«NaN: {nan_weights[:3]}...")
            return False

        if inf_weights:
            print(f"âŒ ä»¥ä¸‹æƒé‡åŒ…å«Inf: {inf_weights[:3]}...")
            return False

        return True

    def get_summary(self):
        """è·å–ç›‘æ§æ‘˜è¦"""
        if len(self.loss_history) == 0:
            return "æ— ç›‘æ§æ•°æ®"

        recent_losses = self.loss_history[-self.window_size:]
        avg_loss = sum(recent_losses) / len(recent_losses)

        if len(self.gradient_norms) > 0:
            recent_grads = self.gradient_norms[-self.window_size:]
            avg_grad = sum(recent_grads) / len(recent_grads)
        else:
            avg_grad = 0.0

        return f"è¿‘æœŸå¹³å‡Loss: {avg_loss:.4f}, æ¢¯åº¦èŒƒæ•°: {avg_grad:.2f}, NaNæ¬¡æ•°: {self.nan_count}, Infæ¬¡æ•°: {self.inf_count}"
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True


def pretrain_text_stage1(config, processor, save_dir):
    """
    Texté¢„è®­ç»ƒé˜¶æ®µ1: çº¯MLMé¢†åŸŸé€‚åº”
    ä½¿ç”¨24ä¸‡æ•°æ® - ä¿®å¤ç‰ˆ
    """
    print("\n" + "=" * 60)
    print(f"ğŸ¯ Texté¢„è®­ç»ƒé˜¶æ®µ1: çº¯MLM")
    print(f"   è½®æ•°: {config.pretrain.stage1_epochs}")
    print("=" * 60)

    from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup
    from torch.utils.data import DataLoader
    import pandas as pd
    from tqdm import tqdm

    # åŠ è½½tokenizer
    tokenizer = BertTokenizer.from_pretrained(config.model.bert_model_name)

    # âœ… å…ˆåˆå§‹åŒ–num_added
    num_added = 0
    original_vocab_size = len(tokenizer)  # ä¹Ÿæå‰è·å–ï¼Œé¿å…åé¢æœªå®šä¹‰

    # ============================================================
    # â­ å…³é”®ä¿®æ”¹1: æ·»åŠ ç”¨æˆ·è¯å…¸åˆ°tokenizer
    # ============================================================
    if hasattr(processor, 'user_dict_whitelist') and processor.user_dict_whitelist:
        user_words = list(processor.user_dict_whitelist)
        print(f"\nğŸ“š æ·»åŠ ç”¨æˆ·è¯å…¸åˆ°BERTè¯è¡¨:")
        print(f"  ç”¨æˆ·è¯æ•°é‡: {len(user_words)}")

        num_added = tokenizer.add_tokens(user_words)
        print(f"  æ·»åŠ äº† {num_added} ä¸ªæ–°è¯åˆ°BERTè¯è¡¨")
        print(f"  è¯è¡¨å¤§å°: {original_vocab_size} â†’ {len(tokenizer)}")

        # æµ‹è¯•åˆ†è¯
        test_text = "å¤šæ¬¡æŠ•è¯‰ä¿¡å·å·®é—®é¢˜æœªè§£å†³"
        tokens = tokenizer.tokenize(test_text)
        print(f"\n  æµ‹è¯•åˆ†è¯: {test_text}")
        print(f"  ç»“æœ: {tokens}\n")

    # åŠ è½½BERTæ¨¡å‹
    model = BertForMaskedLM.from_pretrained(config.model.bert_model_name)

    # âœ… é‡è¦: è°ƒæ•´embeddingå±‚å¤§å°ä»¥åŒ¹é…æ–°è¯è¡¨
    if num_added > 0:
        model.resize_token_embeddings(len(tokenizer))

        # â­ å…³é”®ä¿®æ”¹1: ç¨³å®šåˆå§‹åŒ–æ–°è¯embedding
        with torch.no_grad():
            # è·å–embeddingå±‚
            embeddings = model.get_input_embeddings()

            # è®¡ç®—åŸå§‹è¯çš„embeddingå‡å€¼å’Œæ ‡å‡†å·®
            original_embeddings = embeddings.weight[:original_vocab_size]
            mean = original_embeddings.mean(dim=0)
            std = original_embeddings.std(dim=0)

            # ç”¨æ›´å°çš„æ ‡å‡†å·®åˆå§‹åŒ–æ–°è¯ (0.02 â†’ 0.01)
            new_embeddings = embeddings.weight[original_vocab_size:]
            new_embeddings.normal_(mean=0.0, std=0.01)

            # ä¹Ÿå¯ä»¥é€‰æ‹©ç”¨åŸå§‹embeddingçš„å¹³å‡å€¼åˆå§‹åŒ–(æ›´ä¿å®ˆ)
            # new_embeddings.copy_(mean.unsqueeze(0).expand(num_added, -1))

        print(f"  è°ƒæ•´model embeddingå±‚: {original_vocab_size} â†’ {len(tokenizer)}")
        print(f"  âœ“ æ–°è¯embeddingå·²ç”¨æ›´ç¨³å®šçš„åˆå§‹åŒ– (std=0.01)\n")

    model = model.to(config.training.device)

    # ============================================================
    # â­ å…³é”®ä¿®æ”¹2: ç›´æ¥è¯»å–å¹¶æ¸…æ´—åŸå§‹æ–‡æœ¬
    # ============================================================
    print(f"ğŸ“‚ åŠ è½½å¹¶æ¸…æ´—é¢„è®­ç»ƒæ•°æ®: {config.training.large_data_file}")

    # è¯»å–Excel
    df = pd.read_excel(config.training.large_data_file)
    print(f"  åŸå§‹æ•°æ®é‡: {len(df)}")

    # æå–æ–‡æœ¬åˆ—
    raw_texts = df['biz_cntt'].fillna('').astype(str).tolist()

    # æ¸…æ´—æ–‡æœ¬
    print(f"  æ­£åœ¨æ¸…æ´—æ–‡æœ¬...")
    cleaned_texts = []
    for text in tqdm(raw_texts, desc="æ¸…æ´—æ–‡æœ¬"):
        cleaned = processor.clean_text_smart(text)
        if cleaned:  # åªä¿ç•™éç©ºæ–‡æœ¬
            cleaned_texts.append(cleaned)

    print(f"  æ¸…æ´—åæ•°æ®é‡: {len(cleaned_texts)}")
    print(f"  ç¤ºä¾‹æ–‡æœ¬:")
    for i, text in enumerate(cleaned_texts[:3]):
        print(f"    {i + 1}. {text[:60]}...")
    print()

    # ============================================================
    # â­ å…³é”®ä¿®æ”¹3: ä¼ å…¥æ¸…æ´—åçš„æ–‡æœ¬å­—ç¬¦ä¸²
    # ============================================================
    from pretrain_tasks import TextPretrainDataset

    pretrain_dataset = TextPretrainDataset(
        texts=cleaned_texts,  # âœ… ä¼ å…¥æ¸…æ´—åçš„æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨
        tokenizer=tokenizer,
        max_length=config.model.bert_max_length,
        mask_prob=config.pretrain.stage1_mask_prob,
        use_span_masking=config.pretrain.use_span_masking,
        span_length=config.pretrain.span_mask_length
    )

    dataloader = DataLoader(
        pretrain_dataset,
        batch_size=config.pretrain.pretrain_batch_size,
        shuffle=True,
        num_workers=0,
        drop_last=True
    )

    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=config.pretrain.stage1_lr)
    total_steps = len(dataloader) * config.pretrain.stage1_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=500,
        num_training_steps=total_steps
    )

    # è®­ç»ƒ
    best_loss = float('inf')
    stage1_save_dir = os.path.join(save_dir, 'stage1')
    os.makedirs(stage1_save_dir, exist_ok=True)

    for epoch in range(config.pretrain.stage1_epochs):
        model.train()
        total_loss = 0

        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{config.pretrain.stage1_epochs}")

        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch['input_ids'].to(config.training.device)
            attention_mask = batch['attention_mask'].to(config.training.device)
            labels = batch['mlm_labels'].to(config.training.device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss

            # â­ å…³é”®ä¿®æ”¹2: NaNæ£€æµ‹å’Œè·³è¿‡
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nâš ï¸ è­¦å‘Š: Batch {batch_idx} çš„lossæ˜¯ {loss.item()}, è·³è¿‡æ­¤batch")
                optimizer.zero_grad()
                continue

            loss.backward()

            # â­ é¢å¤–ä¿æŠ¤: æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN
            has_nan_grad = False
            for name, param in model.named_parameters():
                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):
                    print(f"\nâš ï¸ è­¦å‘Š: å‚æ•° {name} çš„æ¢¯åº¦åŒ…å«NaN/Inf, è·³è¿‡æ­¤batch")
                    has_nan_grad = True
                    break

            if has_nan_grad:
                optimizer.zero_grad()
                continue

            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch + 1} - MLM Loss: {avg_loss:.4f}")

        # â­ ä¿®æ”¹4-æ”¹è¿›ç‰ˆ: æ™ºèƒ½ä¿å­˜ç­–ç•¥
        # åªæœ‰lossæ­£å¸¸ä¸”æ›´å¥½æ—¶æ‰ä¿å­˜
        if not torch.isnan(torch.tensor(avg_loss)) and not torch.isinf(torch.tensor(avg_loss)):
            if avg_loss < best_loss:
                best_loss = avg_loss
                model.save_pretrained(stage1_save_dir)
                tokenizer.save_pretrained(stage1_save_dir)
                print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (loss={avg_loss:.4f})")
        else:
            print(f"  âš ï¸ æœ¬epochçš„losså¼‚å¸¸({avg_loss}), è·³è¿‡ä¿å­˜")

    print(f"\nâœ… é˜¶æ®µ1å®Œæˆ - æœ€ä½³Loss: {best_loss:.4f}")

    # â­ å¦‚æœæ‰€æœ‰epochéƒ½å¤±è´¥,æŠ¥é”™å¹¶ç»ˆæ­¢
    if best_loss == float('inf'):
        error_msg = (
            "\nâŒ ä¸¥é‡é”™è¯¯: æ‰€æœ‰epochçš„losséƒ½æ˜¯NaN/Inf!\n"
            "å¯èƒ½åŸå› :\n"
            "  1. å­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸\n"
            "  2. æ•°æ®ä¸­å­˜åœ¨æç«¯å¼‚å¸¸å€¼\n"
            "  3. æ‰¹æ¬¡å¤§å°è¿‡å°å¯¼è‡´è®­ç»ƒä¸ç¨³å®š\n"
            "å»ºè®®:\n"
            "  1. é™ä½å­¦ä¹ ç‡ (å¦‚ 1e-5)\n"
            "  2. å¢åŠ batch size (å¦‚ 16 æˆ– 32)\n"
            "  3. æ£€æŸ¥æ•°æ®è´¨é‡\n"
        )
        print(error_msg)
        raise RuntimeError("é¢„è®­ç»ƒå¤±è´¥: losså§‹ç»ˆä¸ºNaN/Inf")

    print()
    return stage1_save_dir


# ============================================================
# ä¿®æ”¹: Texté¢„è®­ç»ƒé˜¶æ®µ2 - å¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬
# åŸæ¥: MLM + Classification
# ç°åœ¨: Supervised Contrastive Learning
# ============================================================

def pretrain_text_stage2_supcon(config, processor=None, save_dir="./pretrained_complaint_bert_improved"):
    """
    Texté¢„è®­ç»ƒé˜¶æ®µ2: Supervised Contrastive Learning

    æµç¨‹:
        1. åŠ è½½é˜¶æ®µ1é¢„è®­ç»ƒçš„BERT
        2. åˆ›å»ºBERTForContrastiveLearning (BERT + Projection)
        3. åŠ è½½24ä¸‡æ•°æ® + Repeat complaintæ ‡ç­¾
        4. ä½¿ç”¨BalancedBatchSampler (30% pos + 70% neg)
        5. SupCon Lossè®­ç»ƒ20è½®
        6. ä¿å­˜BERT (ä¸¢å¼ƒprojection)

    Args:
        config: é…ç½®å¯¹è±¡
        save_dir: ä¿å­˜è·¯å¾„

    Returns:
        save_dir: é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„
    """

    print("=" * 70)
    print("ğŸ¯ Texté¢„è®­ç»ƒé˜¶æ®µ2: Supervised Contrastive Learning")
    print("=" * 70)
    print(f"æ–¹æ³•: ç›‘ç£å¯¹æ¯”å­¦ä¹  (SupCon)")
    print(f"è½®æ•°: {config.pretrain.stage2_epochs}")
    print(f"Batch size: {config.pretrain.pretrain_batch_size}")
    print(f"æ¸©åº¦å‚æ•°: {config.pretrain.contrastive_temperature}")
    print(f"æŠ•å½±ç»´åº¦: 128")
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹: 30%")
    print("=" * 70 + "\n")
    # ===== 0. åˆ›å»ºprocessor(å¦‚æœæœªä¼ å…¥) =====
    if processor is None:
        from data_processor import ComplaintDataProcessor
        processor = ComplaintDataProcessor(config)
        print("âœ“ å·²åˆ›å»ºæ–°processor")
    # ===== 1. åŠ è½½æ•°æ® =====
    print("ğŸ“‚ Step 1: åŠ è½½é¢„è®­ç»ƒæ•°æ®")
    print(f"æ–‡ä»¶: {config.training.large_data_file}")

    import pandas as pd
    from tqdm import tqdm

    df = pd.read_excel(config.training.large_data_file)
    print(f"âœ“ åŸå§‹æ•°æ®é‡: {len(df)}")

    # æå–æ–‡æœ¬å’Œæ ‡ç­¾
    texts = df['biz_cntt'].fillna('').astype(str).tolist()
    labels = df['Repeat complaint'].astype(int).tolist()

    # ç»Ÿè®¡
    num_pos = sum(labels)
    num_neg = len(labels) - num_pos

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  - æœ‰æ•ˆæ ·æœ¬: {len(texts)}")
    print(f"  - é‡å¤æŠ•è¯‰: {num_pos} ({num_pos / len(labels) * 100:.2f}%)")
    print(f"  - éé‡å¤: {num_neg} ({num_neg / len(labels) * 100:.2f}%)")
    print()

    # ===== 2. åˆ›å»ºæ¨¡å‹ =====
    print("ğŸ”¨ Step 2: åˆ›å»ºæ¨¡å‹")

    # 2.1 ç¡®å®šé˜¶æ®µ1æ¨¡å‹è·¯å¾„
    stage1_dir = os.path.join(config.training.pretrain_save_dir, "stage1")

    if not os.path.exists(stage1_dir):
        print(f"âš ï¸  é˜¶æ®µ1æ¨¡å‹ä¸å­˜åœ¨: {stage1_dir}")
        print(f"ä½¿ç”¨åŸå§‹BERT: {config.model.bert_model_name}")
        stage1_dir = config.model.bert_model_name
    else:
        print(f"âœ“ åŠ è½½é˜¶æ®µ1æ¨¡å‹: {stage1_dir}")

    # 2.2 åˆ›å»ºå¯¹æ¯”å­¦ä¹ æ¨¡å‹
    from model import BERTForContrastiveLearning, SupConLoss

    model = BERTForContrastiveLearning(
        bert_model_name=stage1_dir,
        projection_dim=128
    )
    model.to(config.training.device)

    print(f"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"  - è®¾å¤‡: {config.training.device}")
    print()

    # ===== 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ =====
    print("ğŸ“¦ Step 3: åˆ›å»ºæ•°æ®åŠ è½½å™¨")

    from transformers import BertTokenizer
    from data_processor import BalancedBatchSampler, ContrastiveTextDataset
    from torch.utils.data import DataLoader

    # 3.1 Tokenizer
    tokenizer = BertTokenizer.from_pretrained(config.model.bert_model_name)

    # 3.2 Dataset
    dataset = ContrastiveTextDataset(
        texts=texts,
        labels=labels,
        tokenizer=tokenizer,
        max_length=config.model.bert_max_length
    )

    # 3.3 BalancedBatchSampler â­ æ ¸å¿ƒ!
    batch_sampler = BalancedBatchSampler(
        labels=labels,
        batch_size=config.pretrain.pretrain_batch_size,
        pos_ratio=0.3,  # 30%é‡å¤æŠ•è¯‰
        shuffle=True
    )

    # 3.4 DataLoader
    dataloader = DataLoader(
        dataset,
        batch_sampler=batch_sampler,
        num_workers=4,
        pin_memory=True if config.training.device == 'cuda' else False
    )

    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"  - Batchæ•°/epoch: {len(dataloader)}")
    print(f"  - æ¯epochæ ·æœ¬æ•°: {len(dataloader) * config.pretrain.pretrain_batch_size}")
    print()

    # ===== 4. åˆ›å»ºä¼˜åŒ–å™¨ =====
    print("âš™ï¸  Step 4: åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨")

    from torch.optim import AdamW
    from transformers import get_linear_schedule_with_warmup

    optimizer = AdamW(
        model.parameters(),
        lr=config.pretrain.stage2_lr,
        weight_decay=0.01
    )

    # å­¦ä¹ ç‡è°ƒåº¦: warmup + linear decay
    total_steps = len(dataloader) * config.pretrain.stage2_epochs
    warmup_steps = int(total_steps * 0.1)  # 10% warmup

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )

    # SupCon Loss
    criterion = SupConLoss(temperature=config.pretrain.contrastive_temperature)

    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
    print(f"  - å­¦ä¹ ç‡: {config.pretrain.stage2_lr}")
    print(f"  - Warmup steps: {warmup_steps}")
    print(f"  - Total steps: {total_steps}")
    print(f"  - æ¸©åº¦å‚æ•°: {config.pretrain.contrastive_temperature}")
    print()

    # ===== 5. è®­ç»ƒå¾ªç¯ =====
    print("ğŸš€ Step 5: å¼€å§‹è®­ç»ƒ")
    print("=" * 70 + "\n")

    best_loss = float('inf')

    for epoch in range(config.pretrain.stage2_epochs):
        model.train()
        total_loss = 0
        num_batches = 0

        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch + 1}/{config.pretrain.stage2_epochs}"
        )

        for batch in progress_bar:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(config.training.device)
            attention_mask = batch['attention_mask'].to(config.training.device)
            labels_batch = batch['label'].to(config.training.device)

            # å‰å‘ä¼ æ’­: è·å–å½’ä¸€åŒ–çš„æŠ•å½±ç‰¹å¾
            features = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_projection=True  # â­ è¿”å›128ç»´æŠ•å½±
            )
            # features: [batch, 128], å·²L2å½’ä¸€åŒ–

            # è®¡ç®—SupCon Loss
            loss = criterion(features, labels_batch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            scheduler.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        # Epochç»“æŸç»Ÿè®¡
        avg_loss = total_loss / num_batches

        print(f"\nEpoch {epoch + 1}/{config.pretrain.stage2_epochs}")
        print(f"  Loss: {avg_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss

            # âš ï¸ å…³é”®: åªä¿å­˜BERTï¼Œä¸¢å¼ƒprojection!
            bert_only = model.get_bert_only()

            # âœ… ä¿®å¤1: åŒæ—¶ä¿å­˜åˆ°çˆ¶ç›®å½•å’Œstage2ç›®å½•
            # çˆ¶ç›®å½•ä¿å­˜(ä¾›è¯¾ç¨‹å­¦ä¹ åŠ è½½)
            bert_only.save_pretrained(save_dir)
            tokenizer.save_pretrained(save_dir)

            # stage2å­ç›®å½•ä¿å­˜(ä¿æŒå…¼å®¹æ€§)
            stage2_save_dir = os.path.join(save_dir, 'stage2')
            os.makedirs(stage2_save_dir, exist_ok=True)
            bert_only.save_pretrained(stage2_save_dir)
            tokenizer.save_pretrained(stage2_save_dir)

            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (loss={avg_loss:.4f})")
            print(f"    çˆ¶ç›®å½•: {save_dir}")
            print(f"    å­ç›®å½•: {stage2_save_dir}")

        print()

    print("=" * 70)
    print(f"âœ… é˜¶æ®µ2è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³Loss: {best_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
    print("=" * 70 + "\n")
    # âœ… ä¿®å¤2: ä¿å­˜processoråˆ°çˆ¶ç›®å½•(ä¾›è¯¾ç¨‹å­¦ä¹ åŠ è½½)
    if hasattr(processor, 'save'):
        processor_save_path = os.path.join(save_dir, 'processor.pkl')
        processor.save(processor_save_path)
        print(f"âœ… Processorå·²ä¿å­˜åˆ°çˆ¶ç›®å½•: {processor_save_path}")

    print("=" * 70 + "\n")
    return save_dir


def pretrain_label_regression(config, save_dir="./pretrained_label_regression"):
    """
    Labelé¢„è®­ç»ƒ - å›å½’æ–¹æ¡ˆ

    ä»»åŠ¡: é¢„æµ‹æ ‡ç­¾è·¯å¾„çš„é‡å¤æŠ•è¯‰ç‡

    ç›®æ ‡:
        - è®©label_encoderå­¦ä¹ è·¯å¾„â†’é£é™©æ˜ å°„
        - è¾“å‡º256ç»´å‘é‡ï¼ŒåŒ…å«åˆ¤åˆ«ä¿¡æ¯
        - ç”¨äºä¸‹æ¸¸ä¸‰æ¨¡æ€èåˆ

    æ•°æ®å‡†å¤‡:
        1. ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾è·¯å¾„çš„å†å²é‡å¤ç‡
        2. åˆ›å»º (è·¯å¾„, é‡å¤ç‡) è®­ç»ƒå¯¹
        3. å›å½’è®­ç»ƒ
    """
    print("\n" + "=" * 60)
    print("ğŸŒ³ Labelé¢„è®­ç»ƒ - è·¯å¾„é£é™©å›å½’")
    print(f"   è½®æ•°: {config.pretrain.global_graph_epochs}")
    print("=" * 60)

    device = config.training.device

    # ========== 1. æ•°æ®å‡†å¤‡ ==========
    import pandas as pd
    from data_processor import ComplaintDataProcessor

    processor = ComplaintDataProcessor(config)

    # åŠ è½½æ•°æ®å¹¶æ„å»ºå…¨å±€å›¾
    df = processor.load_data(config.training.large_data_file, for_pretrain=True)
    processor.build_global_ontology_tree(df['Complaint label'].tolist())

    vocab_size = len(processor.node_to_id)
    print(f"\nğŸ“Š å…¨å±€å›¾ç»Ÿè®¡:")
    print(f"   èŠ‚ç‚¹æ•°: {vocab_size}")
    print(f"   è¾¹æ•°: {processor.edge_index.shape[1]}")

    # ========== 2. ç»Ÿè®¡æ ‡ç­¾è·¯å¾„çš„é‡å¤ç‡ ==========
    print("\nğŸ”¢ ç»Ÿè®¡æ ‡ç­¾è·¯å¾„çš„å†å²é‡å¤ç‡...")

    # âœ… ä¿®å¤3: æ”¹è¿›è·¯å¾„ç»Ÿè®¡é€»è¾‘ (å¤„ç†ç©ºæ ¼é—®é¢˜)
    path_repeat_stats = {}

    for idx, row in df.iterrows():
        label_path = row['Complaint label']
        is_repeat = int(row['Repeat complaint'])

        # å°†è·¯å¾„å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        if isinstance(label_path, str) and label_path.strip():
            # âœ… å…³é”®ä¿®å¤: å…ˆç§»é™¤æ‰€æœ‰ç©ºæ ¼,å†ç»Ÿä¸€åˆ†éš”ç¬¦
            # åŸå§‹: "æŠ•è¯‰ â†’ ç½‘ç»œè´¨é‡ â†’ ä¿¡å·å·®"
            # å¤„ç†å: "æŠ•è¯‰â†’ç½‘ç»œè´¨é‡â†’ä¿¡å·å·®"
            cleaned_path = label_path.replace(' ', '')  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
            normalized_path = cleaned_path.replace('>', 'â†’').replace('->', 'â†’').strip()

            # åªç»Ÿè®¡åŒ…å«å±‚çº§å…³ç³»çš„è·¯å¾„(è‡³å°‘æœ‰â†’ç¬¦å·)
            if 'â†’' in normalized_path:
                if normalized_path not in path_repeat_stats:
                    path_repeat_stats[normalized_path] = {'total': 0, 'repeat': 0}

                path_repeat_stats[normalized_path]['total'] += 1
                path_repeat_stats[normalized_path]['repeat'] += is_repeat

    print(f"âœ… æ”¶é›†åˆ°åŸå§‹è·¯å¾„: {len(path_repeat_stats)} æ¡")

    # è®¡ç®—é‡å¤ç‡,è¿‡æ»¤æ ·æœ¬å¤ªå°‘çš„è·¯å¾„
    min_samples = 5
    path_risk_dict = {}

    for path, stats in path_repeat_stats.items():
        total = stats['total']
        repeat = stats['repeat']

        if total >= min_samples:
            risk = repeat / total
            path_risk_dict[path] = risk

    print(f"âœ… è¿‡æ»¤åæœ‰æ•ˆè·¯å¾„: {len(path_risk_dict)} æ¡ (æ ·æœ¬ >= {min_samples})")

    if len(path_risk_dict) == 0:
        print("\nâš ï¸ è­¦å‘Š: æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è·¯å¾„!")
        print("   å¯èƒ½åŸå› :")
        print("   1. æ ‡ç­¾æ ¼å¼ä¸åŒ¹é… - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨â†’åˆ†éš”")
        print("   2. æ•°æ®é‡å¤ªå° - æ¯æ¡è·¯å¾„éœ€è¦è‡³å°‘5ä¸ªæ ·æœ¬")
        print(f"   3. æ•°æ®ç¤ºä¾‹: {df['Complaint label'].head(3).tolist()}")
        print("\nè·³è¿‡Labelé¢„è®­ç»ƒ,ç»§ç»­åç»­æµç¨‹...")
        return None  # è¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯

    print(f"\nâœ… è·¯å¾„ç»Ÿè®¡å®Œæˆ:")
    print(f"   æ€»è·¯å¾„æ•°: {len(path_repeat_stats)}")
    print(f"   æœ‰æ•ˆè·¯å¾„æ•°: {len(path_risk_dict)} (æ ·æœ¬ >= {min_samples})")

    # é£é™©åˆ†å¸ƒ
    risks = [v['risk'] for v in path_risk_dict.values()]
    print(f"\nğŸ“ˆ é£é™©åˆ†å¸ƒ:")
    print(f"   å‡å€¼: {np.mean(risks):.4f}")
    print(f"   ä¸­ä½æ•°: {np.median(risks):.4f}")
    print(f"   æ ‡å‡†å·®: {np.std(risks):.4f}")
    print(f"   æœ€å°å€¼: {np.min(risks):.4f}")
    print(f"   æœ€å¤§å€¼: {np.max(risks):.4f}")

    # ========== 3. åˆ›å»ºè®­ç»ƒæ•°æ® ==========
    # å°†è·¯å¾„è½¬æ¢ä¸ºå›¾æ•°æ®
    train_data_list = []
    train_risks = []

    for path, risk in path_risk_dict.items():
        # Split path and clean spaces
        nodes = [n.strip() for n in path.split('â†’') if n.strip()]

        # Map node names to IDs
        node_ids = []
        for node_name in nodes:
            if node_name in processor.node_to_id:
                node_ids.append(processor.node_to_id[node_name])

        # Only add if path has multiple nodes
        if len(node_ids) > 1:
            train_data_list.append(node_ids)
            train_risks.append(risk)  # Use 'risk' not 'stats'

    print(f"\nâœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(train_data_list)} æ¡è·¯å¾„")
    # âœ… æ·»åŠ ï¼šç©ºæ•°æ®é›†æ£€æŸ¥
    if len(train_data_list) == 0:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. æ ‡ç­¾è·¯å¾„æ ¼å¼ä¸åŒ¹é…ï¼ˆç¼ºå°‘â†’ç¬¦å·ï¼‰")
        print("   2. æœ‰æ•ˆæ ·æœ¬æ•°å¤ªå°‘ï¼ˆ< 5ä¸ªæ ·æœ¬çš„è·¯å¾„è¢«è¿‡æ»¤ï¼‰")
        print("   3. processor.node_to_id ä¸­æ²¡æœ‰å¯¹åº”çš„èŠ‚ç‚¹")
        return None, None, None

    if len(train_data_list) < 10:
        print(f"âš ï¸  è­¦å‘Šï¼šè®­ç»ƒæ•°æ®å¤ªå°‘ ({len(train_data_list)}æ¡)")
        print("   å»ºè®®è‡³å°‘50æ¡è·¯å¾„æ‰èƒ½è·å¾—ç¨³å®šçš„é¢„è®­ç»ƒæ•ˆæœ")
        print("   å¯ä»¥å°è¯•é™ä½ min_samples å‚æ•°ï¼ˆå½“å‰ä¸º5ï¼‰")
    # ========== 4. åˆ›å»ºæ¨¡å‹ ==========
    from model import GATLabelEncoder, LabelRiskRegressor

    label_encoder = GATLabelEncoder(
        vocab_size=vocab_size,
        hidden_dim=256,
        num_layers=3,
        num_heads=4
    ).to(device)

    regressor = LabelRiskRegressor(label_encoder, hidden_dim=256).to(device)

    # ========== 5. è®­ç»ƒè®¾ç½® ==========
    from torch.utils.data import Dataset, DataLoader

    class PathRiskDataset(Dataset):
        """è·¯å¾„-é£é™©æ•°æ®é›†"""

        def __init__(self, paths, risks, processor):
            self.paths = paths
            self.risks = risks
            self.processor = processor

        def __len__(self):
            return len(self.paths)

        def __getitem__(self, idx):
            node_ids = self.paths[idx]
            risk = self.risks[idx]

            # æ„é€ è¾¹ï¼ˆé¡ºåºè¿æ¥ï¼‰
            edge_index = []
            if len(node_ids) > 1:
                for i in range(len(node_ids) - 1):
                    edge_index.append([node_ids[i], node_ids[i + 1]])

            # èŠ‚ç‚¹å±‚çº§
            node_levels = []
            for node_id in node_ids:
                # ä»å…¨å±€node_levelsè·å–
                if node_id < len(self.processor.node_levels):
                    node_levels.append(self.processor.node_levels[node_id].item())
                else:
                    node_levels.append(0)

            return {
                'node_ids': torch.tensor(node_ids, dtype=torch.long),
                'edge_index': torch.tensor(edge_index, dtype=torch.long).t() if edge_index else torch.zeros((2, 0),
                                                                                                            dtype=torch.long),
                'node_levels': torch.tensor(node_levels, dtype=torch.long),
                'risk': torch.tensor(risk, dtype=torch.float32)
            }

    dataset = PathRiskDataset(train_data_list, train_risks, processor)

    # 80/20åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_graph_batch)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_graph_batch)

    # ========== 6. è®­ç»ƒ ==========
    optimizer = torch.optim.Adam(regressor.parameters(), lr=config.pretrain.global_graph_lr)
    criterion = nn.MSELoss()  # å›å½’æŸå¤±

    best_val_loss = float('inf')
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(config.pretrain.global_graph_epochs):
        # è®­ç»ƒé˜¶æ®µ
        regressor.train()
        train_loss = 0
        train_mae = 0  # å¹³å‡ç»å¯¹è¯¯å·®

        for batch in train_loader:
            batch = batch.to(device)
            # âœ… æ·»åŠ ï¼šç¡®ä¿å±æ€§å­˜åœ¨
            if not hasattr(batch, 'node_ids'):
                batch.node_ids = batch.x
            if not hasattr(batch, 'risk'):
                batch.risk = batch.y
            # å‰å‘ä¼ æ’­
            pred_risk, _ = regressor(
                batch.node_ids,
                batch.edge_index,
                batch.node_levels,
                batch.batch
            )

            # è®¡ç®—æŸå¤±
            loss = criterion(pred_risk.squeeze(), batch.risk)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_mae += torch.abs(pred_risk.squeeze() - batch.risk).mean().item()

        train_loss /= len(train_loader)
        train_mae /= len(train_loader)

        # éªŒè¯é˜¶æ®µ
        regressor.eval()
        val_loss = 0
        val_mae = 0
        all_preds = []
        all_risks = []

        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                # âœ… æ·»åŠ ï¼šç¡®ä¿å±æ€§å­˜åœ¨
                if not hasattr(batch, 'node_ids'):
                    batch.node_ids = batch.x
                if not hasattr(batch, 'risk'):
                    batch.risk = batch.y

                pred_risk, _ = regressor(
                    batch.node_ids,
                    batch.edge_index,
                    batch.node_levels,
                    batch.batch
                )

                loss = criterion(pred_risk.squeeze(), batch.risk)
                val_loss += loss.item()
                val_mae += torch.abs(pred_risk.squeeze() - batch.risk).mean().item()

                all_preds.extend(pred_risk.squeeze().cpu().numpy())
                all_risks.extend(batch.risk.cpu().numpy())

        val_loss /= len(val_loader)
        val_mae /= len(val_loader)

        # è®¡ç®—ç›¸å…³ç³»æ•°
        corr = np.corrcoef(all_preds, all_risks)[0, 1]

        print(f"Epoch {epoch + 1}/{config.pretrain.global_graph_epochs}:")
        print(f"  Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f}")
        print(f"  Val Loss: {val_loss:.4f}, MAE: {val_mae:.4f}, Corr: {corr:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'label_encoder': label_encoder.state_dict(),
                'regressor': regressor.state_dict(),
                'epoch': epoch,
                'val_loss': val_loss,
                'val_mae': val_mae,
                'corr': corr
            }, os.path.join(save_dir, 'best_model.pt'))
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")

    # ========== 7. é˜ˆå€¼æ ¡å‡† ==========
    print("\n" + "=" * 60)
    print("ğŸ“Š é˜ˆå€¼æ ¡å‡†")
    print("=" * 60)

    from model import ThresholdCalibrator
    calibrator = ThresholdCalibrator()

    # å°†é£é™©åˆ†æ•°è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆç”¨äºæ ¡å‡†ï¼‰
    all_risks_np = np.array(all_risks)
    all_labels = (all_risks_np > 0.5).astype(int)  # ç®€å•é˜ˆå€¼

    optimal_threshold = calibrator.calibrate(np.array(all_preds), all_labels)

    # ä¿å­˜æ ¡å‡†å™¨
    import pickle
    with open(os.path.join(save_dir, 'calibrator.pkl'), 'wb') as f:
        pickle.dump(calibrator, f)

    print(f"\nâœ… Labelé¢„è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f}")
    print(f"   æ ¡å‡†é˜ˆå€¼: {optimal_threshold:.4f}")
    print(f"   æ¨¡å‹ä¿å­˜è‡³: {save_dir}")

    return label_encoder, regressor, calibrator


def collate_graph_batch(batch):
    """
    å°†å¤šä¸ªå›¾æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹æ¬¡
    ä¿®å¤ç‰ˆï¼šæ·»åŠ å¿…è¦çš„å±æ€§ä»¥é¿å…AttributeError
    """
    from torch_geometric.data import Data, Batch

    data_list = []
    for item in batch:
        data = Data(
            x=item['node_ids'],
            edge_index=item['edge_index'],
            node_levels=item['node_levels'],
            y=item['risk']
        )
        data_list.append(data)

    batched = Batch.from_data_list(data_list)

    # âœ… ä¿®å¤ï¼šæ·»åŠ å¿…è¦çš„å±æ€§
    batched.node_ids = batched.x  # å°†xå¤åˆ¶ä¸ºnode_ids
    batched.risk = batched.y  # å°†yå¤åˆ¶ä¸ºrisk

    return batched


def train_curriculum_learning(config, processor, train_data, val_data, pretrain_text_path=None):
    """
    è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ - æ–¹å‘äº”
    ä½¿ç”¨3.5ä¸‡å¹³è¡¡æ•°æ®
    """
    print("\n" + "=" * 60)
    print("ğŸš€ è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
    print("=" * 60)

    # ========== å•æ¨¡æ€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ ==========
    import sys
    test_single_modal = None
    for arg in sys.argv:
        if '--test_single_modal' in arg:
            idx = sys.argv.index(arg)
            if idx + 1 < len(sys.argv):
                test_single_modal = sys.argv[idx + 1]
                break

    if test_single_modal:
        print(f"\nâš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼: åªè®­ç»ƒ {test_single_modal}_only")
        print("=" * 60)

    vocab_size = train_data['vocab_size']
    device = config.training.device

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = ComplaintDataset(
        text_data=train_data['text_data'],
        node_ids_list=train_data['node_ids_list'],
        edges_list=train_data['edges_list'],
        node_levels_list=train_data['node_levels_list'],
        struct_features=train_data['struct_features'],
        targets=train_data['targets']
    )

    val_dataset = ComplaintDataset(
        text_data=val_data['text_data'],
        node_ids_list=val_data['node_ids_list'],
        edges_list=val_data['edges_list'],
        node_levels_list=val_data['node_levels_list'],
        struct_features=val_data['struct_features'],
        targets=val_data['targets']
    )

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size,
                              shuffle=True, num_workers=0, collate_fn=custom_collate_fn,drop_last=True)  # âœ… æ–°å¢
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size,
                            shuffle=False, num_workers=0, collate_fn=custom_collate_fn)  # âœ… æ–°å¢

    # ========== é˜¶æ®µ1: å•æ¨¡æ€é¢„è®­ç»ƒ ==========
    print("\nğŸ“Œ é˜¶æ®µ1: å•æ¨¡æ€é¢„è®­ç»ƒ")

    models = {}
    for mode in ['text_only', 'label_only', 'struct_only']:
        print(f"\nè®­ç»ƒ {mode} æ¨¡å‹...")

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode=mode,
            pretrained_path=pretrain_text_path
        ).to(device)

        optimizer = optim.AdamW(model.parameters(), lr=config.training.stage1_lr)
        criterion = FocalLoss() if config.training.use_focal_loss else nn.CrossEntropyLoss()

        best_acc = 0
        for epoch in range(config.training.stage1_single_modal_epochs):
            model.train()
            total_loss = 0

            for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
                # âœ… ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„æ¨¡æ€åˆ¤æ–­
                need_text = mode in ['text_only', 'text_label', 'text_struct', 'full']
                input_ids = batch['input_ids'].to(device) if need_text else None
                attention_mask = batch['attention_mask'].to(device) if need_text else None

                need_label = mode in ['label_only', 'text_label', 'label_struct', 'full']
                node_ids_list = batch['node_ids'] if need_label else None
                edges_list = batch['edges'] if need_label else None
                node_levels_list = batch['node_levels'] if need_label else None

                need_struct = mode in ['struct_only', 'text_struct', 'label_struct', 'full']
                struct_features = batch['struct_features'].to(device) if need_struct else None

                targets = batch['target'].to(device)

                logits, _ = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=node_ids_list,
                    edges_list=edges_list,
                    node_levels_list=node_levels_list,
                    struct_features=struct_features
                )

                loss = criterion(logits, targets)

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
                optimizer.step()

                total_loss += loss.item()

            # éªŒè¯
            val_acc = evaluate_model(model, val_loader, device, mode)
            print(f"Epoch {epoch + 1} - Loss: {total_loss / len(train_loader):.4f}, Val Acc: {val_acc:.4f}")

            if val_acc > best_acc:
                best_acc = val_acc
                models[mode] = model.state_dict()

        print(f"âœ… {mode} æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
        # å¦‚æœæ˜¯å•æ¨¡æ€æµ‹è¯•ï¼Œè®­ç»ƒå®Œå°±è¿”å›
        if test_single_modal and mode == f"{test_single_modal}_only":
            print(f"\nâœ… å•æ¨¡æ€æµ‹è¯•å®Œæˆ: {mode}")
            print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
            print("\nğŸ’¡ å¦‚æœæ²¡æœ‰æŠ¥é”™ï¼Œè¯´æ˜è¿™ä¸ªæ¨¡æ€çš„ä»£ç æ²¡é—®é¢˜ï¼")
            return None, None

    # ========== é˜¶æ®µ2: åŒæ¨¡æ€äº¤äº’ ==========
    print("\nğŸ“Œ é˜¶æ®µ2: åŒæ¨¡æ€äº¤äº’")

    dual_modes = ['text_label', 'text_struct', 'label_struct']
    for mode in dual_modes:
        print(f"\nè®­ç»ƒ {mode} æ¨¡å‹...")

        model = MultiModalComplaintModel(
            config=config,
            vocab_size=vocab_size,
            mode=mode,
            pretrained_path=pretrain_text_path
        ).to(device)

        optimizer = optim.AdamW(model.parameters(), lr=config.training.stage2_lr)
        criterion = FocalLoss() if config.training.use_focal_loss else nn.CrossEntropyLoss()

        best_acc = 0
        for epoch in range(config.training.stage2_dual_modal_epochs):
            model.train()
            total_loss = 0

            for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
                # âœ… ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„æ¨¡æ€åˆ¤æ–­
                need_text = mode in ['text_only', 'text_label', 'text_struct', 'full']
                input_ids = batch['input_ids'].to(device) if need_text else None
                attention_mask = batch['attention_mask'].to(device) if need_text else None

                need_label = mode in ['label_only', 'text_label', 'label_struct', 'full']
                node_ids_list = batch['node_ids'] if need_label else None
                edges_list = batch['edges'] if need_label else None
                node_levels_list = batch['node_levels'] if need_label else None

                need_struct = mode in ['struct_only', 'text_struct', 'label_struct', 'full']
                struct_features = batch['struct_features'].to(device) if need_struct else None

                targets = batch['target'].to(device)

                logits, _ = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=node_ids_list,
                    edges_list=edges_list,
                    node_levels_list=node_levels_list,
                    struct_features=struct_features
                )

                loss = criterion(logits, targets)

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
                optimizer.step()

                total_loss += loss.item()

            # éªŒè¯
            val_acc = evaluate_model(model, val_loader, device, mode)
            print(f"Epoch {epoch + 1} - Loss: {total_loss / len(train_loader):.4f}, Val Acc: {val_acc:.4f}")

            if val_acc > best_acc:
                best_acc = val_acc
                models[mode] = model.state_dict()

        print(f"âœ… {mode} æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")

    # ========== é˜¶æ®µ3: ä¸‰æ¨¡æ€èåˆ ==========
    print("\nğŸ“Œ é˜¶æ®µ3: ä¸‰æ¨¡æ€èåˆï¼ˆFullæ¨¡å‹ï¼‰")

    full_model = MultiModalComplaintModel(
        config=config,
        vocab_size=vocab_size,
        mode='full',
        pretrained_path=pretrain_text_path
    ).to(device)

    optimizer = optim.AdamW(full_model.parameters(), lr=config.training.stage3_lr)
    criterion = FocalLoss() if config.training.use_focal_loss else nn.CrossEntropyLoss()

    # æ¨¡æ€å¹³è¡¡æŸå¤±ï¼ˆæ–°ç‰ˆ - åŸºäºæ³¨æ„åŠ›æƒé‡ï¼‰
    modal_balance_loss_fn = ModalBalanceLoss(weight=config.training.modal_balance_weight) \
        if config.training.use_modal_balance_loss else None
    # æ¨¡æ€å¹³è¡¡æŸå¤±
    modal_balance_loss_fn = ModalBalanceLoss(weight=config.training.modal_balance_weight) \
        if config.training.use_modal_balance_loss else None

    # ========== æºå¤´é¢„é˜²: å¯åŠ¨å‰éªŒè¯æ¨¡å‹è¿”å›æ ¼å¼ ==========
    print("\nğŸ” éªŒè¯æ¨¡å‹è¿”å›æ ¼å¼...")
    try:
        # è·å–ä¸€ä¸ªæµ‹è¯•batch
        test_batch = next(iter(train_loader))
        test_size = min(2, len(test_batch['input_ids']))

        # æµ‹è¯•1: return_attention=False
        with torch.no_grad():
            result1 = full_model(
                input_ids=test_batch['input_ids'][:test_size].to(device),
                attention_mask=test_batch['attention_mask'][:test_size].to(device),
                node_ids_list=test_batch['node_ids'][:test_size],
                edges_list=test_batch['edges'][:test_size],
                node_levels_list=test_batch['node_levels'][:test_size],
                struct_features=test_batch['struct_features'][:test_size].to(device),
                return_attention=False
            )

        # æ£€æŸ¥è¿”å›æ ¼å¼
        if isinstance(result1, tuple) and len(result1) == 2:
            print("  âœ… return_attention=False è¿”å›æ ¼å¼æ­£ç¡®: (logits, None)")
        else:
            print(f"  âš ï¸ è¿”å›æ ¼å¼: {type(result1)}")

        # æµ‹è¯•2: return_attention=True
        with torch.no_grad():
            result2 = full_model(
                input_ids=test_batch['input_ids'][:test_size].to(device),
                attention_mask=test_batch['attention_mask'][:test_size].to(device),
                node_ids_list=test_batch['node_ids'][:test_size],
                edges_list=test_batch['edges'][:test_size],
                node_levels_list=test_batch['node_levels'][:test_size],
                struct_features=test_batch['struct_features'][:test_size].to(device),
                return_attention=True
            )

        if isinstance(result2, tuple) and len(result2) == 2:
            logits, attn = result2
            if isinstance(attn, dict):
                required_keys = ['text_to_label', 'label_to_text', 'semantic_to_struct', 'struct_to_semantic']
                missing = [k for k in required_keys if k not in attn or attn[k] is None]
                if missing:
                    print(f"  âš ï¸ æ³¨æ„åŠ›æƒé‡ç¼ºå°‘keys: {missing}")
                else:
                    print("  âœ… return_attention=True è¿”å›æ ¼å¼æ­£ç¡®ï¼Œæ‰€æœ‰keyså®Œæ•´")

                    # æµ‹è¯•æ¨¡æ€å¹³è¡¡æŸå¤±ï¼ˆç°åœ¨modal_balance_loss_fnå·²ç»å®šä¹‰äº†ï¼ï¼‰
                    if modal_balance_loss_fn is not None:
                        try:
                            test_balance = modal_balance_loss_fn(attn)
                            print(f"  âœ… æ¨¡æ€å¹³è¡¡æŸå¤±è®¡ç®—æ­£å¸¸: {test_balance.item():.4f}")
                        except Exception as e:
                            print(f"  âš ï¸ æ¨¡æ€å¹³è¡¡æŸå¤±æµ‹è¯•å¤±è´¥: {e}")
                    else:
                        print("  â„¹ï¸ æ¨¡æ€å¹³è¡¡æŸå¤±æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            else:
                print(f"  âš ï¸ attention_weightsç±»å‹é”™è¯¯: {type(attn)}")

        print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡ï¼Œå¼€å§‹è®­ç»ƒ...\n")

    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹éªŒè¯å‡ºç°é—®é¢˜: {e}")
        print("ç»§ç»­è®­ç»ƒï¼Œä½†è¯·æ³¨æ„è§‚å¯Ÿ...")

    # ========== åˆå§‹åŒ–è®­ç»ƒç›‘æ§ ==========
    monitor = TrainingMonitor(window_size=10)
    print("âœ… è®­ç»ƒç›‘æ§å·²å¯åŠ¨")

    best_acc = 0
    best_model_path = os.path.join(config.training.save_dir, 'best_full_model.pth')
    print(f"\nğŸ¯ æ¨¡æ€å¹³è¡¡ç­–ç•¥:")
    if modal_balance_loss_fn:
        print("  âœ… å¯ç”¨æ³¨æ„åŠ›æƒé‡å¹³è¡¡ï¼ˆç†µæœ€å¤§åŒ–ï¼‰")
        print("  ğŸ“Š ç›®æ ‡ï¼štext=33%, label=33%, struct=33%")
    else:
        print("  âš ï¸  æœªå¯ç”¨æ¨¡æ€å¹³è¡¡")

    best_acc = 0
    best_model_path = os.path.join(config.training.save_dir, 'best_full_model.pth')
    # ========== æºå¤´é¢„é˜²: åˆå§‹åŒ–è®­ç»ƒç›‘æ§ ==========
    monitor = TrainingMonitor(window_size=10)
    print("âœ… è®­ç»ƒç›‘æ§å·²å¯åŠ¨")
    for epoch in range(config.training.stage3_full_epochs):
        full_model.train()
        total_loss = 0
        total_cls_loss = 0
        total_balance_loss = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            # âœ… Fullæ¨¡å¼éœ€è¦æ‰€æœ‰ç‰¹å¾
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            node_ids_list = batch['node_ids']
            edges_list = batch['edges']
            node_levels_list = batch['node_levels']
            struct_features = batch['struct_features'].to(device)
            targets = batch['target'].to(device)

            # ========== å‰å‘ä¼ æ’­ï¼ˆè¿”å›æ³¨æ„åŠ›æƒé‡ï¼‰==========
            # å¦‚æœå¯ç”¨æ¨¡æ€å¹³è¡¡ï¼Œéœ€è¦è¿”å›attention_weights
            if modal_balance_loss_fn is not None:
                logits, attention_weights = full_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=node_ids_list,
                    edges_list=edges_list,
                    node_levels_list=node_levels_list,
                    struct_features=struct_features,
                    return_attention=True  # â† å…³é”®ï¼šè¿”å›æ³¨æ„åŠ›æƒé‡
                )
            else:
                # ä¸éœ€è¦attention_weightsï¼ŒåŠ å¿«é€Ÿåº¦
                logits, _ = full_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    node_ids_list=node_ids_list,
                    edges_list=edges_list,
                    node_levels_list=node_levels_list,
                    struct_features=struct_features
                )
                attention_weights = None

            # ========== åˆ†ç±»æŸå¤± ==========
            cls_loss = criterion(logits, targets)

            # ========== æ¨¡æ€å¹³è¡¡æŸå¤±ï¼ˆæ–°ç‰ˆï¼‰==========
            if modal_balance_loss_fn is not None and attention_weights is not None:
                try:
                    # ä½¿ç”¨æ–°çš„åŸºäºæ³¨æ„åŠ›æƒé‡çš„æ–¹æ³•
                    balance_loss = modal_balance_loss_fn(attention_weights)

                    # æ€»æŸå¤±
                    loss = cls_loss + balance_loss
                    total_balance_loss += balance_loss.item()

                except Exception as e:
                    # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œåªç”¨åˆ†ç±»æŸå¤±
                    print(f"  âš ï¸ æ¨¡æ€å¹³è¡¡æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                    loss = cls_loss
            else:
                loss = cls_loss

            # ========== åå‘ä¼ æ’­ + ç›‘æ§æ£€æŸ¥ ==========
            optimizer.zero_grad()

            # æºå¤´é¢„é˜²: æ£€æŸ¥lossæ˜¯å¦å¼‚å¸¸
            if not monitor.check_loss(loss):
                print("âŒ Losså¼‚å¸¸ï¼Œè·³è¿‡æ­¤batch")
                continue

            loss.backward()

            # æºå¤´é¢„é˜²: æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸
            if not monitor.check_gradients(full_model, max_norm=config.training.max_grad_norm):
                print("âŒ æ¢¯åº¦å¼‚å¸¸ï¼Œè·³è¿‡æ­¤batch")
                optimizer.zero_grad()  # æ¸…ç©ºå¼‚å¸¸æ¢¯åº¦
                continue

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(full_model.parameters(), config.training.max_grad_norm)

            # æºå¤´é¢„é˜²: æ›´æ–°å‰æœ€åæ£€æŸ¥
            if not monitor.check_model_weights(full_model):
                print("âŒ æ¨¡å‹æƒé‡å¼‚å¸¸ï¼Œåœæ­¢è®­ç»ƒ")
                break

            optimizer.step()

            total_loss += loss.item()
            total_cls_loss += cls_loss.item()

        avg_loss = total_loss / len(train_loader)
        avg_cls_loss = total_cls_loss / len(train_loader)
        # æ‰“å°ç›‘æ§æ‘˜è¦
        print(f"  ğŸ“Š ç›‘æ§: {monitor.get_summary()}")
        # éªŒè¯
        val_acc, val_metrics = evaluate_model(full_model, val_loader, device, 'full', return_metrics=True)

        print(f"Epoch {epoch + 1}/{config.training.stage3_full_epochs}")
        print(f"  Loss: {avg_loss:.4f} (Cls: {avg_cls_loss:.4f}", end="")
        if modal_balance_loss_fn:
            print(f", Balance: {total_balance_loss / len(train_loader):.4f})", end="")
        else:
            print(")", end="")
        print(f"\n  Val Acc: {val_acc:.4f}, Precision: {val_metrics['precision']:.4f}, "
              f"Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'model_state_dict': full_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'config': config
            }, best_model_path)
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (acc={val_acc:.4f})")

    print(f"\nâœ… è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆ - æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {best_model_path}")

    return full_model, best_model_path


def evaluate_model(model, dataloader, device, mode, return_metrics=False):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch in dataloader:
            # âœ… ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„æ¨¡æ€åˆ¤æ–­
            need_text = mode in ['text_only', 'text_label', 'text_struct', 'full']
            input_ids = batch['input_ids'].to(device) if need_text else None
            attention_mask = batch['attention_mask'].to(device) if need_text else None

            need_label = mode in ['label_only', 'text_label', 'label_struct', 'full']
            node_ids_list = batch['node_ids'] if need_label else None
            edges_list = batch['edges'] if need_label else None
            node_levels_list = batch['node_levels'] if need_label else None

            need_struct = mode in ['struct_only', 'text_struct', 'label_struct', 'full']
            struct_features = batch['struct_features'].to(device) if need_struct else None

            targets = batch['target'].to(device)

            logits, _ = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                node_ids_list=node_ids_list,
                edges_list=edges_list,
                node_levels_list=node_levels_list,
                struct_features=struct_features
            )

            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_preds)

    if return_metrics:
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_targets, all_preds, average='binary'
        )
        return accuracy, {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }

    return accuracy


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®¢æˆ·é‡å¤æŠ•è¯‰é¢„æµ‹ç³»ç»Ÿ')
    parser.add_argument('--mode', type=str, default='train',
                        choices=['train', 'pretrain_only'],
                        help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--quick_test', action='store_true',
                        help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    parser.add_argument('--production', action='store_true',
                        help='ç”Ÿäº§ç¯å¢ƒæ¨¡å¼')
    parser.add_argument('--skip_text_pretrain', action='store_true',
                        help='è·³è¿‡Texté¢„è®­ç»ƒ')
    parser.add_argument('--skip_label_pretrain', action='store_true',
                        help='è·³è¿‡Labelé¢„è®­ç»ƒ')
    parser.add_argument('--test_single_modal', type=str, default=None,
                        choices=['text', 'label', 'struct'],
                        help='å•ç‹¬æµ‹è¯•æŸä¸ªæ¨¡æ€ï¼ˆç”¨äºå¿«é€Ÿè°ƒè¯•ï¼‰')  # â† æ–°æ·»åŠ è¿™è¡Œ
    parser.add_argument('--data_file', type=str, default=None,
                        help='è®­ç»ƒæ•°æ®æ–‡ä»¶')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    # åŠ è½½é…ç½®
    if args.quick_test:
        config = get_quick_test_config()
    elif args.production:
        config = get_production_config()
    else:
        config = Config()

    # è¦†ç›–æ•°æ®æ–‡ä»¶
    if args.data_file:
        config.training.data_file = args.data_file

    # æ‰“å°é…ç½®æ‘˜è¦
    config.print_summary()

    # ä½¿ç”¨è®¾å¤‡
    device = config.training.device
    print(f"ä½¿ç”¨è®¾å¤‡: {device}\n")

    # ========== æ•°æ®å‡†å¤‡ ==========
    print("=" * 60)
    print("ğŸ“‚ åŠ è½½æ•°æ®")
    print("=" * 60)

    processor = ComplaintDataProcessor(
        config=config,
        user_dict_file=config.data.user_dict_file
    )

    # âœ… ç¬¬ä¸€æ­¥:ä½¿ç”¨24ä¸‡æ•°æ®æ„å»ºå…¨å±€æœ¬ä½“æ ‘
    print("\nğŸŒ³ ä½¿ç”¨24ä¸‡æ•°æ®æ„å»ºå…¨å±€æœ¬ä½“æ ‘...")
    large_df = processor.load_data(config.training.large_data_file, for_pretrain=True)
    processor.build_global_ontology_tree(large_df['Complaint label'].tolist())

    # âœ… å…³é”®ä¿®æ”¹1: ä¿å­˜å…¨å±€è¯æ±‡è¡¨(åœ¨é¢„è®­ç»ƒå¼€å§‹å‰)
    vocab_save_path = os.path.join(config.training.pretrain_save_dir, 'stage2', 'global_vocab.pkl')
    os.makedirs(os.path.dirname(vocab_save_path), exist_ok=True)
    processor.save_global_vocab(vocab_save_path)

    # åŒæ—¶ä¿å­˜åˆ°Labelé¢„è®­ç»ƒç›®å½•
    label_vocab_path = os.path.join(config.training.label_pretrain_save_dir, 'global_vocab.pkl')
    os.makedirs(os.path.dirname(label_vocab_path), exist_ok=True)
    processor.save_global_vocab(label_vocab_path)

    # ä¿å­˜å¤„ç†å™¨(åŒ…å«å…¨å±€æœ¬ä½“æ ‘)
    processor_save_path = os.path.join(config.training.pretrain_save_dir, 'processor.pkl')
    processor.save(processor_save_path)

    # ========== é¢„è®­ç»ƒé˜¶æ®µ ==========
    if args.mode == 'pretrain_only' or not args.skip_text_pretrain:
        print("\n" + "=" * 60)
        print("ğŸš€ é¢„è®­ç»ƒé˜¶æ®µ")
        print("=" * 60)

        # Texté¢„è®­ç»ƒé˜¶æ®µ1
        if not args.skip_text_pretrain:
            stage1_dir = pretrain_text_stage1(
                config, processor, config.training.pretrain_save_dir
            )

            # Texté¢„è®­ç»ƒé˜¶æ®µ2
            # âœ… ä¿®å¤5: ä¼ å…¥processor
            stage2_dir = pretrain_text_stage2_supcon(
                config,
                processor=processor,  # ä¼ å…¥å·²åˆ›å»ºçš„processor
                save_dir=config.training.pretrain_save_dir
            )

            pretrain_text_path = stage2_dir
        else:
            pretrain_text_path = config.training.pretrain_save_dir

        # Labelå…¨å±€å›¾é¢„è®­ç»ƒ
            # Labelå…¨å±€å›¾é¢„è®­ç»ƒ
            if not args.skip_label_pretrain:
                print("\n" + "=" * 60)
                print("Label Pretrain - Path Risk Regression")
                print(f"Epochs: {config.pretrain.global_graph_epochs}")
                print("=" * 60)

                save_dir = os.path.join(config.training.pretrain_save_dir, "label_regression")
                label_pretrain_result = pretrain_label_regression(config, save_dir)

                # Handle None return value
                if label_pretrain_result is None:
                    print("WARNING: Label pretrain skipped, using random initialization")
                else:
                    label_encoder, regressor, calibrator = label_pretrain_result

                    if label_encoder:
                        encoder_save_path = os.path.join(config.training.label_pretrain_save_dir, "label_encoder.pt")
                        torch.save(label_encoder.state_dict(), encoder_save_path)
                        print(f"Label pretrain completed: {save_dir}")

    else:
        pretrain_text_path = config.training.pretrain_save_dir

        # âœ… å…³é”®ä¿®æ”¹2: å¦‚æœåªæ˜¯é¢„è®­ç»ƒ,ç¡®ä¿è¯æ±‡è¡¨å·²ä¿å­˜
        if args.mode == 'pretrain_only':
            print("\n" + "=" * 60)
            print("âœ… é¢„è®­ç»ƒå®Œæˆ!")
            print("=" * 60)
            print(f"ğŸ“ ä¿å­˜ä½ç½®:")
            print(f"  - Texté¢„è®­ç»ƒ: {pretrain_text_path}")
            print(f"  - Labelé¢„è®­ç»ƒ: {config.training.label_pretrain_save_dir}")
            print(f"  - å…¨å±€è¯æ±‡è¡¨: {vocab_save_path}")
            print(f"               {label_vocab_path}")

            # éªŒè¯è¯æ±‡è¡¨æ˜¯å¦å­˜åœ¨
            if os.path.exists(vocab_save_path):
                import pickle
                with open(vocab_save_path, 'rb') as f:
                    vocab_data = pickle.load(f)
                print(f"\nâœ… è¯æ±‡è¡¨éªŒè¯:")
                print(f"  - èŠ‚ç‚¹æ•°: {len(vocab_data['node_to_id'])}")
                print(f"  - è¾¹æ•°: {len(vocab_data['global_edges'])}")

            return

    # ========== è®­ç»ƒé˜¶æ®µ ==========
    print("\n" + "=" * 60)
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ3.5ä¸‡å¹³è¡¡æ•°æ®ï¼‰")
    print("=" * 60)
    # âœ… å¯é€‰: æ£€æŸ¥æ˜¯å¦å·²åŠ è½½å…¨å±€è¯æ±‡è¡¨
    if not processor.node_to_id:
        print("âš ï¸  è­¦å‘Š: å…¨å±€è¯æ±‡è¡¨æœªåŠ è½½!")
        print("   è¿™å¯èƒ½æ˜¯å› ä¸ºè·³è¿‡äº†é¢„è®­ç»ƒé˜¶æ®µ")
        print("   å°†ä½¿ç”¨å½“å‰æ•°æ®é‡æ–°æ„å»ºè¯æ±‡è¡¨ï¼ˆå¯èƒ½å¯¼è‡´ç»´åº¦ä¸åŒ¹é…ï¼‰")

        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        user_input = input("\næ˜¯å¦ç»§ç»­? (y/n): ")
        if user_input.lower() != 'y':
            print("å·²é€€å‡ºã€‚è¯·å…ˆè¿è¡Œé¢„è®­ç»ƒ: python main.py --mode pretrain_only")
            return
    # åŠ è½½3.5ä¸‡å¹³è¡¡æ•°æ®
    train_val_data = processor.prepare_datasets(
        train_file=config.training.data_file,
        for_pretrain=False
    )

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_size = len(train_val_data['targets'])
    train_size = int(total_size * (1 - config.training.val_size))
    val_size = total_size - train_size

    indices = torch.randperm(total_size).tolist()
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    # åˆ†å‰²æ•°æ®
    def split_data(data, indices):
        return {
            'text_data': {
                'input_ids': data['text_data']['input_ids'][indices],
                'attention_mask': data['text_data']['attention_mask'][indices]
            },
            'node_ids_list': [data['node_ids_list'][i] for i in indices],
            'edges_list': [data['edges_list'][i] for i in indices],
            'node_levels_list': [data['node_levels_list'][i] for i in indices],
            'struct_features': data['struct_features'][indices],
            'targets': data['targets'][indices],
            'vocab_size': data['vocab_size']
        }

    train_data = split_data(train_val_data, train_indices)
    val_data = split_data(train_val_data, val_indices)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_indices)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_indices)}")

    # è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
    if config.training.use_curriculum_learning:
        final_model, model_path = train_curriculum_learning(
            config, processor, train_data, val_data, pretrain_text_path
        )
    else:
        print("âš ï¸ æš‚ä¸æ”¯æŒéè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼Œè¯·å¯ç”¨è¯¾ç¨‹å­¦ä¹ ")
        return

    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {model_path}")


if __name__ == "__main__":
    main()